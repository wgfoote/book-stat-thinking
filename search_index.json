[
["index.html", "Statistical Thinking Chapter 1 Preliminaries 1.1 The basic problem 1.2 Analytics 1.3 Readiness for statistical thinking 1.4 Chapter outline", " Statistical Thinking Bill Foote 2020-08-01 Chapter 1 Preliminaries Science alone of all the subjects contains within itself the lesson of the danger of belief in the infallibility of the greatest teachers of the preceding generation. - Richard Feynman This book is designed to provide students, analysts, and practitioners (the collective “we” and “us”) with an elemntary, but practical, approach to the use of statistical thinking especially for business decisions. There are many decisions that managers make. In finance there are two primary decisions: investment and raising funds to make an investment. In marketing there are three top line decisions: product mix, sales mix, customer retention and expansion, market launch and retirement. In operations, from a supply chain point of view there are facilities choices, production, distribution, scheduling, and start up - shut down - idling decisions. This book covers data based decision making from various areas in the organization, from analyzing market drivers such as price and product features, to operational scheduling, hiring, laying off, and inventory, to studying global relations among macroeconomic events and participants. The topics in the book begin with exploratory data analysis with descriptive statistics, layer probabalistic models on top of empirical distributions, and infer ranges of estimates and tests of hypotheses that underly any and every decision. 1.1 The basic problem The premises of this book include the following chain of reasoning: Decisions occur in the context of a compound of raw data, models, priorities, and choices New decisions create new data New data informs new explanations of the relationships in the data Valid, probably speaking, relationships in the data support decisions How can we analyze data? We follow a three step process: Collect credible, valid data that is relevant to the decision at hand. Describe and explain the data by examining the shape of each data element and the relationships among the data elements. Infer the acceptance or rejection of a decision based on the probable range of estimates of the expectation of a data element. We will be taking great pains to define exactly what we mean by expectations and ranges throughout the book. On top of those pains we will be very precise about beliefs in the possible and probable range of expectations. 1.2 Analytics By its very nature the science of data analytics is disruptive. That means, among many other things, that much attention should be paid to the scale and range of invalid, as yet not understood, outlying, and emerging trends. This is as true within each domain of knowledge including business decisions. Outliers are where value is gain and lost. Spikes in interest rates, commodity prices, information about the reputation of an organization, as well as the doldrums of a very slow news day, are all deviations from a supposed trend. Decisions live in the deviation from the trend. Analytics, especially those based in probability and statistics, is all about the analysis of deviations from current and projected beliefs, otherwise known as the trend. 1.3 Readiness for statistical thinking Statistical thinking requires that you are prepared in several areas: Ability to think and analyze logically Apply logic and set theory to partition ranges of statistical measures Algebraicly understand absolute value, logarithmic, exponential, and simultaneous equations Evaluate complex formulae Analyze the geometry and parameters of linear equations Understand area under a curve and rates of change Here are several statistics readiness diagnostic questions for you to as you begin your journey into data analytics. If you have problems with calculations, logic, formulae, algebra, ponder this as the opportunity to surface them, confront them, and resolve them. 1.3.1 Solve this riddle Logic is used at least implicitly in statistical reasoning. When we solve equations we are using logic. Here are two logic operators. An operator is an action that transforms a logical statement. The negation operator \\(\\neg\\) means this: The statement \\(\\neg{A}\\) is true if and only if A is false. The conjunction operator \\(\\wedge\\) or \\(\\&amp;\\) means this: The statement \\(A\\wedge{B}\\) is true if A and B are both true; otherwise, it is false. Consider the following logic statement where \\(p\\) is “the sky is blue” and \\(q\\) is “the weather is sunny.” \\[ \\neg((p \\wedge q) \\wedge \\neg(p \\wedge q)) \\] Is the above logical statement above true or false? Always false Always true Ambiguous: sometimes true, sometimes false show / hide Solve this logical statement using a truth table. For example, this table displays all of the combinations of \\(T=true\\) and \\(F=false\\). We work from the inside of parentheses out, just like we do when solving algebraic equations. ## # A tibble: 4 x 6 ## p q p_and_q not_p_and_q p_and_q_not not_p_and_q_not ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 T T T F F T ## 2 T F F T F T ## 3 F T F T F T ## 4 F F F T F T The answer is is the last column: 2. unabiguously true. See this reference for more details 1.3.2 Join these sets Consider now the following statement, where \\(x\\) is the number of times (natural number) you can sink a basketball in 5 minutes: \\[ (x&gt;2) \\wedge (x&lt;4) \\] What is the resulting value of \\(x\\)? \\(x=1\\) \\(x=5\\) \\(x=3\\) &lt;button onclick=“showText(”truth-1“)”&gt;show / hide Again, a truth table is more than helpful! But let’s just use our verbal powers of reasoning. We are looking for the overlap of numbers greater than 2 and less than 4 with this statement. There is only one natural number inbetween these and it is 3. 1.3.3 Partition this board Along a 6 meter straight piece of wood a cabinet maker wants to mark off three sections of equal length. Use the logical symbol \\(\\vee\\), for example, \\(A\\vee{B}\\) means either A or B, just like the union of two sets. Let \\(x\\) be a position in meters along the piece of wood. Here are three possible expressions: \\((0&lt;x)\\vee(x&gt;4)\\) \\((2&lt;x)\\vee(x&gt;4)\\) \\((2&lt;x)\\vee(x\\geq{4})\\) Which statement describes the first and last sections of the board? Statement 1 Statement 2 Ambiguous: sometimes one value, sometimes another Statement 3 show / hide Two different areas are displayed. The board can three sections 2 meters long each ( 6/3). The first section must be from 0 to almost 2 (think of the saw cut, the kerf), the second from 2 to almost 4, the third from 4 to the end of the board. The two ends are joined in a union only with Statement 3. 1.3.4 Calculate this expression For \\(p=0.25\\), \\(n=3\\), and \\(r=2\\), and not using a calculator, What is the numerical value of \\(p^n(1-p)^{n-r}\\)? 1. well over 0.10 2. a little over 0.01 3. practically zero 4. infinite show / hide \\[ p^n(1-p)^{n-r} = (0.25*3) * (1 - 0.25) ^ (3 - 2) \\] Just follow the order of operations. Here is a reference. 1.3.5 Solve this equation Consider the following equation: \\[ ln(|2x-1|)=0 \\] The solution for \\(x\\) is: \\(x=-1\\) \\(x=-1\\) or \\(x=0\\) \\(x=0\\) or \\(x=1\\) There is no valid solution. show / hide There is indeed a valid solution which is \\(x=0\\) or \\(x=1\\). Let’s isolate the absolute value expression by operating on both sides of the equation with \\(e\\) to the power. \\[ exp(ln|2x-1|)=|2x-1|=e^0=1 \\] We get this by using the definition of absolute value. \\[ |z| = \\left\\{ \\begin{array}{ll} z &amp; \\mbox{if } z \\geq 0 \\\\ -z &amp; \\mbox{if } z &lt; 0 \\end{array} \\right. \\] For us \\(z=2x-1-1-=x-2\\), that is minus the right-hand size of the equation. \\[ |2x-2| = \\left\\{ \\begin{array}{ll} 2x-2 &amp; \\mbox{if } 2x-2 \\geq 0 \\\\ -(2x-2) &amp; \\mbox{if } 2x-2 &lt; 0 \\end{array} \\right. \\] The top branch solution is \\[ \\begin{align} 2x-2 &amp; \\geq 0 \\nonumber \\\\ 2x &amp; \\geq 2 \\nonumber \\\\ x &amp; \\geq 1 \\label{eq:1} \\end{align} \\] The bottom branch solution is \\[ \\begin{align} -2x+2 &amp; &lt; 0 \\nonumber \\\\ -2x &amp; &lt; -2 \\nonumber \\\\ x &amp; &lt; 1 \\label{eq:2} \\end{align} \\] 1.3.6 Summation calculations Given two vectors, \\(X\\) and \\(Y\\) \\[ X = [5, 6, 4, 3 ] \\] and \\[ Y = [3, 2, 2, 1 ] \\] Calculate \\(Z=4\\Sigma_1^4X_iY_i-\\left(\\frac{\\Sigma_1^4 X_i}{4}\\right)\\left(\\frac{\\Sigma_1^4 Y_i}{4}\\right)\\) \\(Z=-106\\) \\(Z=143\\) \\(Z=8\\) There is no valid calculation possible. show / hide The answer is \\(Z=143\\), 1.3.7 Weighted average Given the vector \\(X\\) \\[ X = [5, 6, 4, 3 ] \\] suppose each element of \\(X\\) is wieghted by the corresponding element of the weight vector \\(w\\) \\[ w = [.2, ,5, .2, .1 ] \\] The weighted average of \\(X\\) is: 5.1 1.125 18 There is no valid calculation show / hide The answer is 1.125. 1.3.8 Linear coefficients Two points on a scatter plot are connected by a straight line \\(y=mx+b\\). Calculate the slope \\(m\\) and y-intercept \\(b\\) of the line. The slope \\(m\\) and y-intercept \\(b\\) are: \\(m=0.5,1.5\\) \\(m=1,6.5\\) \\(m=-0.5,4.5\\) There is no valid calculation possible. show / hide The answer is \\(m=-0.5,4.5\\), 1.3.9 Linear simultaneous equations Here are two simultaneous linear equations in \\(x\\) and \\(y\\). \\[ \\begin{align} 12x+152y &amp; = 141 \\\\ 152x+1940y &amp; =1802 \\end{align} \\] Choose the best solution for \\((x,y)\\) (approximately): \\((0.136,0.99)\\) \\((-2.07,1.09)\\) \\((0.1,74,0.14)\\) There is no valid calculation possible. show / hide The answer is \\((-2.07,1.09)\\). 1.3.10 Elasticity: a little light calculus Suppose that \\[ y = ax^b \\] describes the production of \\(y\\) widgets using \\(x\\) labor-hours. What is the elasticity of production with respect to labor-hours if \\(a=1\\) and \\(b=1.5\\)? The elasticity of \\(y\\) with respect to \\(x\\) is: 1.0 1.5 0.5 There is no valid calculation possible. show / hide The answer is \\(dln(y)/dln(x)=b=1.5\\). 1.3.11 Area under a curve The probability \\(p\\) that the amount of time in hours \\(t\\) it takes for a worker to complete a task has been found by an analyst to be \\[ p(t) = e^{-t} \\] Find the area under this curve from \\(t=0\\) to \\(t=1\\) hours. The area is: \\((1/e)-1\\) \\(1-1/e\\) \\(1/e\\) There is no valid calculation possible. show / hide We calculate \\(\\int_0^1e^{-t}dt=1-1/e\\). 1.3.12 Next steps? Check your understanding of the answers you produced. Talking with your instructor or a tutor / counselor in academic resources If you find a gap in your skills (e.g., how to solve an equation) or your capabilities (e.g., why you might need to solve the equation) you are on your way to learning! Gaps can, and with the right motivation and opportunity, will be filled. Good luck on your journey. 1.4 Chapter outline Here is an outline of topics covered by chapter. 1. Preliminaries. This chapter introduces us and reviews some of the basics, . 2. Statistical Reasoning. The components of the statistical thought process and product. A basic model is born along with a foray into the logic embedded in statistical thinking. 3. Data. The many faces of data. 4. Tendency. Data from FRED, Yahoo, and other sources. Empirical characteristics of economic and financial time series. Boostrapping confidence intervals and a stab at managing a list of potential risk and return interactions we will, with much trepidation, call stylized facts. 5. Deviation. Quantile (i.e., Value at Risk) and coherent (i.e., Expected Shortfall) risk measures and a provisional answer the the question about how much capital is needed to support a risky financial decision. 6. The shape of data. Combining risk management with portfolio allocations. Optimizing allocations. Simulating the efficient frontier and answers the question about what combination of assets (and liabilities) returns the highest value for the risk as well as how much cash to hold. 7. Probability. Enough probability to generalize empirical distributions and provide a basis for inference. 8. Uniform, binomial, and Poisson distributions. Three widely useful discrete distributions that also help us approximate continuous distributions. 9. Normal, chi-squared, and Student’s t distributions. All roads often (probably) lead to the normal distribution (central limit theorem at work). Thick tailed distributions can provide more conservative inferences. 10. Confidence interval estimation. 11. Hypothesis testing. "],
["statistical-reasoning.html", "Chapter 2 Statistical Reasoning 2.1 Welcome to Statistical Reasoning 2.2 Why do we reason statistically? 2.3 What is statistical reasoning? 2.4 Let’s learn some more from this example: 2.5 Digging deeper 2.6 What is the conceptual framework of statistical reasoning? 2.7 Infer 2.8 Critical thinking 2.9 Try this", " Chapter 2 Statistical Reasoning 2.1 Welcome to Statistical Reasoning You very likely realize that you have been thinking statistically all your like. Try these questions on for size. Have you ever thought or uttered these statements? I probably have! On average classes here are very hard! How often can I skip a meeting before they fire me? How sure am I that I will get promoted? If you have ever made (or thought) statements and questions like these, you are de facto thinking statistically. This sort of reasoning starts from the gathering of cases, observations, and through classifying, measuring, deliberation, forming new concepts and hypotheses, and framing our tolerance for error to test hypotheses, we arrive at a claim about whatever we think then is true or false (probably: because we admit we might be wrong or refutable!). Through this process, and it is ongoing, we discriminate our subjective opinions from objective knowledge that a claim is true or not. That is the core of evidence-based critical thinking. 2.2 Why do we reason statistically? Consider that: We very well know our exprerience of ignorance We are often prone to error (and try to learn from our mistakes!) We still have to make assertions of all sorts when we decide to act on anything The ground of statistical reasoning is that we experience both necessities and contingencies. Some experiences are so direct they are known certainly, without a doubt, and might not even be subject to further revision. An example of a necessity is our experience of health: if we do not eat, we will eventually, and necessarily become very ill. Other experiences are contingent, that is, they depend on the fulfillment of conditions to exist. An example of a contingency comes from our inability to exhaustively enumerate every type and instance of an act. Will I pass this course? Maybe, probably, possibly, even credibly, but not certainly. One test of necessity versus contingency in an act or being is thus certainty. Are there further questions and conditions? Then we have contingency. Statistics and statistical thinking is all about what we probably know is true or not. Getting to this affirmation we must pass through stages of presentation of data and understanding for inference. Only then do we perform the conscious act of affirmation. After that it is up to the decision maker to execute the imperative of a judgment. 2.3 What is statistical reasoning? Statistical reasoning is a process for generating knowledge under uncertainty. a process has inputs, activities (including decisions), and outputs, so what are the inputs, operations, results? To generate something is to develop (i.e, specify, design) and implement (i.e., build, test, operate, update) something, so what would we develop and implement? Under uncertqainty means possibly, maybe, and more exacting, probably Knowledge is not just taking a look: knowing about some_thing_ or action understanding: describing, explaining, knowing how These are components of knowledge, but we want results, we want a judgement that is reasonable after weighing the pros and cons and consulting our priorities! Our definition of knowledge is formally justified true belief see Plato’s Theaetetus 210a-b for details!): A truth-claim and certitude that something is or not (probably), That is based on available evidence against justified true belief as well as relevant methodology Knowledge thus includes truth-claims that we are aware of this or that and might even understand how they came about. Above all knowledge is an assertion and thus we can expect the judgment (assertion or claim) to be justified (warranted) or not depending on the evidence. Judgment is a conscious activity based on meaning: thus something that can be thought about, deliberated, doubted, entertained as a hypothesis, and believed to be true. We move from knowing about (description and explanation) to knowing how (using a conceptual framework) to knowing that something is TRUE or FALSE (what any proposition is). With uncertainty about the future we append **probably* TRUE or FALSE. Note that your opinion is not knowledge. Beliefs are to be ever challenged by evidence, inquiry, investigation, and the discovering of new relationships and systems. Opinion, true or not, is relative to you. Knowledge moves from something just relative to you to the real relationships among things other than you and your brilliant ideas. Having said this, opinion is still a valuable entree into any investigation and may even be part of the data collection you use to investigate a question, a doubt, an hypothesis, or a belief. 2.4 Let’s learn some more from this example: Overall for the purposes of statistical reasoning there are always three broad components of knowledge and a product: Data collection: results in observations, data, imagination, memory, perception Data analysis: results in description and explanation including logic, relations, classification, association, measurement, hypothesis formation, testing in the presence of potential error Results: the product is an affirmation based on an inference that something is or is not backed by 1 and 2 Suppose that we have data on the incomes of 1000 U.S. families in 2009. This data can be summarized by finding the average family income and the spread of these family incomes above and below the average. The data also can be depicted by constructing a table, chart, or graph of the number or proportion of families in each income class or subsetted (filtered) by state of household residence. We propose to test our concept of the level of the family income in the US overall. Is this last statement descriptive or inferential?\", It is explanatory because it calculates the proportion of families in each income class. It is descriptive because of the calculate of average family income. It is inferential because we can test whether family income in the US is above the average show / hide Answer: It is inferential because we can test whether family income in the US is above the average Now suppose we test our hypothesis that family income in the state of Nebraska is less than average family income in the U.S. We find a non-negligible and significant difference with a probability that we are wrong about our conclusion is only 1%. We believe that 1% is tolerable. Based on our data, our methodology, our tolerance for error, we affirm that family income in Nebraska is less than the average family income in the U.S. Is this last statement knowledge or just your considered opinion? It is considered opinion. It is knowledge. It is neither. show / hide Answer: Not 1: Ask if the final result is verified in the data, nut just your conceptual framework. Not 3: Try again! The correct answer is 2. The result derives from a set of facts, and using relevant methodology, along with an allowance for begin wrong, thus verifiable as objectively as possible. Now suppose we wake up in 10 years and collect more data, use an improved methodology, a lower tolerance for error, and eventually affirm that we were wrong about. We conclude we are wrong in 10 years. Does this invalidate our conclusions 10 years ago necessarily? Yes, because the conclusion 10 years ago was, from today’s point of view, just considered opinion. No, knowledge can change from time to time. In fact the conclusions of prior years are new data for a new investigation and in this case a new judgment about average family income. It is neither. show / hide Answer: The result 10 years ago derived from a set of facts, and using relevant methodology, along with an allowance for begin wrong, thus verifiable as objectively as possible. Circumstances change and so might our inferences and thus our judgments. Not 1: What it only an opinion then? Why? Wasn’t it verified in the data and thus a justified true belief? Certainly not 3. 2.4.1 Why? We have both descriptions and explanations through inferences of the sample of family incomes. We do have knowledge about family incomes knowledge of the relationships of family incomes as explanatory above and below an average This is the domain of descriptive statistics. However, We might even be able to affirm with probability that family incomes are far or not far above the average income Since these conclusions are subject to error, we also would have to indicate the probability of error. These statements belong to the domain of statistical inference. This domain pushes us to an affirmation that family income in one subset of the data is less than average family income in the U.S. This affirmation is built on a bridge from data to judgment using a conceptual framework and tolerance for error. 2.5 Digging deeper 2.5.1 Learn by doing this You work for as an intern for a research department of a real estate developer. Which is the best example of knowledge? 50 observations of a group of business analysts, The steps analysts take to perform a task, Most of the time between 40-50% of the analysts show up for work on time. show / hide You have made a probable assetion. From your and other points of view this is an example of justified true belief – knowledge. 2.6 What is the conceptual framework of statistical reasoning? If you answered “Most of the time between 40-50% of the analysts show up for work on time” you have it right. How do you know though? We gather data sampled from a group, here a population of analysts. This data is what we record as the experience of a particular attribute of analysts’ behavior: on time arrival to work. There is a key question hidden in the sentence: what proportion of analysts “…show up for work on time…”? The proportion can be measured. This leads to understanding the attribute of on-time arrival of analysts. We hedge our bets with a claim that a range of possible analyst outcomes and a probabalistic statement: the spread or range is between 40-50/% “most of the time.” we might try to hone our idea of uncertainty with odds that an analyst will arrive on-time. Here are some of the components of everyday statistical reasoning: You have a question about that, when answered, supports a decision Given the context of the business decision you identify the population and collect a sample of data from the population You try to understand the data by exploring averages, ranges, and frequencies of low, medium, and high values of data; you might transform the data into ratios, proportions, percent changes, etc. Then you set a margin of error that you might be wrong about inferring behavior of the population using a limited sample of data from the population; using the error you estimate the range of acceptable aggregated results Finally, you make a judgment about the behavior of the population based on the probable range of behavioral outcomes measured in the sample, 2.6.1 Learn by doing Here is a conceptual framework we can follow to generate knowledge to support a decision. The decision maker Poses a business question Sets a tolerance for error The analyst uses relevant methodology In consultation with the decision maker chooses a population and agrees to risk tolerance Explores the data Infers a range of tolerable answers The decision maker then Affirms as probably true the range of tolerable answers Hover over the nodes and edges of this network diagram to answer the following question: What is the sequence of nodes to support the decision maker? Business question(s), Data exploration, Decision maker, Provisional conclusion Draw sample, Aggregate data, Provisionally conclude, Make decision Assess decision, Pose business questions, Explore sample data, Infer population bevavior, Make decision Set margin of error, Draw sample, Explore data, Make decision show / hide Answer: Start with the decision maker and her decision. Get to the relevand questions. Gather and explore some useful data. Indicate tolerance for error and infer behavior – usually an inductive conclusion. Offer recommendations to the decision maker. 2.7 Infer A large part of the mechanics of statistical thinking is to formulate and make inferences, that is, judgments of fact and value. Reasoning is the process of using existing knowledge to draw conclusions, make predictions, or construct explanations. To make an inference is to reach a conclusion. There are three types of conclusions: A conclusion that is guaranteed, certain, immutable (unless some fact might change) A conclusion that is likely (probable) and which might be altered with new data A conclusion that is likely or guessed explanation of facts The first is classic, logical, deduction. The second is how we generalize from particular examples, samples, otherwise known as induction. The third is how we formulate hypotheses, known as abduction. 2.7.1 Deductive reasoning: conclusion guaranteed Deductive reasoning starts with the assertion of a general rule and proceeds from there to a guaranteed specific conclusion. If the original assertions are true, then the conclusion must also be true. For example, math is deductive: If \\(x=4\\) And if \\(y=1\\) Then \\(2x+y=9\\) In this example, it is a logical necessity that \\(2x+y=9\\) must equal 9, only if, that is, conditional on, both \\(x=4\\) and \\(y=1\\). As a matter of fact, formal, symbolic logic uses a language that looks rather like the mathematical equality above, complete with its own operators and syntax. But a deductive syllogism (think of it as a plain-English version of a math equality) can be expressed in ordinary language. In the syllogism above, the first two statements, the propositions or premises, lead logically to the third statement, the conclusion. Here is another example: A video streaming technology ought to be funded if it has been used successfully to educate adults. Hybrid open classrooms are being used to educate adults successfully in more than 70 new educational environments. Hybrid (sunchronous and asynchronous) classrooms and video streaming technology should be funded. The conclusion to fund both classrooms and technology follows the truth of the success of both treatments to educate adults successfully. Here is an example of a false conclusion reached through a correct deductive process. There is no such thing as drought in the midwest. North Dakota is in the mid-West. North Dakota does not have to plan for a drought. In the example above, though the inferential process itself is valid, the conclusion is false because the premise is questionable, thus this deduction cannot guarantee a necessary true conclusion, Here is the general pattern of deductive reasoning, again with an example. A bag contains objects. Deduction would be: [MAJOR] All objects (O) from the bag are rotini (R) (if: O in the bag are R is TRUE) [MINOR] I drew this object (O) from the bag (if: an O happens) [CONCLUSION] The object (O) must be rotini (R) (then: O must be R) Some might argue that there is no new knowledge through logical deduction. Their idea is that if conclusions contain the terms of the premise, then there is nothing new in the conclusion. This is a view held for example by John Stuart Mill. We might counter that with the idea that the process of deduction itself uncovered the truths contained in the premises. The deduction thus does add something: a reformed and rearranged set of terms from the premises into a new premise, the conclusion. 2.7.2 Inductive reasoning: comclusion not guaranteed Inductive reasoning begins with observations that are specific and limited in scope, and proceeds to a generalized conclusion that is likely, but not certain, in light of accumulated evidence. You could say that inductive reasoning moves from the specific to the general. Much scientific research is carried out by the inductive method: gathering evidence, seeking patterns, and forming a hypothesis or theory to explain what is seen. Conclusions reached by the inductive method are not logical necessities; no amount of inductive evidence guarantees the conclusion. This is because there is no way to know that all the possible evidence has been gathered, and that there exists no further bit of unobserved evidence that might invalidate my hypothesis. Thus, while the newspapers might report the conclusions of scientific research as absolutes, scientific literature itself uses more cautious language, the language of inductively reached, probable conclusions. Here is an example of inductive reasoning using the objects in the bag example again and for comparison. [MAJOR] An object is drawn from the bag (O) [MINOR] I drew a rotini from the bag (R) [CONCLUSION] All objects in the bag are rotini (maybe) (then: O in the bag are probably R) Because inductive conclusions are not logical necessities, inductive arguments are not simply true. Rather, they are cogent: that is, the evidence seems complete, relevant, and generally convincing, and the conclusion is therefore probably true. Nor are inductive arguments simply false; rather, they are not cogent. We thus have a new category in our reasoning arsenal: true or false from deduction, cogency from induction. It is an important difference from deductive reasoning that, while inductive reasoning cannot yield an absolutely certain conclusion, it can actually increase human knowledge. It can make predictions about future events or as-yet unobserved phenomena. For example, Albert Einstein observed the movement of a pocket compass when he was five years old and became fascinated with the idea that something invisible in the space around the compass needle was causing it to move. This observation, combined with additional observations (of moving trains, for example) and the results of logical and mathematical tools (deduction), resulted in a rule that fit his observations and could predict events that were as yet unobserved. 2.7.3 Abductive reasoning: taking your best shot Abductive reasoning begins with whatever information you have on hand. When you reason this way you take that information and abduce (from the Latin aducere to lead from) a probable explanation. The problem is that our information is nearly always incomplete. It might be not only incomplete but also not very credible or useful in explaining anything. Examples of abduction reasoning include medical diagnoses, identification of assailants, the success of a never before seen product in a completely new and unknown market, the validity of any test results when test reported are bribed. Here is the way abduction works. Given a result (I have a high fever, focus groups would buy the product, test results are better than ever), and given a set of symptoms, observations from a focus group, test results what is the diagnosis, the pattern, the outcomes that would best explain most of them? Likewise, when jurors hear evidence in a criminal case, they must consider whether the prosecution or the defense has the best explanation to cover all the points of evidence. While there may be no certainty about their verdict, since there may exist additional evidence that was not admitted in the case, they make their best guess based on what they know. While cogent inductive reasoning requires that the evidence that might shed light on the subject be fairly complete, whether positive or negative, abductive reasoning is characterized by lack of completeness, either in the evidence, or in the explanation, or both. A patient may be unconscious or fail to report every symptom, for example, resulting in incomplete evidence, or a doctor may arrive at a diagnosis that fails to explain several of the symptoms. Still, she must reach the best diagnosis she can. Back to the objects in a bag again, here is abduction. [MAJOR] All objects in the bag are rotini (maybe) (guest: O in the bag are R) [MINOR] I drew a rotini (find: R) [CONCLUSION] An object (maybe) was draw from this bag (then O from the bag) The abductive process can be creative, intuitive, even revolutionary. Einstein’s work, for example, was not just inductive and deductive, but involved a creative leap of imagination and visualization that scarcely seemed warranted by the mere observation of moving trains and falling elevators. In fact, so much of Einstein’s work was done as a hought experiment. We are pretty sure that he never experimentally dropped an elevator. Many of his peers discredited his abductive explanations as the workings of an unbalanced mind. Nevertheless, he appears to have been right - until now his remarkable conclusions about space-time continue to be verified experimentally. 2.8 Critical thinking What is thinking? We have already defined knowledge as justified true belief. Thinking is the process of getting to a belief that is both true and justified. The process yields tables of experiences as data and frameworks and methodologies to understand the relationships, credibility, completeness, sufficiency, and relevance of the data to the problem at hand. Why critical? The origin of the word critical is from the Greek verb kritein which means to judge. In its root sense then, when we are critical, we are judging. But the very act of judging is built on, depends on, acts of experience and understanding (or lack thereof!). A judgment occurs at the nexus of what is weighed, pro and con, raised from acts of understanding data in the light of premisses, scenarios, and hypotheses. New judgments then become new data to be experienced, understood, and weighed into new judgments. Some new judgments derive from new data that shows that previous judgment are probably not justifiable, and thus should not be believed. Every judgment of what probably is (true) or is not (false) is then justified by solid understanding of relevent, and sufficient, experiences. The error of past data, understanding, and judgment occurs when any of these artifacts are no longer relevant, sufficient, complete (enough), or any longer reasonable, thus no longer justified. Most of our knowledge is belief about what we are very confident to be probably true or not true. Confidence and probability can be used hand in hand. Highly confident means also a low tolerance for error. We know what is or is not probably in any case, but overlaying an interpretation confidence and error fills out a picture of belief, as well as justification of judgments. So, then we might summarize that so-called critical thinking is the development of justified true beliefs subjective to tolerances for error and processes for the correction of error all in the context of experience and understanding.1 That’s statistical reasoning in a nutshell from experienced observation through model inquiries onto judgments of what probably is or is not, support decisions to commit. 2.9 Try this From everyday life, work, school, leisure formulate a question. For example if you work in finance, key business questions would include the following: How do we measure the interactions of risk and return in multiple markets? Given the interactions of risk and return in assets and liabilities, what is the combination of assets and liabilities that return the highest value for the risk? Given this combination (a portfolio) what is amount of capital needed to support the portfolio? Given risk tolerances and thresholds for loss, how much cash should be held on the balance sheet? There are only two decisions at the highest levels of consideration in finance: Investment in claims to risky cash flows, and Financing those investments. You can identify questions in your own life and work and link decisions to those questions. Having done that use deduction, induction, and abduction to formulate conclusions based on premises, constraints, observations, beliefs, assumptions. Here is a resource to define and implement critical thinking processes..↩ "],
["the-many-faces-of-data.html", "Chapter 3 The many faces of data 3.1 Learning outomes 3.2 March on 3.3 Air pollution and birth outcomes 3.4 Something a bit more complicated 3.5 What have we learned? 3.6 Review these scenarios 3.7 Try this", " Chapter 3 The many faces of data 3.1 Learning outomes In this unit we will: Identify the different types of data Describe the use of different types of data to explore answers to business questions Identify a research question and justify the the use of different types of data Data is all around us. It is first an observation of an event like number of store visits, and later might be observations of the result of a conceptual framework like customer satisfaction. Where we make a store transaction would result in latitude and longitude data. There is also data that describes other data like kilo-watt-hours as a unit of measurement for electricity usage. We will examine both univariate (from Latin unus, one, and variare, to change) or one-variable data series as well as multivariate (from Latin multus, many) or arrays of univariate series. 3.2 March on Here is a classic example of the many types of data we will confront. Minard: Napoleon’s march to and from Moskow How many data streams can you count in this graphic? show / hide Several are present. But five are important: Latitude and longitude of the march to and from Moskow A scatter plot of the locations of cities en route Troup strength from and to Moskow Temperature differentials from and to Moskow Intervals of troup strength from and to Moskow We can observe several faces of data in this highly informative plot. This last item elucidates, that is, adds to, item three. We include it because it is a great example of the extensibility of lower orders of data (troup strength) into higher orders of analysis of data (changes in troup strength across latitude and longitude). 3.3 Air pollution and birth outcomes Businesses will often attempt to measure the impact of their operations on the populations they serve or are located near. Energy companies manufacture electricity, refined oil products, downstream basic and derived chemicals, and create polluting substances in the process. A California-based energy company wanted to gauge the impact of air pollution on births. Visit this EPA site for more information on the management of air quality. Researchers collected data to examine the relationship between air pollutants and preterm births in Southern California. During the study air pollution levels were measured by air quality monitoring stations. Specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (PM10) in \\(\\mu/m^3\\). Length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that increased ambient PM10 and, to a lesser degree, CO concentrations may be associated with the occurrence of preterm births. In this study, identify the cases, the variables and their types, and the main research question. 3.3.1 Classifying data types A familiar example of a hierarchy is the organizational chart. This chart visualizes the reporting structure of an organization. You can access this article about hierarchy for more information. What is a hierarchy? show / hide A hierarchy is an analytical technique that takes a group of objects and asks two questions: How are the objects (nodes) related to one another (edges)? (just a network) What objects are parents (higher level) or children (lower level) of one another Notice that the departments and their children nodes are mutually exclusive of one another. In each department of science there are hierarchies and networks of metadata. Metadata are data about data. Statistics mirror life, and in life we find ourselves classifying the world around us to make some systematic sense of it. Here is a potential data classification hierarchy we might use with this research example. Let’s lead with this summary: nominal ordinal interval ratio categories: counts x order known x x differences between values x x add / substract values x x fractions and multiples of values x frame of reference / zero value x differentiation / clustering x x hierarchy / relational x x system x x At the bottom of the hierarchy are four typical, and quite inclusive, classes of data: Nominal (from the Latin nomen, name) label variables, without any reference to their quantitative content. This is first step in analysis: break down data into useful categories, assign labels, aggregate data according to the categories. Examples include any classification scheme, just like the data herarchy itself: gender, class section, resident county. Building on nominal categories we have *ordinal scales (from the Latin ordo, ordinis, order) that rank levels of a variable from lowest to highest. There is no significance to the distance between ordinal scale levels, only the order is important. We might order customer satisfaction from a low level to a high level on a scale of 1 to 5. We might rank our prefewrences for beverages in which we might prefer milk (an ordinal value of 5) to beer (an ordinal value of 4), and so on. One level is simply better than another. As with nominal categories, ordinal scales do not contain any information about how much more we might prefer milk to beer. Interval (from Latin intervallum, space between two walls) scales build both the order and the exact differences between the values. The classic example of an interval scale is earnings per share because the difference between each value is the same. For example, the difference between -1.20 and -1.19 EPS is a measurable 0.01 EPS in currency units per share of adjusted common stock, as is the difference between 0.01 and 0.02. Using interval scales we can measure entral tendency with mode, median, or mean; standard deviation can also be calculated. However, consider this: 1.12 EPS plus 1.12 EPS is 2.14 EPS. But the result does not necessarily mean that 2.14 EPS is simply twice as high as 1.12 since EPS can take on negative values. There is no such thing as no EPS. A zero EPS is not no EPS, it is just a measure of positive earnings less negative earnings. There is no baseline or frame of reference to compare EPS. Ratio scales (from Latin ratio, explanation) use a base case, zero, to compute comparable levels of a variable. The maximium of a ratio scale can be any positive value. The minimum is zero. This allows us to compare as a fraction or multiple one value against another, thus providing a componennt of an explanation of the relative value of a variable. Using the example of a range of EPS from -1.10 to 1.20, we see that this is clearly an interval variable range. It is not yet a ratio scale. We would have to transform the series to a new scale where -1.10 is zero. We can do this by subtracting 1.10 from all values of EPS ranging from -1.10 to 1.20. This yields a new scale from 0 to 1.10. Only in this way can we faithly compare one EPSA to another. Thus an EPS of -1.05 is only 0.05 higher than -1.10 or 0.05/1.10 = 4.55% higher than -1.10. Map the data in the research example to the hierarchy. show / hide Let’s classify the data in the the research example with this hierarchy. Cases: or “observations” include measurements relative to 143,196 births between the years 1989 and 1993 (Discrete - simply a convenient lexicographic index, not necessarily ordinal). Variables: air quality station (discrete - nominal), year (discrete - nominal) length of gestation (continuous - ratio - has a 0 starting basis), levels of carbon monoxide were recorded in parts per million (continuous - ratio), nitrogen dioxide and ozone in parts per hundred million (continuous - ratio), and coarse particulate matter (PM10) in \\(\\mu/m^3\\) (continuous - ratio). All continuous variables are based from zero to some positive number. There appear to be no continuous - interval measures that would allow negative numbers (e.g., temperature in degrees centigrade). What further questions might be possibly related to these measures? Do these questions anticipate the collection of discrete- ordinal variables? For example, are there preferences for living in one region versus another (ordinal)? How satisfied are respondents with air quality? Describe the usefulness of the components of your data classification hierarchy to answer the main research question above. The data classification hierarchy is a theory to be tested. A canon is a general law, rule, principle, or criterion by which something is judged. Let’s test this hierarchy against these canons of empirical method, expressed as questions for consideration (see and astrophysicist’s view of these canons, both based on Bernard Lonergan’s Insight, Longmans, London 1957, chapter III : Relevance and Selection: Does the hierarchy use terms and relationships that hint at attributes in the data in the example? Does the hierarchy anticipate, and possibly raise further questions about the data? Does the hierarchy select relevant data and de-select irrevelant data. sufficiency: If you use another node or relationship in the hierarchy, does that add anything new to the description that is relevant? Parsimony: is the hierarchical description of data itself verifiable in the data? Completeness: does this hierarchy account for all of the aspects you observe about data in this example? In other words, does the hierarchy somehow employ all of the data available, relevant, sufficient, and verifiable in this example. show / hide By canon: Relevance and Selection: Does the hierarchy use terms and relationships that hint at attributes in the data in the example? Does the hierarchy anticipate, and possibly raise further questions about the data? Does the hierarchy select relevant data and de-select irrevelant data. Yes it does. Anticipated are variables that are non-integer (as well as integer) and simply unordered categorical data. Other types are possible, but not present. Sufficiency: If you use another node or relationship in the hierarchy, does that add anything new to the description that is relevant? No it does not. It seems that the continuous and discontinuous numeric variables are sufficient for numeric variables if only because they are mutually exclusive. The same for nominal (unordered) and ordinal (ordered). If fact the hierarchy overfits the data in this exercise by the inclusion of discrete and ordinal data types. Including these anticipate relevancy and selection in other studies. Parsimony: is the hierarchical description of data itself verifiable in the data? Yes it is. Again our data seems well matched to the lowest level of the hierarchy. Further levels might delineate attributes of the previous level. For example, add another component, say integer. But integer is just another way of saying discrete variable. so this doesn’t add anything. Are there other categories of numeric or categorical data? There do not seem so. Completeness: does this hierarchy account for all of the aspects you observe about data in this example? In other words, does the hierarchy somehow employ all of the data available, relevant, sufficient, and verifiable in this example. Yes it does. The data in the example are continuous, discrete, nominal, and ordinal. There does not seem to be any other relevant data available within the field of this example. 3.4 Something a bit more complicated Electricity customers now have their choice of service providers. The service providers can offer fixed and flexible rates for electricity usage. Usage data can be collected using remote sensing technology. An example of usage data are the following text strings for a facility in New York City. index usage 1 3 PM Mon 24th-Mar-2014___0.384 kwhNA_R 2 5AM 15-Aug-2014___1.201 kwh5_C 3 8PM Thu 20-Mar-2014__1.523 kwh_4_C 4 6PM 23rd-Apr-2014___0.424 kwh_5_R 5 _1AM Friday 19th-Dec-2014___0.209 kwh_3_R 6 _5AM Tue 19th-Aug-2014___1.228 kwh_4_C All of the raw data is in a text string. We can parse the text into time of day, date, usage level, units of measurement, a quality rating (1-5), and a customer type ( R for residential and C for commercial) . Elements such as blank spaces and underscores (i.e., \"_\") would be eliminated as delimiters. Does the data type hierarchy still apply? show / hide Let’s modify the hierarchy a bit to include the text string raw data. Hover over any arrow line, then click on the line to move the entire network into position for better viewing. Even though the time “3 PM” be would be parsed into 15:00 hours (on a 24-hour clock), the “PM” string could be used as discrete data to mark the second 12 hours of a day. Both usage and time are ratio types since both would start at a zero point. We might argue that days from 1/1/1900 to the date in the data is discrete as might be hours ending 3PM. We have a mixture of categorical and numeric data in this data set. The data could be ordered by the time / data stamp when the observations of electrical usage were collected from meters. In this case we culd classify the data as a time series. One of the implications of this classification is that we might want to compare one time versus another. For example we could ask these questions: - Is morning usage different from nightly usage? - Are summar months’s usage different from winter months? - Is there a trend year to year? On the other hand we could view this data as a cross-section of usage sampled at verious times and classified by the type of electricity user or even by the ordinal quality of service series. In a cross-sectional context, we would view the data irrespective of time. However we can group the data into time buckets such as norming and night to aggregate customer usage. We can ask these questions of the data so arranged: - Are there different levels of usage among the types of customers? - Are qaulity of service levels different among the types of customers? 3.5 What have we learned? Why does any of this matter? The general question is bias and manipulation are possible if we fail to select all relevant, verifiable data that sufficiently describes and explains our research, decisions, and raises questions for further inquiry. - Systematic inquiry uses structures to name, associate, analyze, and conclude the properties of data - A proposed classification hierarchy anticipates the data used to analyze a pratical research problem - A test of the hierarchy using rules of empirical methodology confirms the validity of the classifications of the data and reaches into further questions for inquiry - A provisional structure helps us to identify and correct bias 3.6 Review these scenarios How can a production manager in a start-up summarize and describe to the board of directors the results of testing the life of a sample of 100 revolutionary light-weight organic batteries? show / hide There is too much raw data to show the board. The manager wants to summarize the results by displaying the average life and range of lives of the batteries. The manager might also divide the sample into low, medium, and high durations to illustrate the distribution of lives in the sample. Each of these is an example of descriptive statistics. Why would the manager want to infer that the sample is representative of the whole output of batteries? show / hide The manager can’t test the life of all of the batteries – that would destroy the output of the venture! Instead the manager might want to test the hypothesis that the average battery life is greater than, say, 1,000 hours, with a precision of 5% that the manager is wrong about this hypothesis. Identify the population and sample in this situation. show / hide The manager takes a sample of 100 batteries from a population that includes all of the venture’s output. The manager would like to describe the sample in a way that is probably representative of the output of the venture. The manager is putting together a testing database. What kind of data is the average life of the batteries? show / hide Since life is greater than or equal to zero, life data would numeric. Because I can always find a life value between any two consecutive life values, the data is continuous. What about the number of batteries in each location? show / hide All counts are integers and thus discrete. What if the batteries were produced in various locations. What kind of data might the locations be? show / hide Location is a classification and thus nominal. The manager asks experts to rate the reliability of the batteries. What kind of data are the ratings? show / hide Ratings rank in some order from bad to good, or unlikely to likely, or some other ordered range, and thus ordinal. 3.7 Try this Apply the data hierarchy to the county health outcome rankings and data base for states in the US. Find the types of data and apply the canons of empirical method to your findings. Choose Other measures &gt; socio-economic factors for your investigation. You can navigate to this site for an interactive view. Download this report Identify at least two examples each of nominal, ordinal, interval, and ratio variables. Upon reading this report, are appropriate data types reaching valid conclusions? For example, if a conclusion reached indicates that one segment of the sampled population is twice as large as the sampled population of another segment, are ratio measurement scales used? "],
["first-steps-in-exploring-data.html", "Chapter 4 First Steps in Exploring Data 4.1 Learning objectives 4.2 Business context 4.3 Bullets 4.4 Incidents and accidents 4.5 Where do we go from here? 4.6 Resources", " Chapter 4 First Steps in Exploring Data 4.1 Learning objectives Welcome to the first step in exploring data. In this unit you will Translate a business context into data Arrange the data to answer a business question Pose next steps in the analysis of the business context 4.2 Business context There are several parts of any context. We can arrange the contextual components into the who (including with and by whom), what, when, where, why, how, how often, how much categories of answers. Our new CFO is reviewing revenues for a start-up division in the company. For the past three years there have been low revenues averaging at most USD100 million, a high revenue of USD 300 was attained in a very busy quarter. But typically revenues ranged from USD100 to USD250 million. She anticipates that this movement of revenue will continue into the next budget year. This year’s budget is set at USD260 million. She has three questions for her revenue team: Are revenues on target? Is the target and the YTD revenue pushing the market and our expectations? What can we do to manage our revenue? Her team builds a graphic to walk her through a discussion. 4.3 Bullets The first bullet component is a background heat map that depicts ranges of the metric, in this case USD revenues for a particular division. Let’s add a target revenue on this heat map. Now we have layered a revenue expectation on top of, or overlaying, the ranges of anticipated revenues. Let’s now track year to date (YTD) USD revenues by overlaying a bar. 4.3.1 What did we do? Which bullet graph component depicts expectations of future revenue?\", Blue status bar Black target Three-color heat map No component provides this information. Which bullet graph component depicts bedget requirements? Blue status bar Black target Three-color heat map No component provides this information. Which bullet graph component tracks current levels of the metric? Blue status bar Black target Three-color heat map No component provides this information. show / hide The answers in order: 3. Three-color heat map, 2. Black target, 1. Blue status bar. 4.3.2 What can we say? How will the bullet graph help answer the CFO’s questions? Revenues are not on target. Revenues are within the “middling” level of revenues anticipated. There is obviously more work to do to raise the revenue level, alter anticipations, modify targets. 4.4 Incidents and accidents The security officer of a construction contractor tracks unauthorized access into physical and cyber systems. Here is some summary data from a recent day. All the So wants to know is if incidents are at high enough levels to enable countermeasures. measure high mean low target value Total Events (%) 100 45 25 55 50 Security Events (%) 100 40 20 40 45 Filtered (%) 100 50 10 45 60 Tickets (%) 100 30 5 35 25 All data has been normalized into percentages. This helps the SO compare categories of measures. Total Events include all reports of potential unauthorized access. Security Events are a subset of the total that qualify as critical physical or cyber system issues defined in the organization’s security policy. Filtered events are those that are flagged as highly risky. Tickets are the events that warrant immediate resolution. The SO prepares a bullet graph to summarize this table. What can we observe? show / hide In the end, ticketed events are still in a relatively safe zone. However, filtered events far exceed targets, perhaps reflecting the high levels of security events according to the security policy. The organization does appear to be under attack. 4.5 Where do we go from here? The CFO can make some judgments that revenue has not eaven reached the high revenue region yet, let alone the target. Next steps might include asking questions about drivers of revenue, the associations those drivers have with underlying causes, and inferences about how close the division can get to a target in the time alotted to the budget. These considerations will raise the important issue of how often revenues are low, medium, and high. That discussion needs another graphic: the frequency distribution. 4.6 Resources Stephen Few’s original post that explains the rationale and use of bullet graphs Here is an Excel version of the bullet graphs above with directions to build your own graphs. This blog uses a very similar approach to the Excel spread sheet above Tableau, a popular analytics platform, has this quick start guide to making a bullet chart. You can download a public (free) version of Tableau here. "],
["tendency.html", "Chapter 5 Tendency 5.1 Learning outcomes 5.2 The best we have 5.3 Procedures 5.4 Always problems 5.5 Visualizing with Tukey’s box 5.6 What have we gotten to so far?", " Chapter 5 Tendency 5.1 Learning outcomes In this unit you will learn to: Build a model for which you can derive a simple, but optimized, estimator of a measure of central tendency Using a frequency distribution approach calculate additional measures of aggregate position of data Compare, contrast various measures of tendency By tendency we mean how data elements might aggregate, accumulate, even congregate around or near a particular data point. Elementary examples of this measure are the arithmetic mean and the median. More sophisticated measures of position include quantiles, with quartiles as a special case, and the frequency-weighted average of grouped data. Position measures help us gain insight into trends, beliefs, and upper and lower limits of decision drivers. Bwecause they are aggregates, they necessarily abstract from the individual data points themselves. The measures do ehlp us understand teh systematic movement of a stream of data. But they also indicate how far they are away from any given piece of data. This distance is something we will exploit now and, and in the next installment even more so. 5.2 The best we have All statistics is born in two simple ideas: There either is or is not a sytematic pattern, an aggregation, a trend in the data Individual data do not sytematically deviate from this pattern or trend. Let’s consider this sample of 5 observations of car prices at a recent auction in New Jersey: price 12500 13350 14600 15750 17500 Suppose that as you cross the GW Bridge from the Bonx into New Jersey, you hear an advertisement on the radio proclaiming that, at the very auction you are going to, the average car price is $14,000. Let’s let \\(X_i\\) be the series of \\(i-1\\dots5\\) prices and \\(a=14000\\) be the advertiser’s estimate of the average, trend, or belief in what the car price is. Here is table of how prices deviate from the advertiser’s announcement. price deviation 12500 -1500 13350 -650 14600 600 15750 1750 17500 3500 Of course there may be many such beliefs about the average price and thus many different possible deviations. Our job is to find the average that is best in a very particular sense. Here is our model. \\[ Y_i = m + e_i \\] We think (really we suppose or hypothesize) that each and every price, \\(Y_i\\) is composed of a systematic constant price \\(m\\) plus an unsystematic error or deviation from the average, called here \\(e_i\\). This might be because we do not know enough about auctions, used cars, whatever, to think there might be a systematic factor that might influence car price \\(Y_i\\). Each of the hanging error bars from the blue points to the red line represent deviations of price from a supposed average \\(a=\\) $14,000. Is this the best average we can come up with? Let’s systematically think through this. There are three usual suspects for getting at a best average in this situation. We can try to find the average \\(m\\) that minimizes The sum of deviations or errors: \\(\\Sigma_{i=1}^5(Y_i-m)\\) The sum of squared deviations or errors (SSE): \\(\\Sigma_{i=1}^5(Y_i - m)^2\\) The sum of absolute deviations: \\(|\\Sigma_{i=1}^5(Y_i - m)|\\) There are many criteria we could choose. Which one(s) might work best? show / hide Simply summing the errors and trying to find the best \\(m\\) never seems to work because the sum of errors always must be zero when \\(m\\) is the mean. Try it! The sum of squared deviations from the mean also allows us to take a first derivative and use first order conditions for a minimum. These conditions, in this situation, reduce the problem to the solution of a first-order linear equation in one unknown, \\(m\\). To use mean absolute deviation we would need to employ the simplex method from linear programming. On top of that one of the best LP formulations is to use goal programming. A simple exercise will stretch your calculus muscles by calculating first order conditions for a minimum. If you do not have such musculature available, just sit back and watch the show. 5.2.1 Square those errors This plot depicts the sum of squared deviations for a grid of potential values of what the data points deviate from, \\(m\\). Use of such a criterion allows us a clear and in this case unique calculation of the best linear estimator for the mean. Hover over the graph and brush over the area around the red dot to zoom in. What do we see? show / hide Simply putting the cursor on the red dot indicates a solution: \\(m=\\) 14741. A bit of calculus confirms the brute force choice of the arithmetic mean that minimizes the sum of squared deivations about the mean. First, the sum of squared errors (deviations) of the \\(X_i\\) data points about a mean of \\(m\\) is \\[ SSE = \\Sigma_{i=1}^5 (Y_i - m)^2 \\] Second, we derive the first derivative of \\(SSE\\) with reapect to \\(m\\), holding all else (e.g., sums of \\(X_i\\)) and set the derivative equal to zero for the first order condition for an optimum. \\[ \\frac{d\\,\\,SSE}{dm} = -2\\left(\\Sigma_{i=1}^5 (Y_i - m)\\right) = 0 \\] Here we used the chain and power rules of differentiation. Third, we solve for \\(m\\) to find \\[ m = \\frac{\\Sigma_{i=1}^5 Y_i}{N}=14740 \\] Close enough for us? This is none other than the arithmetic mean. We will perform a very similar procedure to get the sample means of the y-intercept \\(b_0\\) and slope \\(b_1\\) of the relationship \\[ Y_i = b_0 + b_1 X_i + e_i \\] where \\(x_i\\) data points try tp explain movements in the \\(Y_i\\) data points. 5.2.2 Absolutely! Even more interesting is the idea we can find a middling measure that minimizes the sum of absolute deviations of data around this metric (too many \\(m\\)!). \\[ SAD = \\Sigma_{i=1}^5 |Y_i - m| \\] Yes, it is SAD, the sum of abssolute deviations. This is our foray into rank-oder statistics, quite a bit different in nature than the arithmetic mean of \\(SSE\\) fame. We get to basic counting when we try to mind the \\(m\\) that minimizes SAD. To illustrate this suppose our data is all positive (ratio data in fact). If \\(m=5\\) then the function \\[ f(Y;m) = |Y-m| \\] has this appearance, the so-called check function. Intuitively, half the graph seems to be the left of \\(m=5\\), the other have is to the right. Let’s look at the first derivative of the check function with respect to changes in \\(m\\), just like we did with each term in \\(SSE\\). Notice that the (eyeballed) rise over run, i..e., slope, before \\(m=5\\) is -1, and after it is +1. At \\(m=5\\) there is no slope that’s even meaningful. We have two cases to consider. First \\(Y\\) can be less than or equal to \\(m\\) so that \\(Y-m \\leq 0\\). In this case \\[ \\frac{d\\,\\,(Y-m)}{dY} = -1 \\] This corresponds exactly to negatively sloped line rolling into our supposed \\(m=5\\) in the plot. Second, \\(Y\\) can be greater than or equal to \\(m\\) so that \\(Y-m \\geq 0\\). In this case \\[ \\frac{d\\,\\,(Y-m)}{dY} = +1 \\] also correpsonding to the positively sloped portion of the graph. Another graph is in order to imagine this derivative. It’s all or nothing for the derivative, a classic step function. We use this fact in the following (near) finale in our search for \\(m\\). Back to \\(SAD\\). We are looking for the \\(m\\) that minimizes \\(SAD\\): \\[ SAD = \\Sigma_{i=1}^N |Y_i - m| = |Y_1-m| + \\ldots + |Y_N-m| \\] If we take the derivative of \\(SAD\\) with respect to \\(Y\\) data points, we get \\(N\\) minus 1s and \\(N\\) plus ones in our sum because each and every \\(|Y_i-m|\\) could either be greater than or equal to \\(m\\) or less than or equal to \\(m\\), we just just don’t know which, so we need to consider both cases at once. We also don’t know off hand how many data points are to the left or the right of the value of \\(m\\) that minimizes \\(SAD\\)! Let’s play a little roulette and let \\(L\\) be the number of (unknown) points to the left of \\(m\\) and \\(R\\) points to the right. Then \\(SAD\\) looks like it is split into two terms, just like the two intervals leading up to and away from the red dot at the bottom of the check function. \\[ SAD = \\Sigma_{i=1}^R |Y_i - m| + \\Sigma_{i=1}^R |Y_i - m| = (|Y_1-m| + \\ldots + |Y_L-m|) + (|Y_1-m| + \\ldots + |Y_R-m|) \\] \\[ \\frac{d\\,\\,SAD}{dY} = \\Sigma_{i=1}^L (-1) + \\Sigma_{i=1}^R (+1) = (-1)L+ (+1)R \\] When we set this result to zero for the first order condition for an optimum we get a possibly strange, but appropriate result. The tradeoff between left and right must offset one another exactly. \\[ (-1)L + (+1)R = 0 \\] \\[ L = R \\] Whatever number of points are to the left must also be to the right of \\(m\\). If \\(L\\) points also include \\(m\\), then \\(L/N\\geq1/2\\) as well as for the \\(R\\) points if they include \\(m\\) so that \\(R/N\\geq1/2\\). We have arrived at what a median is. Now we come up with a precise statement of the middle of a data series, the notorious median. We let \\(P()\\) be the proportion of data points at and above (if \\(Y \\geq M\\)) or at and below (\\(Y \\leq m\\)). THe median, \\(m\\), is the first time a data point in a data series reaches both \\(P(Y \\leq m) \\geq 1/2\\) (from minimum data point) and \\(P(Y \\geq m) \\geq 1/2\\) (from the maximum data point) That definition will work for us whether each data point is equally likely (\\(1/N\\)) or from grouped data with symmetric or skewed relative frequency distributions. Two cases arise: Even number of data points. So if \\(N=10\\), the only way that can happen is if there are 5.5 points to the left of \\(m\\) and 5.5 points to the right. Yes, the value \\(m\\) is halfway between data point number 5 and data point number 6. Odd number of data points. If \\(N=9\\) data points, the only way that can happen is if there are 5 points to the left of \\(m\\), including \\(m\\), and 5 points to the right, also including \\(m\\). Yes, the value \\(m\\) is data point number 5. Thus the complexities of order statistics obliterate calm composure. So what about our odd number of price data points? show / hide Each price data point represents an equal proportion of the total number of data points. The proportion is \\(1/5 = 0.2\\) of the data. i price proportion min-to-max max-to-min 1 12500 0.2 0.2 1.0 2 13350 0.2 0.4 0.8 3 14600 0.2 0.6 0.6 4 15750 0.2 0.8 0.4 5 17500 0.2 1.0 0.2 The first data point that accumulates a propportion of the data greater than or equal to 1/2, starting from the minimum, is data point 3, with price of 14600. The first data point that accumulates a proportion of the data greater than or equal to 1/2, starting from the maximum, is also data point 3, with price of 14600. The movement up from the minimum and down from the maximum price agreed on one data point. That will always happen for data sets with odd numbers of data points. What if there is an even number of data points? Add a price of d$18000 and let’s see. show / hide Each price data point represents an equal proportion of the total number of data points. The proportion is now is \\(1/6 = 0.17\\) of the data. i price proportion min-to-max max-to-min 1 12500 0.17 0.17 1.00 2 13350 0.17 0.33 0.83 3 14600 0.17 0.50 0.67 4 15750 0.17 0.67 0.50 5 17500 0.17 0.83 0.33 6 18000 0.17 1.00 0.17 5.2.3 Quantiles anyone? We can use the same approach to finding those data points that correspond to the 25th quantile, otherwise known as the first quartile \\(Q1\\). Instead of 1/2 to the left or right of the median data point, we go 1/4 to the left (including the Q1 we search for), which means to the right there is \\(1-1/4=3/4\\) of the data to the right (and including \\(Q\\)) of the quartile. Does this work? (it better!) show / hide i price proportion min-to-max max-to-min 1 12500 0.17 0.17 1.00 2 13350 0.17 0.33 0.83 3 14600 0.17 0.50 0.67 4 15750 0.17 0.67 0.50 5 17500 0.17 0.83 0.33 6 18000 0.17 1.00 0.17 It does! The first data point that accumulates a propportion of the data greater than or equal to 1/4 (25%), starting from the minimum, is data point 2, with price of 13350. The first data point that accumulates a proportion of the data greater than or equal to 3/4 (75%), starting from the maximum, is also data point 2, with price of 13350. Thus \\[ Q1 = 13350 \\] And cleanly at that. By symmetry then data point 5 must be \\(Q3\\), the 75th quartile with price 17500. Now let’s try something really daunting. Suppose that the 6 prices occur with relative frequencies (in ascending order) of 0.1, 0.4, 0.2, 0.1, 0.1, 0.1, What happens now? Will our approach still work? Let’s this time find the 0.40 quantile so that 40% of the data is at or below this point and 60% of the data is at or above this point. show / hide We will only need the cumulative proportions from the minimum data point for this one. i price proportion min-to-max 1 12500 0.1 0.1 2 13350 0.4 0.5 3 14600 0.2 0.7 4 15750 0.1 0.8 5 17500 0.1 0.9 6 18000 0.1 1.0 The first data point that accumulates a propportion of the data greater than or equal to 40%, starting from the minimum, is data point 2, with price of 13350. We know that 40% is just 10% below the 50% accumulation at data point 2. We can interpolate a value somewhere, closer to 13350 than to 12500, that is the 40% mark. Moving from 10% at data point 1 to 50% at data point 2, we see that 40% must be 3/4s (10% + (10% times 3) = 40% of the way from data point 1, or just 1/4 of the way below data point 2 on its way to data point 1. Let’s try that second idea. there is a distance of 850 from data points 1 to 2. A quarter of that is 212.5. Let’s subtrract this amount from data point 2 to get our interpolated (linearly!) 40% quantile of 13137.5. All of this occurs at that mythical and fractional index value of 1.75, thre quanters of the way from index value 1 to index value 2. Yet another way of complicating our otherwise calm and cool composures. We have entered the world of {singular statistics}(http://watanabe-www.math.dis.titech.ac.jp/users/swatanab/e-manga.html), where we are otherwise sucked into a vortex. An exxample of that vortex is the check function we just used. It is a statistical singularity like the physical black holes in the cosmos. It ends up that a lot of the statistics practically done (right and well) uses the thinking and techniques behind our determination of the singular median. 5.3 Procedures Let’s summarize some basic procedures for calculating position and tendency (central or otherwise): Mean: arithmetic mean and weighted mean (or average as some like to call it) Median: another quartile? (of course) Mode: good for nominal data Quantile: percentile, and a special subset of quantile, the quartile, and the median too 5.3.1 Mean If \\(Y_i\\) is all of the data possible in the universe (population) indexed by \\(i = 1 \\dots N\\) with \\(N\\) elements, then the arithmetic mean is the well-known (and just derived through calculus): \\[ \\mu = \\frac{\\Sigma_{i=1}^N Y_i}{N} \\] If \\(Y_i\\) is a sample (subset) indexed by \\(i = 1 \\dots N\\) with \\(N\\) elements from the population, then (the same formula!) \\[ \\bar{Y} = m = \\frac{\\Sigma_{i=1}^N Y_i}{N} \\] We use the \\(\\bar{}\\) over the \\(Y\\) to indicate a sample mean. The arithmetic mean assumes that all the observations \\(Y_i\\) are equally important. Why? show / hide Each observation is weighted by \\(1/N\\) where \\(N\\) is the number of observations. If \\(N=5\\) as with the car auction prices, then each observation contributes \\(1/5=20\\)% to the overall average. Let \\(f_i\\) be the frequency (count) of each observation (could be grouped into bins as well). Then the weighted mean (or average) is \\[ m = \\Sigma_{i=1}^N\\left(\\frac{f_i}{N}\\right)Y_i \\] Here \\(f_i/N\\) is the relative frequency of observation \\(Y_i\\). Aren’t the arithmetic mean and weighted mean really equivalent? show / hide They are equivalent only if each and every \\(f_i=1\\) so that the mean is then \\[ m = \\Sigma_{i=1}^N\\left(\\frac{1}{N}\\right)Y_i = \\bar{Y} = \\frac{\\Sigma_{i=1}^N Y_i}{N} \\] Each observation is weighted by \\(1/N\\) where \\(N\\) is the number of observations. 5.3.2 Median The middle of the data. We can use the Percentile method below with \\(P = 50\\). 5.3.3 Mode The most frequently occurring value in the data. 5.3.4 Percentile and quantile How to compute? Organize the numbers into an ascending-order array. Calculate the percentile location \\(i\\) \\[ i = \\frac{P}{100}N \\] where \\(P\\) = the percentile of interest \\(i\\) = percentile location \\(N\\) = number of elements in the data set Determine the location by either (a) or (b). If \\(i\\) is a whole number, the \\(P\\)th percentile is the average of the value at the \\(i\\)th location and the value at the \\((i + 1)\\)st location. If \\(i\\) is not a whole number, the \\(P\\)th percentile value is located at the whole number part of \\(i + 1\\). For example, suppose you want to determine the 80th percentile of 1240 numbers. \\(P\\) is 80 and \\(N\\) is 1240. Order the numbers from lowest to highest. Calculate the location of the 80th percentile. \\[ i = \\frac{80}{100}(1240) = 992 \\] Because \\(i = 992\\) is a whole number, follow the directions in step 3a. The 80th percentile is the average of the 992nd number and the 993rd number. 5.4 Always problems Determine the arithmetic mean, median, and the mode for the following numbers. 2,4,8,4,6,2,7,8,4,3,8,9,4,3,5 show / hide Answer The arithmetic mean is \\(66/15=4.4\\). Both median and mode happen to be 4: Arrange in ascending order: 2, 2, 3, 3, 4, 4, 4, 4, 5, 6, 7, 8, 8, 8, 9 There are 15 terms. Since there are an odd number of terms, the median is the middle number. Using the percentile formula, the median is located at the \\((N + 1)/2 = (15+1)/2 = 16/2=8\\), the 8th data point. The 8th data point is 4 3, For the mode 2, 2, 3, 3, 4, 4, 4, 4, 5, 6, 7, 8, 8, 8, 9 The mode = 4, the most frequently occurring value The 2018 list of the 15 largest banks in the world by assets is in this survey. Compute the median and the mean assets from this group. Which of these two measures do you think is most appropriate for summarizing these data and why? What is the value of Q2 and Q3? Determine the 63rd percentile for the data. Determine the 29th percentile for the data. Build an error bar graph around the 75th quantile (also the third quartile). bank total.assets Industrial &amp; Commercial Bank of China (ICBC) 3452 China Construction Bank Corp. 2819 Agricultural Bank of China 2716 HSBC Holdings 2670 JPMorgan Chase &amp; Co. 2600 Bank of China 2584 BNP Paribas 2527 Mitsubishi UFJ Financial Group 2337 Credit Agricole Group 2144 Barclays PLC 2114 Bank of America 2105 Deutsche Bank 2078 Citigroup, Inc. 1843 Japan Post Bank 1736 Wells Fargo 1687 show / hide Answer Mean : \\[ m = \\frac{35412}{15} = 2360.8 \\] The median is located at the 8th observation, \\(Q_2 = 2337\\) First, \\(Q_3 = P_{75}\\) For \\(Q_3\\), \\(i=75/100 (15)=11.25\\), then \\(Q_3\\) is located at the \\(11 + 1 = 12\\)th observation, \\(Q_3 = 2670\\) \\(P_{63}\\) is located at the \\(9 + 1 = 10\\)th observation \\(P_{63} = 2584\\) \\(P_{29}\\) is located at the \\(4 + 1 = 5\\)th observation, \\(P_{29} = 2105\\) This graph seems to show a threshold that divides banks into two groups: a large group of smaller banks below the line and very few very large banks above the line. The largest bank is clearly head and shoulders larger than the next largest bank. Based on total assets 25% of all the top banks are well above $2500 billion. 5.5 Visualizing with Tukey’s box In 1977 John Tukey introduced the box-and-whisker plot or if you want to practice your French the boite a moustacke. Like a bullet graph, the box plot visualizes several aspects of data using a box. Here we imagine a vertical rectangle: The 75th percentile is on the top of the box The 25th percentile is on the bottom of the box The 505h percentile is in the middle of the box somewhere Outliers, inclusing the maximum and minimun data points are on lines that extend from the top and bottom of box and are called whiskers (c’est a dire, moustache) Draw the box and whiskers plot for the bank asset data. show / hide Your drawing should look something like this. hover over the plot to view the relevant statistics. Or it can be arranged horizontally like this. 5.6 What have we gotten to so far? We seem to have hit all of the learning outcomes: We did build a (very naive) model of a variable, car auction price, and recognized a systematic pattern (average) and an unsystematic residue (error or deviation from the average) in the data with the model (average). We did use a frequency distribution approach throughout. Where? We interpreted the arithmetic average as a weighted average where the weights (frequencies or counts) are all equal. We also built out percentiles, which is just the ogive curve. Did we compare and contrast? A bit. But we need to do more. The error bar chart is in the right direction. It indicates thresholds for clustering data at the very least. That will jump out in the next installment: deviations. The box plot is another devise to view the relative position statistics. It will pop up again the next installment as well. "],
["deviation.html", "Chapter 6 Deviation 6.1 Learning outcomes 6.2 Rev up the deuce 6.3 Blinded by the light 6.4 The tails have it! 6.5 How can we use all of this? 6.6 Problems, always problems 6.7 What have we learned?", " Chapter 6 Deviation 6.1 Learning outcomes Using various measures of deviation from a tendency, calculate average scales of data Identify the applicability of each deviation measure to answer business questions. Formulate and explanation an interpretation of deviation measure results to answer business questions. Deviation, distance, variation, scale, redisual, and even error are very similar concepts. They form the backbone of statistics because they measure how far away actual data is from our model, our supposition, our hypothesis, our belief of where we would like to think the data is. And of course there are lot ways to compute distance and deviation. We just computed several locations, trends, averages, percentiles, even quartiles with a box! Each of these are our peculiar mental image of where we think the data ought to be, at least most of the time. The rest of the time data is, where data happens to be when we stumble on it. Getting to an understanding of scale and distance and deviation we had to form a belief, calculate an average, build a trend. Now we move on to a more complete story of any particular data stream. 6.2 Rev up the deuce This is where we will start, again. This time we are going to mix it up a bit. We are back at our car auction in New Jersey. We have heard enough about price and now we want to know why price might vary. One explanation from our far-reaching understanding of economic choice is that we might be buying car quality. One measure of quality is how many miles the previoius owner drove the car. We read odometers to get that data. We now have two sets (vectors) of data. Each observation is a pair of two observations for each car: price and miles. i price miles 1 12500 43521 2 13350 31002 3 14600 18868 4 15750 12339 5 17500 9997 We can set up three ways of thinking about our car data: Price (\\(Y_i\\)) variations on their own: \\(Y_i=\\bar{Y}+e_i\\) Miles (\\(X_i\\)) variations on their own: \\(X_i=\\bar{X}+e_i\\) Price dependency on miles: \\(Y_i=b_0+b_1X_i+e_i\\) To put all of these models together on one graph we can plot each of the 5 points on a scatter plot like so: Let’s look at the Y-axis, price first and draw a horizontal line to depict the arithmetic average of price (\\(Y_i\\)). Then draw error bars from each data point to this line all in the Y-direction. This so looks like the error bar plot we generated before where now miles seems to act like the observations index. Try to draw a similar plot of the X-axis error bars around the arithmetic mean of miles. show / hide We just had to rotate our thnking by 90 degrees clockwise, that’s all. Hover over the virtical average lilne to see the mean of miles as X_bar. Now let’s layer one graph on the other. This will get a little dense, but will not so badly illustrate the interaction of price and miles. We not have price and miles in the cross-hairs and begin to see not just the individual variations of price and miles about their respective means, but can begin to visualize the co-variation of price and miles. To illustrate model 3, price dependency on miles: \\(Y_i=b_0+b_1X_i+e_i\\), let’s draw the best line we can through the scatter, one that will minimize squared deviations of price about our odometer-inspired model of price. Let’s leave that calculation till later. Willingly we will suspend any disbelief about that calculation. Let’s instead believe the numbers to be true. Let’s also keep the cross-hairs. Here we go. hover over the dark blue error bars that connect data pairs of miles and price to a new average, a downward-sloping straight (linear that is) line. If we were to be told that the average slope is -0.1314, what would we think the average Y-intercept is? show / hide Our new average price, \\(\\bar{Y}\\), is an estimate of the average of \\(Y\\) conditional on miles, \\(X\\) and its average, still \\(\\bar{X}\\), is \\[ \\bar{Y} = b_0 + b_1 \\bar{X} \\] We know that \\(\\bar{Y}=14740\\), \\(\\bar{X}=23145.4\\), and have just been informed that the slope parameter \\(b_1=-0.1314\\). So we plug (that is substitute) these numbers into the formula to get this. \\[ 14740=b_0-0.1314(23145.4) \\] We now solve for the Y-intercept \\(b_0\\) to find this result. \\[ b_0=14740+0.1314\\bar{X}=17782.4396 \\] Phew!, but not so bad, just one equation in one unknown. We now have jumped into hyper-space (at least 2-space) to expand our consciousness from univariate to bivariate relationships. Our next stop is to use these and other versions of deviations to get a handle on the scale, range, variation, deviation, and yes, error in the data, at least with respect to our beliefs about the data. We just expanded our beliefs from univariate to multivariate. When we get to multivariate we will look at how variations relate to one another, in our case bivariate variations in price relating to variations in miles and vice-versa. 6.3 Blinded by the light 6.3.1 What’s a sample? (again) Let’s recall what a sample is. It is a random draw from a larger group of data called a population. The word random derives from the Frankish (Germanic language from a while back) word rant much like our word rant and eventually meaning (by about 1880) simply indiscriminate. Our sample of auction prices is whatever we could get our hands on at the time of the sampling, thus a sort of random draw from the population of all auctioned cars. This begins to allow us to look at the error bars we generated as random deviations from the mean. Let’s also recall that this drawing of the random sample is the first analytical step after identifying a business question and a population from which to sample. So we have a sample of two univariate data series. We have also thought that is is reasonable to relate the two series together. Let’s now find some measures of scale, deviation, and variation for each of the univariate series. 6.3.2 1. Standard deviation If \\(x\\) is a sample (subset) indexed by \\(i = 1 \\dots N\\) with \\(N\\) elements from the population, then we already know that \\[ \\bar{X} = \\frac{\\Sigma_{i=1}^N X_i}{N} \\] Our model of deviations from the mean produced error terms \\(e_i=X_i-\\bar{X}\\), which we assigned to the miles univariate data series. If we were to add up these deviations like this \\[ \\Sigma_{i=1}^{5}(X_i-\\bar{X}) \\] what would we get? show / hide ZERO. Right, this aggregation of deviations will always give us zero, by definition. Again, like we did to find the best average, let’s calculate the sum of squared errors, the \\(SSE\\). Because we sampled the data, the average \\(SSE\\) will have to be divided by \\(N-1=5-1=4\\), called, for the moment, the number of degrees of freedom. This will produce an unbiased measure, another topic for another time! \\[ s^2 = \\frac{\\Sigma_{i=1}^N (x_i - \\bar{x})^2}{N-1} \\] The squared measure \\(s^2\\) is officially called the variance. It’s square root \\[ s = \\sqrt{s^2} \\] is the standard deviation. THe notion of standard is that of an average. We already know what a deviation is. Let’s build a table of four columns to calculate this measure of scale, deviation, error, and variation. show / hide i miles deviation deviation squared 1 43521 20376 415165075 2 31002 7857 61726164 3 18868 -4277 18296151 4 12339 -10806 116778281 5 9997 -13148 172880423 Some really big numbers emerge. That is typical and we often try to scale these down when performing computations, again a topic for later. Let’s sum up the deviations again (just to prove they add up to zero) and the squared deviations (\\(SSE\\)) \\[ var(miles)=s^2=\\frac{784846093.2}{5 - 1}=196211523.3 \\] and then \\[ s=\\sqrt{variance}=\\sqrt{196211523.3}=14007.5524 \\] What about the standard deviation of price? show / hide We should have gotten 1975.2848 6.3.3 2. Robust measures Range: the distance between the max and min \\[ Range = max(x) - min(x) \\] Interquartile Range, IQR: \\(Q_3\\) net of \\(Q_1\\) (\\(P_{75} - P_{25}\\)) gives us a robust view like that in Tukey’s (1977) box plot. What is the IQR of miles and price? show / hide We should have gotten \\[ IQR_{miles}=31002-12339=18663 \\] and \\[ IQR_{price}=15750-13350=2400 \\] Wider, broader scale than the standard deviations? This measure is robust to highly skewed distributions with thick tails. Mean Absolute Deviation: MAD is robust to outliers. \\[ MAD = \\frac{\\Sigma_{i=1}^N |X_i - \\mu|}{N} \\] Let’s compute this statistic for price and miles. show / hide This time we should have gotten for \\(X_i= miles_i\\): \\[ MAD_{miles} = \\frac{\\Sigma_{i=1}^N |X_i - \\mu|}{N} = \\frac{56464.4}{5} = 11292.88 \\] 6.3.4 3. Correlation Correlation measures the degree of relationship between two variables. The measure ranges from a low of -1 to a high of +1. The -1 is a perfectly correlated inverse relationship between two variables. The +1 correlation measure a perfectly possitive relationship between two variables. A 0 indicates no relationship seems to exist. This is not cause and effect, just two variables happening to bump together or not in the street one day in the Bronx. We otherwise call this an antecedent-consequent relationship. 6.3.4.1 Three steps to a correlation. Calculate the covariance between two variables, \\(X_i\\) and \\(Y_i\\). \\[ cov(X, Y) = s_{xy} = \\frac{\\Sigma_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y})}{N-1} \\] The numerative sums up the pairwaise ups and downs of how \\(X\\) varies with \\(Y\\). This number may net out to positive, negative, or just very close to zero. We lose at least one degree of freedom because we have to use the arithmetic mean to calculate deviations, just like we did for the sample standard deviation. Calculate the standard deviations for each of the two variables, \\(X_i\\) and \\(Y_i\\). First, the variance, here illustrated for \\(X\\), miles in our example. \\[ var(X) = s_{x}^2 = \\frac{\\Sigma_{i=1}^N(X_i-\\bar{X})^2}{N-1} = \\frac{784846093.2}{5} = 196211523.3 \\] \\[ s_X = \\sqrt{var(X)} = 14007.5524 \\] and for \\(Y\\) as price we have \\[ s_Y = 1975.2848 \\] Calculate the ratio of covariance to the product of the standard deviations. This step transform the unwieldly, and hard to interpret covariance from the \\(-\\infty...+\\infty\\) to the \\(-1...+1\\) range. For samples we call the correlation \\(r_{XY}\\). \\[ r_{XY} = \\frac{cov(X,Y)}{s_x s_y} = \\frac{-25791807.5}{(14007.5524)(1975.2848) } = -0.9322 \\] Let’s build a table and calculate away. show / hide First a table. i X=miles Y=price (X-X_bar) (Y-Y_bar) (X-X_bar)(Y-Y_bar) 1 43521 12500 20376 -2240 -45641344 2 31002 13350 7857 -1390 -10920674 3 18868 14600 -4277 -140 598836 4 12339 15750 -10806 1010 -10914464 5 9997 17500 -13148 2760 -36289584 Summing up the cross deviation terms \\((X_i-\\bar{X})(Y_i-\\bar{Y})\\) we get \\[ \\Sigma_{i=1}^5 (X_i-\\bar{X})(Y_i-\\bar{Y}) = -103167230 \\] Dividing by the degrees of freedom \\(N-1=5-1=4\\) we then have the sample covariance \\[ cov(X, Y) = \\frac{\\Sigma_{i=1}^5 (X_i-\\bar{X})(Y_i-\\bar{Y})}{N-1} = -25791807.5 \\] We already have the standard deviations of miles, 14007.5524, and price, 1975.2848, so now we can compute the sample correlation as \\[ r_{XY} = r_{XY} = \\frac{cov(X,Y)}{s_x s_y} = -0.9322 \\] 6.3.4.2 What about the slope? We will show later (yes, with calculus – free of charge) that the slope parameter \\(b_1\\) is just the ratio of the covariance of miles (\\(X\\)) and price (\\(Y\\)) to the variance of miles (\\(X\\). Should we try it? show / hide \\[ b_1 = \\frac{cov(X,Y)}{var(X)} = \\frac{-25791807.5}{196211523.3} = -0.1314 \\] Negatively sloped, reflecting the negative correlation: when variations in miles are positive, variations in prices are negative, on average in this sample. If we find a car at the auction with an odometer reading of 15010 miles. What price would we expect based on our model? show / hide Sure. We found that if we knew \\(b_1=-0.1314\\), then we could calculate at the cross-hairs of average \\(X\\) and \\(Y\\) that \\(b_0=17782.4396\\). We substitute this into our model to get \\[ price = b_0 + b_1\\,\\,miles = 17782.4396 + (-0.1314)(15010) = 15810.1256 \\] Would we be willing to pay that amount? We might want to look at other factors such as the age and condition of the car, among many other features of our auction in New Jersey. 6.4 The tails have it! To round out our discussion about the shape of data, we ask two more questions: What direction do the tails of distribution tend to go, to the right or the left? How thick are the tails? The first question gets at the asymmetry in lots of data. An example of this is this data on losses from trading common stock in solar ppwer companies. We gather several days of stock prices, then comppute returns as percentage changes in the daily prices. A loss is whenever the returns are negative. Suppose that the latest price per share is USD 45.01 and we have 100,000 shares. Let’s eyeball answers to the questions. show / hide What direction does the distribution tend to? This data looks like the skewness is to the right. There is a preponderance of observations in the body to the left of the 100000 mark. The distirbution therefore looks like it is right- or positively-skewed. How thick tailed is the distribution? The losses seem to be somewhat frequent in the tails. It looks like it might be tick tailed. In financial terms this means that the volatility (standard deviation, IQR) is itself volatile. The answer to these questions can also come from two more aggregations, both based on deviations and variation of data points from thwir means. Skewness needs a direction so a positive metric will mean deviations can be found in the positive tail on average, while a negative metric will indicate a net average long tail in the opposite diredtion. Here is a metric that does this: \\[ skew = \\frac{\\frac{\\Sigma_{i=1}^N (X_i - m)^3}{N-1}}{s^3} \\] Just like correlation is dimensionless by scaling the covariance with the product of standard deviations, so the skewness measure looks at the cubed deviations per cubed standard deviation. What direction is the price data? show / hide Using the formula we construct a table like so. i Y=price (Y-Y_bar) (Y-Y_bar)^3 1 12500 -2240 -11239424000 2 13350 -1390 -2685619000 3 14600 -140 -2744000 4 15750 1010 1030301000 5 17500 2760 21024576000 We see lots of negative cubed deviations. Let’s add them up to get 8127090000. Divide this by $N-1=$5 and we get 2031772500. Dividing by the cube of the standard deviation we see our result. \\[ skew = \\frac{\\frac{\\Sigma_{i=1}^N (X_i - m)^3}{N-1}}{s^3} = \\frac{2031772500}{7707067427.797} = 0.2636 \\] We are sure to check out the units of measurement here. The skew is positive. Price variations on average are above their mean. What about miles? By the by, the solar loss ditribution does indeed compute a positive skewness. But we could see that with our own eyes as well. So now how thick is the tail? Here we can use a variant of the skew just by squaring the variance term. Kurtosis describes the condition of the thickness of the tail. Just as skewness tells us whether the deviations are on average above or below the mean, so kurtosis tells us how volatile the standard deviation is. Try this formula out for kurtosis on the price variable. \\[ kurtosis = \\frac{\\frac{\\Sigma_{i=1}^N (X_i - m)^4}{N-1}}{s^4} \\] How thick or thin are the tails? show / hide Using the formula we construct a table like so. i Y=price (Y-Y_bar) (Y-Y_bar)^4 1 12500 -2240 25176309760000 2 13350 -1390 3733010410000 3 14600 -140 384160000 4 15750 1010 1040604010000 5 17500 2760 58027829760000 We see lots of negative cubed deviations. Let’s add them up to get 8127090000. Divide this by $N-1=$5 and we get 2031772500. Dividing by the cube of the standard deviation we see our result. \\[ kurtosis = \\frac{\\frac{\\Sigma_{i=1}^N (X_i - m)^4}{N-1}}{s^4} = \\frac{2031772500}{15223653062500} = 0.0001 \\] THe kurtosis is very small, indeed a very thin tale. This indicates a very stable volatility in prices. We will see later with teh normal distribution that the normal kurtosis is 3.00. Compared to the normal distribution, the price distribution is a very thin tailed distribution. An implication might be that it is a rare occurrence to see a high price in this sample. We must remember thata there are only 5 data points in the first place! The solar loss distribution has a kurtosis that is only slightly greater than 3. We are sure to check out the units of measurement here. The skew is positive. Price variations on average are above their mean. What about miles? 6.5 How can we use all of this? At least five come to mind. All of these build on a foundation of where (location, tendency) a distribution tends to land. Empirical Rule: if we thnk the distribution is symmetrical (“normal”) then the proportion of observations will be distance proportion \\(\\mu \\pm 1 \\sigma\\) 68.0 \\(\\mu \\pm 2 \\sigma\\) 95.0 \\(\\mu \\pm 3 \\sigma\\) 99.7 Chebychev: at least 75% of all values are within ±2σ of the mean regardless of the shape of a distribution lie within \\(\\pm 2 \\sigma\\) of the mean; for \\(\\mu \\pm k \\sigma\\), \\(k\\) standard deviations and the proportion of observations is \\[ 1 - \\frac{1}{k^2} \\] Outlier analysis: using IQR as a quantile inspired deviation we can build fences: Tukey(1977) once recommended We add 1.5 times IQR to Q3 and see if any data exceeded that fence and We subtract 1.5 times IQR from Q1 and see if any data fell short of that fence Coefficient of Variation: the ratio of the standard deviation to the mean expressed in percentage and is denoted CV \\[ CV = \\frac{\\sigma}{\\mu} \\times 100 \\] Comparisons using the z-score: converts means into standard deviation units; the number of standard deviations a value from the distribution is above or below the mean of the distribution \\[ z = \\frac{x - \\mu}{\\sigma} \\] Higher moments. The third and fourth moments are present in the skewnewss (cubed) and kurtosis (fourth power). Very nice math with a meaningful punch. Skewness will help us understand if deviations are above (positive skewness) or below (negative skewness) the average. Kurtosis will tell us if the standard deviation varies more or less than the normal distribution average of 3.00 (to be dug into later, of course). 6.6 Problems, always problems Shown below are the top nine leading retailers in the United States in a recent year according to Statista.com. Compute range, IQR, MAD, standard deviation, CV, and invoke the empirical rule and Chebychev’s Theorem, along with the z-score for each. Treat this as a sample (then what’s the population?). company revenue Walmart 343.62 The Kroger Co. 103.03 Costco 79.69 The Home Depot 74.20 Walgreen 72.67 Target 72.62 CVS Caremark 67.97 Lowe’s Companies 54.81 Amazon. Com 49.38 show / hide Answers (so many) Range: the distance between the max and min \\[ Range = |max(x) - min(x)| = 343.62- 49.38 = 294.24 \\] IQR: \\(Q_3\\) net of \\(Q_1\\) (\\(P_{75} - P_{25}\\)) \\[ IQR = Q_3 - Q_2 = 79.69 - 67.97 = 11.72 \\] Mean Absolute Deviation: MAD is robust \\[ MAD = \\frac{\\Sigma_{i=1}^N |x_i - \\mu|}{N} \\] \\[ \\mu = \\frac{\\Sigma_{i=1}^N x_i}{N}=\\frac{917.99}{9} \\] \\[ MAD = \\frac{485.3044}{9} \\] - Standard Deviation: first the square of the standard deviation is the variance \\[ \\sigma^2 = \\frac{\\Sigma_{i=1}^N (x_i - \\mu)^2}{N} \\] next, the standard deviation is \\[ \\sigma = \\sqrt{\\sigma^2} \\] A slightly easier way to compute \\(\\sigma^2\\) is \\[ \\sigma^2 = \\frac{\\Sigma_{i=1}^N x_i^2 - N(\\bar{x})^2}{N} \\] \\[ \\sigma^2 = \\frac{161163.0561 - 9(101.9989)^2}{9} = 8441.137 \\] \\[ \\sigma = 91.8757 \\] Empirical Rule: if we think the distribution is symmetrical (“normal”) then the proportion of observations will be and Chebychev: at least 75% of all values are within ±2σ of the mean regardless of the shape of a distribution lie within \\(\\pm 2 \\sigma\\) of the mean; for \\(\\mu \\pm k \\sigma\\), \\(k\\) standard deviations and the proportion of observations is \\[ 1 - \\frac{1}{k^2} \\] distance proportion k distance.1 chebychev \\(\\mu \\pm 1 \\sigma\\) 68.0 1 \\(101 \\pm 1*92\\) \\(\\mu \\pm 2 \\sigma\\) 95.0 2 \\(101 \\pm 2*92\\) 0.75 \\(\\mu \\pm 3 \\sigma\\) 99.7 3 \\(101 \\pm 3*92\\) 0.89 Coefficient of Variation: the ratio of the standard deviation to the mean expressed in percentage and is denoted CV \\[ CV = \\frac{\\sigma}{\\mu} \\times 100 = \\frac{91.8757}{101.9989} \\times 100 = 90.0752 \\] z-score: converts means into standard deviation units; the number of standard deviations a value from the distribution is above or below the mean of the distribution. For the first data point \\[ z = \\frac{x - \\mu}{\\sigma} = \\frac{343.62 - 101.9989}{91.8757}=\\frac{241.6211}{91.8757} \\] This observation is 2.6299 standard deviations from the mean 101.9989. Sample versus population If \\(x\\) is a sample (subset) indexed by \\(i = 1 \\dots N\\) with \\(N\\) elements from the population, then (the same formula!) \\[ \\bar{x} = \\frac{\\Sigma_{i=1}^N x_i}{N} = \\frac{917.99}{9} = 101.9989 \\] and \\[ s^2 = \\frac{\\Sigma_{i=1}^N (x_i - \\bar{x})^2}{N-1} - \\frac{67529.0961}{9 -1} = 8441.137 \\] and then \\[ s = \\sqrt{s^2} = 91.8757 \\] Shown below are the per diem business travel expenses in 11 international cities selected from a study conducted for Business Travel News’ 2015 Corporate Travel Index, which is an annual study showing the daily cost of business travel in cities across the Globe. The per diem rates include hotel, car, and food expenses. Use this list to calculate the z scores for Lagos, Riyadh, and Bangkok. Treat the list as a sample. city expense London 576 Mexico City 240 Tokyo 484 Bangalore 199 Bangkok 234 Riyadh 483 Lagos 506 Cape Town 230 Zurich 508 Paris 483 Guatemala City 213 show / hide Answer Range: the distance between the max and min \\[ Range = |max(x) - min(x)| = 576- 199 = 377 \\] IQR: \\(Q_3\\) net of \\(Q_1\\) (\\(P_{75} - P_{25}\\)) \\[ IQR = Q_3 - Q_2 = 495 - 232 = 263 \\] Mean Absolute Deviation: MAD is robust \\[ MAD = \\frac{\\Sigma_{i=1}^N |x_i - \\mu|}{N} \\] \\[ \\mu = \\frac{\\Sigma_{i=1}^N x_i}{N}=\\frac{4156}{11} = 377.8182 \\] \\[ MAD = \\frac{1546.1818}{11} = 140.562 \\] - Standard Deviation: first the square of the standard deviation is the variance \\[ \\sigma^2 = \\frac{\\Sigma_{i=1}^N (x_i - \\mu)^2}{N} \\] next, the standard deviation is \\[ \\sigma = \\sqrt{\\sigma^2} \\] A slightly easier way to compute \\(\\sigma^2\\) is \\[ \\sigma^2 = \\frac{\\Sigma_{i=1}^N x_i^2 - N(\\bar{x})^2}{N} \\] \\[ \\sigma^2 = \\frac{1796936 - 11(377.8182)^2}{11} = 22672.3636 \\] \\[ \\sigma = 150.5734 \\] Empirical Rule: if we think the distribution is symmetrical (“normal”) then the proportion of observations will be and Chebychev: at least 75% of all values are within ±2σ of the mean regardless of the shape of a distribution lie within \\(\\pm 2 \\sigma\\) of the mean; for \\(\\mu \\pm k \\sigma\\), \\(k\\) standard deviations and the proportion of observations is \\[ 1 - \\frac{1}{k^2} \\] distance proportion k distance.1 chebychev \\(\\mu \\pm 1 \\sigma\\) 0.680 1 \\(378 \\pm 1*151\\) \\(\\mu \\pm 2 \\sigma\\) 0.950 2 \\(378 \\pm 2*151\\) 0.75 \\(\\mu \\pm 3 \\sigma\\) 0.997 3 \\(378 \\pm 3*151\\) 0.89 Coefficient of Variation: the ratio of the standard deviation to the mean expressed in percentage and is denoted CV \\[ CV = \\frac{\\sigma}{\\mu} \\times 100 = \\frac{150.5734}{377.8182} \\times 100 = 39.8534 \\] z-score: converts means into standard deviation units; the number of standard deviations a value from the distribution is above or below the mean of the distribution. For the first data point \\[ z = \\frac{x - \\mu}{\\sigma} = \\frac{576 - 377.8182}{150.5734}=\\frac{198.1818}{150.5734} \\] This observation is 1.3162 standard deviations from the mean 377.8182. Sample versus population If \\(x\\) is a sample (subset) indexed by \\(i = 1 \\dots N\\) with \\(N\\) elements from the population, then (the same formula!) \\[ \\bar{x} = \\frac{\\Sigma_{i=1}^N x_i}{N} = \\frac{4156}{11} = 377.8182 \\] and \\[ s^2 = \\frac{\\Sigma_{i=1}^N (x_i - \\bar{x})^2}{N-1} - \\frac{226723.6364}{11 -1} = 22672.3636 \\] and then \\[ s = \\sqrt{s^2} = 150.5734 \\] 6.7 What have we learned? So much! More to come. Deviation drives statistical thinking Deviation depends on movements away from where we believe data tends We have a model: \\(y = \\bar{y} + e\\) where \\(e\\) is the deviation of a variable \\(y\\) from its tendency \\(\\bar{y}\\). "],
["binomial-distribution.html", "Chapter 7 Binomial Distribution 7.1 What is a probability distribution?", " Chapter 7 Binomial Distribution 7.1 What is a probability distribution? For a given variable (e.g., house prices), the frequency of ranges of this variable (i.e., classes, groups, intervals). 7.1.1 Discrete or continuous? Discrete: the number of distinct events (integers) Continuous: always possible to find a number between any two events (real number) 7.1.2 Examples of distributions The Bernoulli distribution is used in situations where an uncertain parameter can take on one of only two possible values. The binomial distribution is used for the number of outcomes on repeated trials when each trial is independently sampled (with replacement). The hypergeometric distribution is used for the number of outcomes on repeated trials when each trial is dependent on another trial (without replacement). The poisson distribution is used for the number of outcomes in a unit of time. The uniform distribution describes an outcome that is equally likely to fall anywhere between a minimum and a maximum value. The triangular distribution is a more flexible family of continuous distributions: these distributions are specified by three parameters: the minimum, maximum, and most likely values. The normal distribution is a symmetric distribution, usually specified by its mean and standard deviation. The exponential distribution describes the frequency of times elapsed between random poisson occurrences. "],
["imagine-this.html", "Chapter 8 Imagine this", " Chapter 8 Imagine this You are considering to found an equity fund to track risky investments in privately owned organizations You have a history of stock prices You know that today’s stock price is the present value of stockholders’ free cashflow for the foreseeable and not-so-foreseeable future You need a back of the envelope way to describe how stock prices and value evolve You also want to use this evolution to understand how a claim contingent on stock prices also evolves And the present value of such a claim What do you want to know? Is there a distribution you can use to describe the evolution and also price a claim on the stock prices? Why? So you can acquire or divest an asset, a project, or anything that generates cash flows over time and has the same risk profile as the stock price. "],
["whats-a-binomial.html", "Chapter 9 What’s a binomial? 9.1 Then there were two 9.2 Being binomial 9.3 What’s a combination? 9.4 Combinations from permutations 9.5 So how do we use the binomial process? 9.6 Try this: binomial distribution 9.7 What does a graph of the binomial look like? 9.8 Binomial statistics 9.9 Short exercises 9.10 Binomial barrels", " Chapter 9 What’s a binomial? Suppose we know that the current stock price of Ross Stores (NASDQ: ROST) \\(S\\) is $109 per share. We might consider a simple forecast of the uncertain level of the asset’s value in one day, or even several days into the future: it might go up or it might go down. That’s a binomial. We picture the binomial view of an anticipated stock price \\(S\\) as branches from a root of \\(S_0=\\$109\\) per share today at time \\(t=0\\). How can we forecast asset value at the end of one day \\(S_1\\)? We can do so by supposing that the stock price might simply rise or fall. Both up \\(u\\) and down \\(d\\) stock price outcomes, \\(S_{1,u}\\) and \\(S_{1,d}\\), might occur in \\(t=1\\) day. We might also want to represent our optimisim or pessimism about how often up moves and down moves might occur. Let’s assume just for a minute that we are optimistic about the future so that the probability of any up move in this stock is \\(p = 0.60\\). If the probability of an up move is 0.60, then the probability of a down move must be show / hide Probability of a down movement is 1 minue the probability of an up movement or \\(1-p=1-0.06=0.40\\). We can hover over the diagram below and locate the initial stock price and the up and down possibilities as well as probabilities of an up or down movement. Now we can suppose that the stock price grows or declines by an amount. That amount can be learned from the sample standard deviation \\(\\sigma\\) of the rate of return of the stock price. Rates of return are just growth (or decline) rates. So that the one day ahead stock price will go up to \\[ S_{1,u} = S_0 + S_0 \\sigma = S_0(1+\\sigma) \\approx S_0 e^{\\sigma} \\] Where we use the approximmation \\((1+\\sigma) \\approx e^{\\sigma}\\). We must keep in mind that this is the one day standard deviation. Let’s get this a little more down to earth. We find that over the past 251 trading days that the stock return daily standard deviation is 0.0165 or 1.65% per day. If we multiply this by 251 days a $1 invested today would grow into 62.8971. This means that after 251 up jumps from today we would get this amount. Intuitively we might feel this is not very likely! Anyway, after just one up jump (a stock price move for one day) and if today’s stock price is $109 per share, then \\[ S_{1,u} = S_0 + S_0 \\sigma = S_0(1+\\sigma) \\approx S_0 e^{\\sigma} = 109 e^{0.0165} = 110.8134 \\] What goes up might just as well go down. If stock returns have a positive \\(\\sigma = 0.0165\\), and since \\(\\sigma = var^{1/2}\\), then it is possible for a negative or down turn in the return. Now the factor is $e^{-0.0165}=0.9836, a discount to the current stock price of $109 per share. \\[ S_{1,d} = S_0 - S_0 \\sigma = S_0(1-\\sigma) \\approx S_0 e^{-\\sigma} = 109 e^{-0.0165} = 107.2163 \\] If the stock could jump up 251 times it might decline that many times too. In that scenario, the stock price would move from $109 to what level? show / hide 1.733 What a decline! What would be the expected value of the stock price in one day? This question is just a weighted average of the probabilities of up and down with the stock price outcomes, a random variable in one day. show / hide \\[ E(S_1) = p S_{1,u} + (1-p) S_{1,d} = (0.60)(110.81)+(0.40)(107.2163)=109.37 \\] Just a tad above today’s price. 9.1 Then there were two Yes, two draws of stock prices, one after the other, one conditional on the other. Let’s look at this up and down tree. SOme description is in order. At the top is right now, day 0. The second row is day 1. The third row is day 2. If the stock goes up from day 0 to day 1, then the value is \\(S(1,u)\\). Similarly if the stock goes down. If after the stock goes up at day 1, then the stock can either go up to \\(S(2,uu)\\) (that is, jump up twice) to go down to \\(S(2,ud)\\) (that is, after jumping up it then jumps down). The same thoughts will occur when the stock goes down to \\(S(1,d)\\). How many paths are there to get to the third row where we have forecasted the 2nd day’s stock price? show / hide For \\(S(2,uu)\\) there is only one path, up and up. For \\(S(2,ud=du)\\) there are two paths, up and down, and down and up. It does not matter, they both lead to the same node. For \\(S(2,dd)\\) there is only one path, down and down. 9.1.1 How often? We can compute outcomes all day. But how often does a node occur? We remember that our task is to forecast stock prices in the future. To do that we realize that stock prices occur in a probable range. That means they are random variables, where the adjective random has the notion of an indiscriminate sampling of prices. But a random variable is not so colloquially indiscriminate as to not have a notion of a frequency of occurrence. The relative frequency, as we continue to see, is what we measure to be probability. Allowing probability into our lives also admits our beliefs into the analysis. So what is the probability of a stock price after one up and one down jump? show / hide This is an example of a both-and event. Both an up and a down jump must occur. The probability of an up jump, according to our analysis is 0.60. The probabiilty of a down jump is then just \\(1-0.60=0.40\\). Intuitively, if stock prices were to rise, that would account for 60% of our experiments. Then after an up jump, the stock falls, so out of 60 pf a humdred experiments with stock prices, 40% of those might occur. Thus the probability of an up jump followed by a down jump is \\[ Prob(S(2, ud)) = p^1(1-p)^1 = (0.60)^1(0.40)^1 = 0.24 \\] THis is the probability of a single up and down combination. Hang on! In the stock price tree we just admitted that there are two paths to get to a stock price that experiences both an up and a down jump. Okay, that would then mean there are two mutually exclusive ways of getting to an outcome like that. EIther the stock price jumps up then jumps down, or the stock price jumps down then up. This is thus an either-or event. We must therefore add the independently occurring probabilities. \\[ Prob(s(2, ud) \\cup S(2, du)) = Prob(S(2, ud)) + Prob(S(2, du)) = 0.24 + 0.24 = 2(0.24) = 0.48 \\] Thus the complete answer is that 48% of the time we might experience a stock price that jump up one day and down another. How would we calculate the \\(S(2,du=ud)\\) outcome? show / hide Let’s use the up and down sequence. We already know that \\[ S_{1,u} = S_0 + S_0 \\sigma = S_0(1+\\sigma) \\approx S_0 e^{\\sigma} = 109 e^{0.0165} = 110.8134 \\] Yes, the stock price rises. After the rise to 110.8134, the stock price now jumps down. We take our up estimate and condition it with a down movement like this \\[ S_{2,ud} = S_{1,u} - S_{1.u} \\sigma = S_{1,u}(1-\\sigma) \\approx S_{1,u} e^{-\\sigma} = 110.8134 e^{-0.0165} = 109 \\] Yes, right back where we started. Do we not observe a symmeterical outcome? Yes, indeed we do. The \\(S(2, du=ud)\\) outcome occurs 48% of the time given our optimism in this market. 9.2 Being binomial 9.2.1 Consumer goods ‘r’ us From a Consumer Food database. What proportion of the database households are in the Metro area? Use this as the value of \\(p\\) in a binomial distribution. If you were to randomly select 25 of these households, what is the probability that exactly 8 would be in the Metro area? If you were to randomly select 12 of these households, what is the probability that 3 or fewer would be households in the Metro area? Just like the probability of an up swing in stock prices, the proportion of Metro area households is the probability of that up or success (yes we found Metro households!) brach of the binomial tree. We can go on to then answer qeustions 2 and 3. We will need to know how many paths it can take to get to an outcome, the probability of a single path as well. Armed with this knowledge we can answer questions just like these. 9.2.2 What’s a binomial? Let’s get more precise. Two possible event outcomes only in each run or scenario of a binomial process E.g., default/not-default, reject/accept, comply/not-comply events Let \\(x\\) = comply, then \\(P(x)\\) = probability of compliance and \\(1 - P(x)\\) = probability of non-compliance. We use three assumptions: Each replication of the process is a combination of events that results in one of two possible outcomes (usually referred to as “success”\" or “failure” events). The probability of success is the same for each replication. The replications are independent, meaning here that a success in one replication does not influence the probability of success in another. 9.3 What’s a combination? Start with a set of choices or categories. Combinations are the complete set of different ways you can arrange the various subsets of choices or categories. 9.3.1 For a simple example Start with the set of \\(A= \\{1,2\\}\\), where “1” is “comply” and “2” is “don’t comply.” You can form four subsets of this set: {}, {1}, {2}, and {1,2} (don’t forget the null or “do nothing” subset). These subsets are the combinations of set A. Order does not matter so that \\(\\{1,2\\}\\) is the same as \\(\\{2,1\\}\\) 9.3.2 We have to start with permutation first A permutation does care about the order of the elements in a subset, much like the order of letter in a word. We will find that combinations are found from permutations. 9.3.3 Start with \\(A = \\{a,b,c,d,e\\}\\) letters in a text to your friend. With 5 letters to choose from we can select the first letter in 5 ways. We now have 4 letters left, so the second letter can be chosen in 4 ways, Then the third letter in 3 ways The fourth letter in 2 ways The fifth letter in 1 way or \\[ 5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120 \\] ways to build a word of text (granted some of these might be code). 9.3.4 Find the number of 3 letter words you can form from a list of 5 letters. Using the same logic as above this is \\[ 5 \\times 4 \\times 3 = 60 \\] There are 120 possible words you can form from 5 letters. There are 60 possible 3 letter sequences. This is called a permutation. In symbols we have 5 permute 3 or \\(_{5}P_{3}\\). We notice that \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 5 \\times 4 \\times 3 \\times 2!\\). So all we need to do to get the 60 permutations is take 5! and divide by 2! to get \\[ _{5}P_{3} = \\frac{5!}{2!} = 60 \\] Generally we have for \\(n\\) elements permuted \\(x\\) at a time: \\[ _{n}P_{x} = \\frac{n!}{(n-x)!} \\] 9.3.5 What about combinations? Now we don’t care about the order of the letters. We seem to also know that Permutations track with the order of elements, here, letters: {a,d,c} \\(\\neq\\) {d,c,a} Combinations don’t worry about the order of elements: {a,d,c} = {d,c,a} Thus out of the 60 permutations we just counted, we have to keep only one in \\(3 \\times 2 \\times 1 = 3! = 6\\) (1/6th) of the 60 Permutations or 10 combinations. We then have 6 times more permutations than we need to account for combinations. 9.4 Combinations from permutations For our 5 letters taken 3 at a time without regard to the order of letters, we have \\[ _{5}C_{3} = {5 \\choose 3} = \\frac{5!}{3! \\, (5 - 3)!} \\] and then we compute \\[ = \\frac{5x4x3x2x1}{(3x2x1)(2x1)} = 10 \\] More generally, we have \\[ _{n}C_{x} = {n \\choose x} = \\frac{n!}{x! \\, (n - x)!} \\] and \\[ _{n}P_{x} = _{n}C_{x}x! = {n \\choose x}x! = \\frac{n!}{(n - x)!} \\] 9.4.1 Try these: combinations and permutations A risk management practice has 8 members. Teams of three are formed to work with client project teams and provide management, subject matter expertise and budget and scheduling control. The practice leader wants to know How many different project teams (“teams”) can be formed from the practice? How many teams of 3 members can be formed from the practice if each team is to have a team manager, a project controller, and a subject matter expert? show / hide Let’s get to it. How many different project teams (“teams”) can be formed from the practice? We will find the number of combinations of 8 members taken 3 at a time without regard for the order. Calculate \\[ _{8}C_{3} = {8 \\choose 3} = \\frac{8!}{3! \\, (8 - 3)!} \\] so that \\[ _{8}C_{3} = \\frac{8x7x6x5x4x3x2x1}{(3x2x1)(5x4x3x2x1)} = \\frac{336}{6} = 56 \\] potential combinations of practice members. How many teams of 3 members can be formed from the practice if each team is to have a team manager, a project controller, and a subject matter expert? The practice leader is now interested in finding the number of permutations of 8 members taken 3 at a time, when the order matters. Calculate \\[ _{8}P_{3} = {8 \\choose 3}3! = \\frac{8!}{3! \\, (8 - 3)!}3! \\] so that \\[ = 56 \\, (3x2x1) = 336 \\] potential permutations of practice members. 9.5 So how do we use the binomial process? Follow these steps: How many times did we repeat the process (number of observations)? \\(n\\). How many successes (events we are tracking)? \\(x\\) How often does a success occur (event we are tracking)? \\(p(X = x)\\) How many \\(x\\) successes (events) in \\(n\\) replications (trials, observations)? The number of possible combinations of \\(x\\) successes in \\(n\\) replications is \\[ _{n}C_{x} = {n \\choose x} = \\frac{n!}{x! \\, (n - x)!} \\] The \\(x\\) successes can occur anywhere among the \\(n\\) trials (observations). There are \\(_{n}C_{x}\\) different ways of distributing \\(x\\) successes in a sequence of \\(n\\) trials (observations). What is the probability of a single scenario (combination = 1) of \\(x\\) successes? \\(p^x\\) What is the probability of a single scenario of \\(n-x\\) failures? \\((1-p)^{n-x}\\) What is the probability of all combinations of \\(x\\) successes in \\(n\\) trials (using the Excel formula)? \\[ P(X = x \\mid n, p) = {n \\choose x}p^x (1-p)^{n-x} \\] We can also use the \\(BINOM.DIST(x,n,p,FALSE)\\) formula in Excel, where FALSE indicates that we calculate the probability mass function value (the relative frequency) and TRUE indicates that we calculate cumulative probability value (the cumulative relative frequency). 9.6 Try this: binomial distribution A environmental control specialist picks a sample of 10 sensors from a large shipment of sensors. Experience has shown that 1 in 5 sensors fail to work when installed. The specialist is scheduling her time for the week and wants to know What is the probability that she will pick exactly 2 of the defective sensors? What is the probability that she will pick no more than 2 of the defective sensors? show / hide Using (a bit slavishly) our protocol: What is the probability that she will pick exactly 2 of the defective sensors? Follow these steps: How many times did we repeat the process (number of observations)? Here \\(10\\) is the sample size. How many successes (we are tracking “defective” sensors)? No more than \\(2\\) defective sensors. How often does a success (“defective” sensor) occur (event we are tracking)? \\(p = 1/5 = 0.20\\). How many \\(x\\) successes (events) in \\(n\\) replications (trials, observations)? The number of possible combinations of \\(2\\) successes (“defective” sensors) in \\(10\\) replications (sample size) is \\[ _{10}C_{2} = {10 \\choose 2} = \\frac{10 !}{2! \\, (10 - 2)!} = 45 \\] The \\(2\\) defective sensors can occur anywhere among the \\(10\\) sampled sensors. There are \\(_{10}C_{2}\\) different ways of distributing \\(2\\) defective sensors in a sequence of \\(10\\) sampled sensors (observations). What is the probability of a single scenario (combination = 1) of \\(x\\) successes? \\((0.2)^2 = 0.04\\). What is the probability of a single scenario of \\(n-x\\) failures? \\((1-0.2)^{10-2} = (0.8)^8 = 0.1678\\) What is the probability of all combinations of \\(2\\) defective sensors in a sample of \\(10\\) sensors? \\[ Binom(10,2,0.2) = {10 \\choose 2}(0.2)^2 (1-0.2)^{10-2} \\] or \\[ Binom(10,2,0.2) = (45)(0.04)(0.1678) = 0.3020 \\] and in Excel BINOM.DIST(2,10,0.2,FALSE). What is the probability that she will pick no more than 2 of the defective sensors? Assuming no intersections and independent sampling of the 10 sensors,the probability is the union of three events, finding 0, 1, and 2 defective sensors. We have \\[ P(X \\leq 2) = P(X = 0) + P(X = 1) + P(X = 2) \\] We already have \\(P(X = 2) = 0.3020\\). Now we calculate \\[ P(X = 0) = Binom(10,0,0.2) = {10 \\choose 0}(0.2)^0 (1-0.2)^{10-0} \\] or \\[ Binom(10,0,0.2) = 0.1074 \\] Similarly,\\(P(X = 1) = 0.2684\\). We have then \\[ P(X \\leq 2) = P(X = 0) + P(X = 1) + P(X = 2) = 0.6778 \\] 9.6.1 Try another Using the same sensor problem, what is the probability that she will pick 8 or more defective sensors? What if she picks more than 8 defective sensors? Build a simple spreadsheet table to answer these questions. show / hide Assuming no intersections and independent sampling of the 10 sensors,the probability is the union of three events, finding 8, 9, and 10 defective sensors. We have, if she finds 8 or more defective sensors, \\[ P(X \\geq 8) = P(\\{X=8\\}\\cup \\{X=9\\} \\cup \\{X=10\\}) \\] and with mutually exclusive, independent (with replacement) events, \\[ P(\\{X=8\\}\\cup \\{X=9\\} \\cup \\{X=10\\}) \\] \\[ = P(X = 8) + P(X = 9) + P(X = 10) = 0.0078\\% \\] Finding more than 8 sensors \\[ P(X &gt; 8) = P(\\{X=9\\} \\cup \\{X=10\\}) \\] and with mutually exclusive, independent (with replacement) events, \\[ P(\\{X=9\\} \\cup \\{X=10\\}) = P(X = 9) + P(X = 10) = 0.00042\\% \\] Here is a worksheet table that builds up these calculations. 9.7 What does a graph of the binomial look like? 9.8 Binomial statistics Here we need just the mean and standard deviation. Mean of a binomial distribution \\[ \\mu = n p \\] Standard deviation of a binomial distribution \\[ \\sigma = (n p (1-p))^{1/2} \\] 9.8.1 Try this Use the data from the last example above. What are the mean and standard deviation of the sensor problem? show / hide We calculate for \\(n = 10\\) and \\(p = 0.20\\) Mean of the sensor binomial distribution \\[ \\mu = n p = (10)(0.20) = 2 \\] defective sensors. Standard deviation of the sensor binomial distribution \\[ \\sigma = (n p (1-p))^{1/2} = ((10)(0.20)(0.80)) \\] 1.26, or rounding down, 1 defective sensor. 9.9 Short exercises Hospital records show that of patients suffering from a certain rare disease, 75% die of it. What is the probability that of 6 randomly selected patients, 4 will recover? Answer: 3.3% For the patients suffering from a certain disease, What is the probability that all 6 randomly selected patients will recover? Answer: very, very small You are trying reach your surveying team in the iron rich creases of the Mesabi mountain range. there was a probability of 80% of success in any attempt to connect by cell phone. What is the probability of having 7 successes in 10 attempts? Answer: a little over 20% A manufacturer of metal pistons finds that, on the average, 12% of the pistons are rejected by customers because they are either oversize or undersize. What is the probability that a batch of 10 pistons will contain no more than 2 rejects? Answer: 89% 9.10 Binomial barrels You own a small refinery. Inputs are barrels of crude products. Outputs are barrels of refined products. Your corporate policy is to allow shipments where no more than 3 sampled barrels of unacceptable quality (non-compliant) are discovered in a sample of 10 barrels. However, because of the nature of the production process and the quality of inputs, it is 20% of time that bad barrels occur. Use this Excel workbook (binomial.xlsx) to perform these tasks and answer the questions below: Sample 30 barrels from the population. Set up the spreadsheet model for probabilities (p) of a non-compliant barrel equal to 20%, 50%, and 80%. What is the probability that 5 or fewer barrels will likely be found? What is the probability that 15 or greater barrels will be found? How probable is it that between 5 and 15 barrels will found exclusive of 5 or 15? "],
["poisson-distribution.html", "Chapter 10 Poisson Distribution 10.1 Imagine this 10.2 Let’s calculate 10.3 Some (mercifully) short exercises", " Chapter 10 Poisson Distribution 10.1 Imagine this Here are some events we might consider. Your corporate website get 2 hits per minute There have been 3 pandemics in the last 100 years Your satellite TV fails to record a show at least 4 times a month Your most important client has 6 meltdowns a month Let’s ask some questions next. What do we want to know? How probable any and / or all of the events will happen! Why? So we can plan our life a bit better – allocate scarce time and money more efficiently, at the very least. There is a distribution we can use to model the incomplete, and thus uncertaain information contained in these events and in our potential response to these events. The Poisson random variable will help us with this task and satisfies the following conditions: The number of successes in two disjoint (non-overlapping) time intervals is independent. The probability of a success during a small time interval is proportional to the entire length of the time interval. Apart from disjoint time intervals, the Poisson random variable also applies to disjoint regions of any kind of space, such as geographical and market. Here are some applications of Poisson thinking about events. The number of deaths by horse kicking in the Prussian army (first application) Birth defects and genetic mutations Rare diseases Car accidents Traffic flow and ideal vehicle gap distance Number of typing errors on a page Hairs found in McDonald’s hamburgers Extinction of an endangered animal in Africa Failure of a machine in one month Frequency of cyber attacks in a year Why not stick with the binomial distribution? Good question. We use the binomial when we need to find the probability of the number of successes in so many trials. On the other hand we use the Poisson distribution when we need to find the probability of a number of successes unit of time or space. 10.2 Let’s calculate As always we define success first. First define the number of success as \\(X\\). Next, suppose successes occur with intensity (same as: at a frequency or rate) of \\(\\lambda\\) per unit time or space. Then, the probability of a specified number of successes \\(X = x\\) per unit time (or space) is \\[ P(X = x) = \\frac{\\lambda^{x}e^{-x}}{x!} \\] where \\(e = 2.71828\\) is the base of the natural logarithm. 10.2.1 Try this We estimate that 6 accidents (“vehicular contacts”) per month occur at the intersection of Crosby and Westchester Avenues in the Bronx. What is the probability of 3 accidents in any month? Plot the distribution of accidents from 0 to 20. what is the probability of 3 accidents or less in any month? 10.2.2 Results What is the probability of 3 accidents in any month? We let \\(X\\) be the number of possible accidents per month at this intersection. Out data is \\(X = x = 3\\) and \\(\\lambda = 6\\). We can then compute \\[ \\begin{array}{c} P(X = 3) = \\frac{\\lambda^{x}e^{-x}}{x!} \\\\ \\\\ = \\frac{6^{3}e^{-3}}{3!} \\\\ \\\\ = \\frac{(216)(0.002478)}{3x2x1} \\\\ \\\\ = \\frac{0.5354105}{6} \\\\ \\\\ = 0.08923508 \\end{array} \\] that is, 8.9%. Plot the distribution of accidents from 0 to 20. What is the probability of 3 accidents or less in one month? 10.3 Some (mercifully) short exercises Customers pass through the doors of a retail outlet at an average rate of 300 per hour. Find the probability that none passes in a given minute. What is the expected number passing in two minutes? Find the probability that this expected number actually pass through in a given two-minute period. Answers : 0.0006, 10, 12.5% A company makes windmill rotors. The probability a rotor is defective is 0.01. What is the probability that a sample of 300 rotors will contain exactly 5 defective rotors? [Use both the Poisson and the Binomial distributions to arrive at an answer.] Answer : a little over 10% 10.3.1 Not so short exercises The number of calls into your call center averages 3 per minute. Find the probability that no calls come in a given 1 minute period. Answer: 4.978% Assume that the number of calls arriving in two different minutes are independent. Find the probability that at least two calls will arrive in a given two minute period. Answer: 98.26% 10.3.2 Answers Let \\(X_1\\) and \\(X_2\\) be the number of calls coming in the first and second minutes respectively. We want \\(P(X_1 + X_2 \\geq 2)\\). Let’s organize the answer this way \\[ P(X_1 + X_2 \\geq 2) = 1-P(X_1 + X_2 &lt; 2) \\] In this way we only need to look at no calls (0 minutes) and 1 minute calls, or, for each possible sequence of 0 and 1 minute calls each sequence happening together (intersection) and independently. We have \\(1-P(X_1 + X_2 &lt; 2) = 1-(P(X_1 = 0 \\cap X_2 = 0)\\) \\(+ P(X_1 = 1 \\cap X_2 = 0) + P(X_1 = 0 \\cap X_2 = 1))\\) which, in turn, equals \\(= 1 - [P(X_1 = 0) P(X_2 = 0)\\) \\(+ P(X_1 = 1) P(X_2 = 0) + P(X_1 = 0) P(X_2 = 1)]\\) We already know from the first answer that \\[ P(X = 0) = \\frac{e^{-3}3^0}{0!} = e^{-3} \\] which equals 0.0498. We can then compute \\[ P(X = 1) = \\frac{e^{-3}3^1}{1!} = 3exp(-3) \\] which equals 0.1494. Putting this altogether we have \\(P(X_1 + X_2 \\geq 2) =\\) \\(1- [(0.0498)(0.0498)+(0.1494)(0.0498)+(0.0498)(0.1494)]\\) \\(= 1-0.01736\\) \\(= 0.9826\\) or 98.26%. "],
["how-likely-is-that.html", "Chapter 11 How likely is that? 11.1 Learning outcomes 11.2 Invitation to a formal 11.3 Dress down a bit 11.4 Try another 11.5 Law of Conditional Probability (Bayes) 11.6 Practice 1 11.7 Practice 2 11.8 Nothing is random!", " Chapter 11 How likely is that? 11.1 Learning outcomes How likely is it we ask the question, “How likely is that?” Alot! This unit moves our thinking from simply counting how often an event we have sampled occurs to how probable. The move is subtle. Relative frequency empirically becomes associated with the probability that an event occurs, and Cumulative relative frequency empirically becomes associated with the cumulative probaability that a set of events occurs. In this unit we will Define and use sample spaces of events (all of our sampled data) summarized in a cross-tabulation table Calculate the probability that either one event or another might occur Calculate the probability that both one event and another might occur Calculate the probability that an event might occur given that another event might occur Calculate aa revision of a probability given new or different information has arrived So it is one table and four calculations in this unit. What do you get from this exercise? The ability to use empirical data and query the many possible relationships in that data all summarized by intersections (both - and), unions (either - or), and conditions (one thing given the existence of another). These queries will answer the question: “how likely is that?” 11.2 Invitation to a formal Let’s get speicifc about what we are talking: Sample space \\(S\\): all possible outcomes of an experiment \\(E\\) Example: “Raining two days in a row” \\(S = \\{ (rain, not), (not, rain), (rain, rain), (not, not) \\}\\) Events in a sample sample \\(F\\): subset of sample space Example: “If it rains today, then”\" \\(F = \\{ (rain, not), (rain, rain) \\}\\) Measure of probability \\(P\\): maps \\(F\\) to the real numbers between \\([0,1]\\) \\(p \\in [0,1]\\) for all events \\(i \\in F\\): \\(\\Sigma_i p_i = 1\\) the union of any disjoint events \\(A_j \\in F\\): \\(\\cup_i\\,Prob(A_j) = \\Sigma_i\\,Prob(A_j)\\) We use these same ideas when we talk about non-overlapping intervals of data, the frequency of the number of observations in an interval, the relative and cumulative relative frequency of the empirical occurence of data in an interval. What we are now adding is how one data stream migh relate to another. We can now ask a question like this: “how likely is it that when we see high prices, do we also see high square-ffot lot sizes?” Let’s relate our formal concepts to the language we normally use. 11.2.1 A bit of English Let’s wrap some everyday language around these formal concepts. phrase meaning math \\(x\\) is at least \\(k\\) the least \\(x\\) is allowed to be is \\(k\\) \\(x \\geq k\\) \\(x\\) is at most \\(k\\) the most \\(x\\) is allowed to be is \\(k\\) \\(x \\leq k\\) both \\(x\\) and \\(y\\) common elements of \\(x\\) and \\(y\\) \\(x \\cap y\\) either \\(x\\) or \\(y\\) one, or the other, or both \\(x\\) and \\(y\\) \\(x \\cup y\\) 11.3 Dress down a bit We can use probability to help us anticipate the range of possible outcomes we might experience. For example, in two days will the stock of Johnson and Johnson rise or fall? It turns out it can do both potentially, How likely, and by how much, will that stock price rise (or fall)? Suppose the experiment \\(E\\) consists of stock price moves UP (U) and DOWN (D). So on day 1 (tomorrow), the stock price sample space is just \\(S_1 = \\{U, D\\}\\), where the subscript stands for day 1, that simple. But on day two, the day after tomorrow, the paths get a bit more complicated. If the stock goes up tomorrow, then on the next day the stock might either go up or go down. Thus there are two events we have to account for in two days if the stock price was up the day before \\(S_2^U = \\{UU, UD\\}\\), where the superscript helps us to cull only day 1 ups. The same logic will apply if on day one the stock price had gone down. In this case the sequence is \\(S_2^D = \\{DU, DD\\}\\) Putting it all together the sample space by the end of day two may be represented by \\(S_2 = \\{UU, UD, DU, DD\\}\\). just like a toss of a coin. Here the coin is the “market.” Given that the coin is fair and that the coin is tossed in an independent and identical manner, it is reasonable to apply the equally likely model: a 50% chance of a rise or a fall in any given round of flip the coin, or a day in the life of a stock price. But we can also load the coin and be more pessimistic or optimistic about a rise or fall in the price. What is the probability of at least 1 UP tick on day one or two? Looking at the sample space we see three (out of four) elements \\(\\{UU, UD, DU\\}\\) have at least one UP; thus, Prob(at least 1 UP) = 3/4 = 75%. What is the probability of no UP ticks)? Notice that the event {no UPs} = {at least one UP\\(\\}^c\\) = {all DOWN }, so that using the notations \\(-UP\\) for no (anything, in this case UP): \\[ Prob(-UP) = 1 - Prob(at\\,\\,least\\,\\,one\\,\\,UP) \\] \\[ = Prob(\\{UU\\}\\cup\\{UD\\}\\cup\\{DU\\}) = 1 - 3/4 = 1/4. \\] Here we use the language either-or, a union of events. We will use this trick often. 11.4 Try another Suppose the experiment \\(E\\) consists of 92 people some of whom identify with being female (F) or being male (M), some of whom smoke (S) or don’t (N). There are several events in this space that combine with gender and smoking status. 11.4.1 What does the event space look like? show / hide Here is a table of empirical results from the survey. Table 11.1: Counts: Gender x Smoking no yes Sum female 27 8 35 male 37 20 57 Sum 64 28 92 We notice the counts are in a bold black font. The row and column sums are in bold blue. We can use this information to answer several questions to enhance our knowledge of what might or might not happen. That knowledge is what we might label anticipation, or even belief, and if we are really bold, a forecast or prediction. 11.4.2 What is… There are several ways we can interpret our results by querying: What is the probability of being both a female and a smoker? What is the probability of either being a female or a smoker? If you are a female, what is the probability of being a smoker? We can use our table of counts to help us with our inquiries. Let’s try question one first. What can we calculate? show / hide What is the probability of being both a female (F) and a smoker (S), the yes column? Find the percentage of each cell relative to the whole sample (93). \\[ Prob(F \\cap S) = \\frac{n(F\\cap S)}{n} = \\frac{8}{92} = 8.70\\% \\] where \\(n\\) is the whole sample. Let’s use the same idea but twist it a bit into an either-or, or union calculation instead of an intersection to answer question 2. Let’s try it. show / hide What is the probability of either being a female or a smoker? Find the probability of being a female \\(Prob(F) = 38.04\\%\\). Find the probability of being a smoker \\(Prob(S) = 30.43\\%\\). Then find the probability of being both a female and a smoker \\(Prob(F \\cap S) = 8.70\\%\\), just so we don’t double count anybody. Finally, \\[ Prob(F\\,\\,or\\,\\,S) = Prob(F \\cup S) = Prob(F) + Prob(S) - Prob(F \\cap S) \\] \\[ = 38.04\\% + 30.43\\% - 8.70\\% = 59.77\\% \\] Why? We always remember not to double count observations, or events for that matter. This also shows that identifying as a smoker or as a female are not two mutually exclusive events. There are 8 respondents who would agree. If you are a female, what is the probability of being a smoker? We focus on smokers only and thus look at the column in the table for yes, a smoker. We use this as the basis (also known as the denominator) for figuring out the probability we seek. We again consult our tabulation table. But this time we only look at the counts in the female row at the cell that intersects with smokers, because respondents who identify as females might smoke or not. We need this data: How many females smoke? How many smokers are there in the survey? show / hide THe number of smokers who respond as females is the intersection of the number of smokers and the number of females. \\[ n(S\\cap F) = 8 \\] where \\(n()\\) is the number or count of what is in the parentheses. We can read the total number of smokers as the column sum of yes (smokers). \\[ n(S) = 28 \\] Not so bad, and now we continue to one of the more important concepts, that of conditional probability. We model \\(Prob(S | F)\\) as the probability of seeing a smoker in the survey, among all of the respondents whoa re females. The symbol \\(|\\) is read as conditional or given, so the \\(S | F\\) means \\(S\\) conditional on \\(F\\) and \\(S\\) given only \\(F\\). Thus we look at all \\(F\\) observations an filter only \\(S\\) of those observations. We would be right if we guessed that \\(F\\) is the independent variable and that \\(S\\) depends on the \\(F\\) part of the sample. In fact one way of looking at our regression model \\[ Y = a + bX + e \\] is using the notation that \\(Y \\mid X\\): given \\(X\\) how do we get \\(Y\\). Here \\(Y\\), and \\(S\\) are conditional on the occurrences in \\(X\\) and \\(F\\). Just like the way we calculated the slope \\(b\\) as \\[ b = \\frac{cov(X,Y)}{var(X)} \\] to recognize that there are covariations of \\(X\\) and \\(Y\\) relative to the total variation in \\(X\\), we have \\[ Prob(S\\,\\, given\\,\\,F) = Prob(S\\mid F) = \\frac{n(S\\cap F) / n(S)}{n(F) / n(S)} = \\frac{n(S\\cap F)}{n(F)} \\] or with numbers from the tabulation we have for gender and smoking status: \\[ Prob(S \\mid F) = \\frac{Prob(S\\cap F)}{Prob(F)} = \\frac{8.70}{38.04} = 22.87\\% \\] In the numerator are the number of ways in which smokers, the dependent variable, and females interact (intersect and might even co-vary). In the denominator are the total number of females, the independent variable. But…what is \\(Prob(F \\mid S)\\)? show / hide We flip the logic a bit here. Now females are the dependent variable, and we look to the total number of smokers as the independent variable. Again, using the original counts, we get a different result. \\[ Prob(F|S) = \\frac{Prob(F\\cap S)}{Prob(S)} = \\frac{8/92}{28/92} = 28.57\\% \\] 11.5 Law of Conditional Probability (Bayes) Our female smokers lead us to a more general approach here. If we know \\(Prob(B \\mid A)\\), can we find \\(Prob(A \\mid B)\\)? \\[ Prob(B \\mid A) =\\frac{Prob(A \\cap B)}{Prob(A)} = \\frac{Prob(B \\cap A)}{Prob(A)} \\] \\[ Prob(A \\cap B) = Prob(B \\mid A) Prob(A) \\] \\[ Prob(A \\cap B) = \\frac{n(B)}{n(A)} \\frac{n(A)}{n(S)} = \\frac{n(B)}{n(S)} \\] But then we also have \\[ Prob(B \\cap A) = Prob(A \\mid B) Prob(B) \\] \\[ Prob(B \\cap A) = \\frac{n(A)}{n(B)} \\frac{n(B)}{n(S)} = \\frac{n(A)}{n(S)} \\] But from our work on contingency tables we also know that $ Prob(B A) = Prob(A B)$, and this means that \\[ Prob(B \\cap A) = Prob(A \\mid B) Prob(B) = Prob(A \\cap B) = Prob(B \\mid A) Prob(A) \\] \\[ Prob(A \\mid B) Prob(B) = Prob(B \\mid A) Prob(A) \\] Bayes’ Rule (Theorem) \\[ Prob(A \\mid B) = \\frac{Prob(B \\mid A) Prob(A)}{Prob(B)} \\] Yes we can is the right answer. 11.6 Practice 1 Your company keeps detailed records of quality metrics for its manufacturing operations. At the gargleblaster plant in West Adelbrad the morning shift (“M”), 200 gargleblasters are defective (“D”) for every 100,000 produced. For the evening shift (“E”), 500 items are defective per 100,000 produced. In the average 24 hour period 1,000 items are produced by the morning shift and 600 by the evening shift. What is the probability that an item picked at random from the average total output produced in the 24 hour period: Was produced by the morning shift and is defective? Was produced by the evening shift and is defective? Was produced by the evening shift and is not defective? Is defective, whether produced by the evening or the morning shift? Is defective and was produced only by the morning shift? Is not defective and was produced only by the evening shift? show / hide What is the probability, \\(Prob()\\), that an item picked at random from the average total produced in the 24 hour period: Was produced by the morning shift and is defective? We are looking for the probability of output that is from “both the morning shift and is defective,” that is, \\(Prob(M \\cap D)\\). This probability is found from the multiplication law of probability which states \\[ Prob(M \\cap D) = Prob(M) Prob(D \\mid M) \\] and similarly for \\(Prob(E \\cap D)\\). The marginal probabilities of picking an item when produced by the morning shift (“M”) is \\[ Prob(M) = \\frac{1,000}{1,600} = 0.625 \\] and by the evening shift (“E”) \\[ Prob(E) = \\frac{600}{1,600} = 0.375 \\] These are marginal probabilities across both defective and not defective items. The probability of picking a defective item (“D”) from the morning shift is \\[ Prob(D \\mid M) = \\frac{200}{100,000} = 0.002 \\] This conditional probability examines the split of defective and not defective items within the sample of morning shift items. The conditional probability of picking a defective item from the sample of evening shift output is \\[ Prob(D \\mid E) = \\frac{500}{100,000} = 0.005 \\] The probability that an item is picked at random from a total of 1,600 items produced during a 24 hour period was produced by the morning shift and is defective is \\[ Prob(M \\cap D) = Prob(M) Prob(D \\mid M) = (0.625)(0.002) = 0.00125 \\] Was produced by the evening shift and is defective? \\[ Prob(E \\cap D) = Prob(E) Prob(D \\mid E) = (0.375)(0.005) = 0.001875 \\] Was produced by the evening shift and is not defective? The conditional probability that given the evening shift output only, the probability of no defects (\\(\\neg\\) means “not”) is \\[ Prob(\\neg D \\mid E) = 1 - Prob(D \\mid E) = 1 - 0.005 = 0.995 \\] so we then have \\[ Prob(E \\cap \\neg D) = Prob(E) Prob(\\neg D \\mid E) = (0.375)(0.995) = 0.373125 \\] Is defective and produced by the morning or the evening shift? The expected number of defective items from the morning shift equals the probability of a defective item from the morning shift times the number of items produced only by the morning shift. \\[ Prob(D \\mid M)count(M) = (0.002)(1,000) = 2 \\] defective items. From the evening shift we expect \\[ Prob(D \\mid E)Count(E) = (0.005)(600) = 3 \\] defective items. Thus we expect \\(2 + 3 = 5\\) defective items to be produced during a 24 hour period. If there were to be 5 defective items, then the probability of a random pick of 5 defective items out of a total of 1,600 items is \\[ Prob(D) = 5/1,600 = 0.003125 \\] Is defective and was produced only by the morning shift? We are looking for \\(Prob(M \\mid D)\\). We already have \\(Prob(D \\mid M)\\). Here we can use Bayes’ theorem to retrieve \\(Prob(M \\mid D)\\). Let’s derive Bayes’ theorem first. We already know that according to the multiplication law of probability we have \\[ Prob(D \\cap M) = Prob(M)Prob(D \\mid M) \\] Dividing both sides of this equation by \\(Prob(M)\\), and rearranging, gives us a formula for conditional probability \\[ Prob(D \\mid M) = \\frac{Prob(D \\cap M)}{Prob(M)} \\] We also know that \\(Prob(D \\cap M) = Prob(M \\cap D)\\) from our work on contingency tables. Thus substituting \\(Prob(M \\cap D)\\) for \\(Prob(D \\cap M)\\) we get \\[ Prob(D \\mid M) = \\frac{Prob(M \\cap D)}{Prob(M)} = \\frac{Prob(D)Prob(M \\mid D)}{Prob(M)} \\] Solving for \\(Prob(M \\mid D)\\) we finally have Bayes’ theorem \\[ Prob(M \\mid D) = \\frac{Prob(M)Prob(D \\mid M)}{Prob(D)} \\] For our data, \\(Prob(M) = 0.625\\), \\(Prob(D \\mid M) = 0.002\\), and \\(Prob(D) = 0.003125\\), so we have \\[ Prob(M \\mid D) = \\frac{(0.625)(0.002)}{0.003125} = 0.60 \\] The probability that a defective item picked at random from the total 24 hour output is produced by the morning shift is 60%. This probability expresses the contribution of defective items by the morning shift only. Is not defective and was produced only by the evening shift? Now we look for \\(Prob(E \\mid \\neg D)\\). Using Bayes’ theorem we have \\[ Prob(E \\mid \\neg D) = \\frac{Prob(E)Prob(\\neg D \\mid E)}{Prob(\\neg D)} \\] For our data, \\(Prob(E) = 0.375\\), \\(Prob(\\neg D \\mid E) = 1 - 0.005 = 0.995\\), and \\(Prob(\\neg D) = 1- 0.003125 = 0.996875\\), so we have \\[ Prob(E \\mid \\neg D) = \\frac{(0.375)(0.995)}{0.996875} = 0.3742947 \\] The probability that there are no defective items picked at random from the total 24 hour output when produced by the evening shift is 37.43%. This probability expresses the contribution of not defective items by the evening shift only. 11.7 Practice 2 The marketing department of a everyday-low-price retailer is attempting to optimize marketing promotions. The department analysts estimate that approximately 1 in 50 potential buyers of a product will see the ad after hearing about the ad from a friend, and 1 in 5 sees a corresponding ad on the internet. One in 100 potential buyers will either hear about the ad or see it on the internet. One in 3 actually purchases the product after seeing the ad, while 1 in 10 without seeing it. What is the probability that a randomly selected potential customer will purchase the product? show / hide Define the following events: A: buyer hears about the ad from a friend (“word of mouth”) B: buyer sees only the internet ad C: buyer purchases the product What is the probability that the buyer hears about the ad through word of mouth? \\[ Prob(A) = \\frac{1}{50} = 0.02 \\] What is the probability that the buyer sees only the internet ad? \\[ Prob(B) = \\frac{1}{5} = 0.20 \\] What is the probability that a buyer knows about the ad? \\[ Prob(A \\cap B) = \\frac{1}{100} = 0.01 \\] What is the probability that a buyer hears about the ad and sees it on the internet? \\[ Prob(A \\cup B) = Prob(A) + Prob(B) - Prob(A \\cap B) = 0.02 + 0.20 - 0.01 = 0.21 \\] What is the probability that buyer does not see or does not hear about an ad? \\[ Prob(\\neg A \\cup \\neg B) = 1 - Prob(A \\cup B) = 1 - Prob(A \\cup B) = 1 - 0.21 = 0.79 \\] What is the probability that a buyer will purchase after knowing about the ad? \\[ Prob(C \\mid (A \\cup B)) = \\frac{1}{3} = 0.33 \\] What is the probability that a buyer will purchase without knowing about the ad? \\[ Prob(C \\mid (\\neg A \\cup \\neg B)) = \\frac{1}{10} = 0.1 \\] Finally, we get to answer the question: What is the probability that a buyer will purchase the item? There are two ways the buyer would purchase the item. One way is through the ad channel, so that the probability of buying is composed of the probability that a buyer hears about the ad and sees it on the internet and the probability that a buyer will purchase after knowing about the ad. Here \\(Ads = (A \\cup B)\\), so that \\[ Prob(Buying \\cap Ads) = Prob(A \\cup B)Prob(C \\mid (A \\cup B)) = (0.21)(0.33) = 0.0693 \\] The second channel is without knowledge of ads. The probability of buying is composed of the probability that a buyer does not hear about the ad and does not see it on the internet and the probability that a buyer will still purchase after not knowing about the ad. Here \\(\\neg Ads = (\\neg A \\cup \\neg B)\\) so that \\[ Prob(Buying \\cap \\neg Ads) = Prob(\\neg A \\cup \\neg B)Prob(C \\mid (\\neg A \\cup \\neg B)) = (0.79)(0.10) = 0.0790 \\] Combining the two channels we have \\[Prob(Buying) = 0.0693 + 0.0790 = 0.1483\\] The probability that a randomly picked customer buys the product is 14.83%. 11.8 Nothing is random! Well not quite as many outcomes are simply uncertain, and at least indiscriminately appearing before us. Suppose we there’s a 20% chance of rain: will it rain? it might (rain = 1, with probability 0-20%) it might not (rain = 0, with probability 80%) Rain as defined here is a so-called random variable. Any random variable (really a functional mapping) is a set of possible outcomes, each of which is assigned a probability of occurrence. a number of outcomes {1, 0} each outcome assigned a probability {20%, 80%} 11.8.1 Suppose 20% of the people in a city prefer Pepsi-Cola as their soft drink of choice. If a random sample of six people is chosen, the number of Pepsi drinkers could range from? show / hide zero to six Shown here are the possible numbers of Pepsi drinkers in a sample of six people and the probability of that number of Pepsi drinkers occurring in the sample. Drinkers Probability 0 0.262 1 0.393 2 0.246 3 0.082 4 0.015 5 0.002 6 0.000 Is there a random variable in our midst? show / hide YES! Let \\(D\\) equal the number of drinkers from zero to six. Each outcome has a probability of occurrence associated with the outcome. We have outcomes mapped to probabilities and thus we have a random variable. Actually we have a binomial process at work here: 6 drinkers taken from 0 to 6 at a time when there is a 20% probability of finding a Pepsi-Cola (YUCK!) drinker. Is this a discrete or a continuous random variable? show / hide It is discrete: a set of integers describes the outcomes. There is no number allowed here between any two of these outcomes. Why? It is simply nonsensical to have 1/3 of a drinker. Add a drinker to the mythical 1/3 drinker and you have a just as mythical 1 and 2/3 drinker, and so on. If we allowed parts of drinkers, then maybe we would have a continuous random variable. What is the probability that 2 or more drinkers are Pepsi-cola drinkers? show / hide First, the probability of either 0 or 1 drinker: \\(Prob(D &lt; 2) = 0.655\\) Second, the probability of 2 or more drinkers: \\(Prob(D \\geq 2) = 1 - Prob(D &lt; 2) = 1 - 0.655 = 0.345\\) What is the mean and standard deviation of the random variable drinkers of Pepsi, which we will call \\(D\\)? show / hide See this worksheet: Index Drinkers Probability Expected Outcome Expected Deviation Squared \\(i\\) \\(D_i\\) \\(P(D_i)\\) \\(P(D_i) D_i\\) \\(P(D_i) ( D_i - \\mu)^2\\) 0 0 0.262 0 0.377 1 1 0.393 0.393 0.016 2 2 0.246 0.492 0.157 3 3 0.082 0.246 0.266 4 4 0.015 0.061 0.118 5 5 0.002 0.008 0.029 6 6 0 0 0.000 where \\(\\mu = 1.2\\), \\(\\sigma = \\sqrt{variance} = 0.9813\\) "],
["confidence-intervals.html", "Chapter 12 Confidence Intervals 12.1 Imagine this… 12.2 Objectives 12.3 Try this 12.4 What about the sample standard deviation? 12.5 Explore a bit further 12.6 Confidence interval ##1: known population standard deviation 12.7 Our procedure 12.8 On to the unknown 12.9 By the way, who is Student? 12.10 Our procedure 12.11 Exercises", " Chapter 12 Confidence Intervals 12.1 Imagine this… Your team job is to handle the 579 current client billings in your team’s book of business. You only can contact 10 clients in the short time between now and when you must estimate the range of billings for a revenue forecast for your team’s managing director. Specifically, What is the expected level of billings? With a high degree of confidence, what is the range of billings we might expect? What is a team member to do? 12.2 Objectives With this unit, you will be able to: Understand the reason for estimating with confidence interval Calculate confidence intervals for population proportions Interpret a confidence interval Know the meaning of margin of error and its use Compute sample sizes for different experimental setting Know when and how to use t-score and interval to estimate the population mean Compute sample sizes for estimating the population mean 12.3 Try this What is a team member to do? Experiment! That’s what. Suppose there are only 10 billings. What if you take samples of 4 billings? There are \\(_{10}C_4 = 210\\) combinations of samples. Enumerate them all. Calculate means for each sample. Calculate the mean of the sample means. Compare with the mean of the population of the five experimental billings. show / hide In the simulation worksheet of the accompanying workbook is the experimental population of 10 billings and their mean. Next to the population are several samples of 4 each from this population with replacement. Means of each sample are tabulated along with the mean of the sample means. Below is the distribution the random variable, sample means. This figure along with summary statistics is in the summary worksheet. Table 12.1: Billing data: 10 observations mean std_dev median skewness kurtosis 139 37.4 138 0.103 2.04 Table 12.1: Sample Means: N = 10 , n = 4 , Samples = 210 mean std_dev median skewness kurtosis 140 17.2 139 0.095 2.84 Observations? Approximately symmetric Defined on + to - infinity Almost mesokurtic Mean of sample means is equal to the population mean Standard deviation of sample means is about half of the population standard deviation Recommendations? 12.4 What about the sample standard deviation? We supposed we had a experimental population of 10 billings. We just pulled several samples of sample size 4 from this population. We just found out that the mean of the sample means is the same as the population mean. This the same as saying that the point estimate of the mean of the sample means is unbiased. The samples are all pulled from a population with population mean \\(\\mu = 138\\) and has a population standard deviation of \\(\\sigma = 35.33\\). All of this means that each and every draw of each of the 4 sampled billings comes from a population distributed with a \\(\\mu = 138\\) and a population standard deviation of \\(\\sigma = 35.33\\). 12.5 Explore a bit further Each and every draw of each of the 4 sampled billings comes from a population distributed with a \\(\\mu = 138\\) and a population standard deviation of \\(\\sigma = 35.33\\). Each sample is a draw of 4 billings \\(X = \\{X_1, X_2, X_3, X_4\\}\\), where 1, 2, 3 and 4 are simply any four draws from the population. Each of the drawn Xs came from the experimental population of 10 billings. The mean of the samples is then \\[ \\bar X = \\frac{1}{4}(X_1+X_2+X_3+X_4) \\] The variance (square of the standard deviation) of the sample mean is then \\[ \\sigma_{\\bar X}^2 = \\left( \\frac{1}{4} \\right)^2 (\\sigma_{X_1}^2 + \\sigma_{X_2}^2 + \\sigma_{X_3}^2 + \\sigma_{X_4}^2) \\] The variance (square of the standard deviation) of the independently drawn (no intersection!) sum of the samples themselves is \\[ \\sigma_{(X_1+X_2+X_3+X_4)}^2 = \\sigma_{X_1}^2 + \\sigma_{X_2}^2 + \\sigma_{X_3}^2 + \\sigma_{X_4}^2 \\] But \\(\\sigma_{X_1}^2 + \\sigma_{X_2}^2 + \\sigma_{X_3}^2 + \\sigma_{X_4}^2 = 4\\sigma^2\\), four times the square of the population standard deviation. So that, \\[ \\sigma_{\\bar X}^2 = \\left( \\frac{1}{4} \\right)^2 4\\sigma^2 = \\frac{\\sigma^2}{4} \\] and for any sample size \\(n\\), we have the standard deviation of the sampled means as \\[ \\sigma_{\\bar X} = \\frac{\\sigma}{\\sqrt{n}} \\] For our experiment all of this indicates that the distribution of the sample means is ` 1. Approximately normally distributed with 2. Mean = population mean \\(\\mu = 138\\), and 3. Standard deviation \\(\\sigma_{\\bar X} = \\frac{35.33}{\\sqrt{4}} = 17.70\\) 12.6 Confidence interval ##1: known population standard deviation Our first “forecasting” exercise is upon us. If we know the population standard deviation we use the Normal sample means distribution to help us think about “confidence.” Out of all of the possible average billings, What is a range of expected billings such that the MD would have 95% confidence in the results? If the population standard deviation is known, then we can estimate expected billings such that \\(\\mu\\) is somewhere between a lower bound \\(\\ell\\) and an upper bound \\(u\\) \\[ \\ell \\leq \\mu \\leq u \\] Our beliefs will be a probabilistic calculation of the lower and upper bounds. Suppose our required confidence level is 95%. We have two tails which add up to the maximum probability of error, which we will call the \\(\\alpha\\) significance level. In turn \\(alpha\\) equals one minus the confidence level, which is \\(1- \\alpha = 0.95\\). For the two tail interval, calculate \\(1 - confidence = \\alpha = 1- 0.95 = 0.05\\), so that \\((1-\\alpha) / 2\\) for the amount of alpha in each tail. For \\(1- \\alpha = 95\\%\\) there is \\(\\alpha / 2 = 0.05/2 = 0.025\\) in each tail. The upper tail for the \\(\\alpha\\) confidence level begins at \\(1 - 0.025 = 0.975\\) cumulative probability or \\(97.5\\%\\). The lower tail for the \\(\\alpha\\) confidence level ends at \\(0.025\\) cumulative probability or \\(2.5\\%\\). 12.7 Our procedure We will base lower and upper bounds using the \\(z\\) score. Start with the \\(z\\) score and solve for the population mean \\(\\mu\\) and remembering that \\(z\\) can take on plus and minus values: \\[ z = \\frac{\\bar X - \\mu}{\\sigma / \\sqrt{n}} \\] \\[ \\mu = \\bar X \\pm z \\sigma / \\sqrt{n} \\] If the population standard deviation \\(\\sigma\\) is known then your belief about the size of the population mean \\(\\mu\\) may be represented by the normal distribution of sample means. Suppose you desire a alpha 95% confidence about the size of the population mean. Remember that in our experiment the sample size \\(n = 3\\).Then calculate \\(\\ell = \\bar X - z_{0.025}\\sigma / \\sqrt{n}\\), where \\(z_{0.025} =\\) NORM.INV(0.025,0,1) = -1.96, so that \\[ \\bar{X} - z_{0.025}\\sigma/\\sqrt{n} = 138.13 + (-1.96)(35.33 / \\sqrt{4}) = 138.13 - 34.6 = 104 \\] \\(u = \\bar{X} + z_{0.975}\\sigma/\\sqrt{n}\\), where \\(z_{0.975} =\\) NORM.INV(0.975,0,1) = 1.96, so that \\[ \\bar{X} + z_{0.975}\\sigma/\\sqrt{n} = 138.13 + (1.96)(35.33/ \\sqrt{4}) = 138.13 + 34.6 = 173 \\] Thus we are 95% confident that the expected billings \\(\\mu\\) lie in the interval: \\[ 104 \\leq \\mu \\leq 173 \\] 12.8 On to the unknown Let’s now suppose we do not know the population standard deviation. Now the sample standard deviation is also a random variable, like the sample mean. In practice this is nearly always the case. What do we do now? Use the Student’s t distribution to correct for confidences that are, well, not so confident. Here’s a plot of the Student’s t overlaid with the normal distribution. What do you notice? The normal (red) distribution is more pinched in than t (kurtosis? right!) Student’s-t (blue) distribution has thicker tails than normal Both are symmetrical Let’s check tail thickness: in Excel we can use =T.INV(2.5%,3) which returns -4.30, and where the degrees of freedom \\(df\\) of our 4 sample billings is \\(df = n - k = 4 - 1 = 3\\). Thus for the t distribution it takes 4.30 standard deviations below the mean to hit the 2.5% level of cumulative probability. It only took 1.96 standard deviations on the normal distribution. There are 3 degrees of freedom because it only takes 3 out of the 4 sampled billings to get the third sampled billing (we do this by using 1 estimator, the mean we calculated). That it took fewer standard deviations for the normal than for the t distribution to hit the 2.5% level of cumulative probability means that the t distribution is thicker tailed than the normal. 12.9 By the way, who is Student? “Guiness is Good for You” W. S. Gosset (1876-1937) was a modest, well-liked Englishman who was a brewer and agricultural statistician for the famous Guinness brewing company in Dublin. Guiness insisted that its employees keep their work secret, so he published the distribution under the pseudonym “Student” in 1908. This was one of the first results in modern small-sample statistics. 12.10 Our procedure We will base lower and upper bounds using the \\(t\\) score. Start with the \\(t\\) score and solve for the population mean \\(\\mu\\) and remembering that \\(t\\) can take on plus and minus values: \\[ t = \\frac{\\bar X - \\mu}{\\hat s / \\sqrt{n}} \\] \\[ \\mu = \\bar X \\pm t \\hat s / \\sqrt{n} \\] If the population standard deviation \\(\\sigma\\) is not known then your belief about the size of the population mean \\(\\mu\\) may be represented by the Student’s t distribution of sample means. Suppose you desire a 95% confidence about the size of the population mean. This means you have a \\((1 - 0.95)/2 = 0.025\\) \\(\\alpha\\) probability of error in mind. Remember that in our experiment the sample size \\(n = 4\\).Then calculate Instead of the population standard deviation \\(\\sigma\\), calculate the sample standard error \\(s\\). Suppose \\(s = 37.23\\). Given an 95% confidence level and 3 degrees of \\(\\ell = \\bar X - |t_{0.025}| \\hat s / \\sqrt{n}\\), where \\(t_{0.025} =\\) T.INV(0.025,3) = -3.18, and we take the absolute value of \\(t_{0.025}\\) since the \\(\\alpha\\) significance rate 2.5% is symmetrically positioned on the t distribution in each tail. We then have \\[ \\mu_{\\ell} = 138.13 - (3.18)(35.33 / \\sqrt{4}) = 82.11 \\] \\(u = \\bar X + t_{0.975}\\hat s / sqrt{n}\\), where \\(t_{0.975} =\\) T.INV(0.975,2) = 3.18, so that \\[ \\bar X + t_{0.975}\\sigma / \\sqrt{n} = \\bar X + (3.18)(35.33/ \\sqrt{4}) = 194.55 \\] Thus we are 95% confident that the expected billings \\(\\mu\\) lie in the interval: \\[ 82.11 \\leq \\mu \\leq 194.55 \\] Much wider an interval than if we knew the population standard deviation! 12.11 Exercises The Hiatus retail outlet takes a random sample of 25 customers from a segment population of 1,000 with a mean average transaction size of $80 normally distributed with a known population standard deviation of $20 per transaction. Find The 90% confidence interval for transaction size, and The 95% confidence interval for transaction size, and The 99% confidence interval for transaction size. What do these results indicate for management? show / hide \\(\\mu\\) is between 70 and 90 with 90% confidence, between 68 and 92 with 95% confidence, and between 65 and 95 with 99% confidence. A compensation analyst for an investment bank wants to estimate the mean hourly wages of several hundred employees in the first 5 pay bands plus or minus within plus or minus $20. Management wants a 99% confidence level for the analysis. Assume that the population standard deviation is known to be $40 and that hourly wages are normally distributed. Find the minimum sample size required for this analysis. show / hide 27 EXTRA: Find the confidence intervals for 1 if the population is not known and the sample standard deviation is $23 per transaction. Find the minimum sample size required for 2 if the population is not known and a sample standard deviation is $34. "],
["more-inference-hypothesis-testing.html", "Chapter 13 More Inference: Hypothesis Testing 13.1 Imagine this 13.2 On to the unknown 13.3 On with our story… 13.4 What about two shifts? 13.5 Exercises", " Chapter 13 More Inference: Hypothesis Testing 13.1 Imagine this An online startup company carefully and thoroughly searches documents on behalf of clients from a variety of domains including science, engineering, healthcare, and finance. The company has access to a huge corpus of information for each domain. Researchers express their information needs through queries in terms of topics, categories, and individual words, as well as by reference through citations of work product. Queries are input to a search engine, the output of which matches queries with information objects in the corpus. The company measures its performance in terms of Information retrieval coverage as the fraction of objects the searcher discovers, and The fraction of objects that are relevant to the search. 13.1.1 Two errors are possible Retrieval faces off with relevance. Two errors are possible: Retrieved Not Retrieved Revelant OK False Negative Not Relevant False Positive OK Type I Error: the False Negative means the researcher did not retrieve relevant information. Type II Error: the False Positive means that the research retrieved irrelevant information. How can the research company control for error? Be specific: How can the company ensure that the number of retrieved and relevant documents per day is as high as possible? 13.1.2 Control is probability Here is what the company does: Management makes an assumption and forms a hypothesis about the average rate of documents found in searches. This is a precise statement about a specific metric. Here the metric is the average number of retrieved and relevant documents per day, \\(\\mu\\), Suppose this target level is 1000 documents per day. The null hypothesis (\\(H_0\\)) is that the population metric equals a target value \\(\\mu_0\\) or \\(H_0: \\mu = \\mu_0\\). Suppose that \\(H_0: \\mu = 1000\\). The alternative hypothesis (\\(H_1\\)) is that the population metric does not equal (or is just greater or less than) the target value. Thus we would have \\(H_1: \\mu \\neq 1000\\). Corporate policy sets a degree of confidence in accepting as true the assumption or hypothesis about the metric. The company determines that 95% of the time \\(\\mu = 1000\\). This means there is an \\(\\alpha =\\) 5% significance that the company would be willing to be wrong about rejecting that \\(H_0: \\mu = 1000\\) is true. Under the null hypothesis it is probable that above or below a mean value of 1000 there is an error of \\(\\alpha = 0.05\\) in total, or \\(\\alpha / 2 = 0.025\\) above and \\(\\alpha / 2 = 0.025\\) below the mean. Because management expresses the alternative hypothesis, \\(H_1: \\mu \\neq 1000\\), as “not equal” then this translates into a two-tailed test of the null hypothesis. What if management expressed the alternative hypothesis as \\(H_1 &gt; 1000\\)? show / hide If \\(H_1: \\mu &gt; 1000\\), then management in effect specifies a one-tailed test. This means that management believes that under the null hypotheses \\(H_0:\\, \\mu = 0\\), that the distribution of documents per day for which the null hypothesis is probably true extends from the lowest number of documents all the way up to the 95%tile of occurrences of the number of documents per day. The region of the distribution beyond the 95%tile is 5% of the time and represents the highest range of documents per day. Similarly for the \\(H_1:\\,\\mu&lt;1000\\) case. 13.2 On to the unknown Let’s now suppose we do not know the population standard deviation. Now the sample standard deviation is also a random variable, like the sample mean. In practice this is nearly always the case. What do we do now? Use the Student’s t distribution to correct for confidences that are, well, not so confident. Here’s a plot of the Student’s t overlaid with the normal distribution. What do we notice? Normal is more pinched in than t (kurtosis? right!) t has thicker tails than normal Let’s check that: in Excel use =T.INV(2.5%,3) which returns -3.18, and where the degrees of freedom \\(df\\) of our 4 sample billings from our work in confidence intervals is \\(df = n - k = 4 - 1 = 3\\). Here \\(n\\) is the sample size of 4 rnadomly sampled billings and \\(k\\) is the number of estimators we are building, just one in this case \\(\\mu\\). Thus for the t distribution it takes 3.18 standard deviations below the mean to hit the 2.5% level of cumulative probability. It only took 1.96 standard deviations on the normal distribution. There are \\(k=3\\) degrees of freedom because it only takes 3 out of the 4 sampled billings to get the third sampled billing (we do this by using 1 estimator, the mean we calculated). That it took fewer standard deviations for the normal than for the t distribution to hit the 2.5% level of cumulative probability means that the t distribution is thicker tailed than the normal. 13.2.1 By the way, who is Student? “Guiness is Good for You” W. S. Gosset (1876-1937) was a modest, well-liked Englishman who was a brewer and agricultural statistician for the famous Guinness brewing company in Dublin. Guiness insisted that its employees keep their work secret, so he published the distribution under the pseudonym “Student” in 1908. This was one of the first results in modern small-sample statistics. 13.3 On with our story… When management does not know the population standard deviation, the analyst must use the Student’s t distribution to correct for small sample sizes. As this is almost always the case for hypothesis testing, management has decreed that the Student-t distribution will be used for hypothesis testing. CONTINUED — management decides on regions of the distribution for acceptance that the null hypothesis is probably true and for rejection of the null hypothesis as well. This picture tells those and about 900+ more words. Management takes a random sample of \\(n = 100\\) searches. An analyst then computes the sample average \\(\\bar X = 980\\) of retrieved and relevant searches with a standard deviation of \\(s = 80\\), meant to represent the very unknown population \\(\\sigma\\). They then compute the \\(t\\) score, just like the z-score for the normal distribution: \\[ t = \\frac{\\bar X - \\mu_0}{s / \\sqrt{n}} = \\frac{980 - 1000}{80 / \\sqrt{100}} = -2.5 \\] and compare this value with the the acceptance region of the null hypotheses \\(H_0\\). So, what is this value? For a sample size of \\(n = 100\\) and \\(k = 1\\) estimator (\\(\\bar X\\)), the degrees of freedom \\(df = n - k = 100 - 1\\). Under a Student’s t distribution with 99 \\(df\\), and using Excel’s =T.INV(0.025, 99), the region is bounded by t scores between \\(-1.98\\) and \\(+1.98\\). The computed t score is -2.5 and falls in the rejection region of the null hypothesis. The analyst can report that she is 95% confident that management may reject the null hypothesis that reseachers retrieve 1,000 relevant documents each day. Another way of reporting this is that there is a 5% probability that management would be wrong in concluding that researchers do not retrieve 1,000 relevant documents each day. 13.4 What about two shifts? Now management wants to know how two different shifts of researchers compare. Specifically, management has been assuming that the day shift (shift 1) retrieves more relevant documents than the night shift (shift 2). The analyst formulates the null hypothesis that the mean relevant retrieved documents in one day is the same for both shifts, and thus their difference is zero, or as \\[ H_0: \\mu_1 - \\mu_2 = 0 \\] and the alternative hypothesis as \\[ H_1: \\mu_1 - \\mu_2 &gt;0. \\] This is a one-tailed test where the \\(\\alpha = 0.05\\) significance level region of the rejection of the null hypothesis \\(H_0\\) is entirely in the upper tail of the Student’s t distribution. The number of degrees of freedom now equal all of the observations from the night and the day shift minus the number of estimators, now equal to 2, or, \\(n_1 + n_2 - 2\\). If the analyst samples \\(n_1 = 45\\) searches from the day shift and \\(n_2 = 54\\) searches from the night shift, then the number of degrees of freedom is \\(n_1+n_2-2 = 99 - 2 = 97\\). The analyst samples 45 searches in the day shift (shift 1) and 54 searches in the night shift (shift 2), and uses the same \\(\\alpha = 0.05\\) significance level for rejection of the null hypothesis. She estimates \\(\\bar X_1 = 600\\) with \\(s_{\\bar X_1}=60\\) \\(\\bar X_2 = 540\\) with \\(s_{\\bar X_2}=68\\). Her next job is to pool (also known as “aggregate”) the standard deviations together since the risk associated with the null hypothesis relates to two pooled sample means \\(\\bar X_1 - \\bar X_2 = 600 - 540 = 60\\). The analyst assumes that the two samples are not at all correlated with one another. \\[ s_{\\bar X_1 - \\bar X_2 } = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}} \\] \\[ = \\sqrt{\\frac{60^2}{45}+\\frac{68^2}{54}} = 12.87 \\] and calculates the \\(t\\) score as \\[ t = \\frac{(\\bar X_1 - \\bar X_2) - (\\mu_{0,1} - \\mu_{0,2})}{s_{\\bar X_1 - \\bar X_2 }} = \\frac{60 - 0}{12.87} = 4.66 \\] A \\(t\\) score of 4.66 means that the score, and the difference between the means, is in the region of rejection of the null hypothesis. There is at most a 5% chance that management is wrong in its assertion that the day shift out performs the night shift. Another interpretation is possible. Using Excel the analyst can calculate = 1 - T.DIST(4.66, 97) = 0.0005% which is the so-called p-value or cumulative probability greater than \\(t=4.66\\) that she was wrong about the rejection of the null hypothesis, very slim indeed. The analyst compares the p-value with the significance level of 5% and sees that there is an even slimmer chance of a Type I false negative error than indicated simply by looking at the rejection region. 13.5 Exercises An electric car manufacturer buys aluminum alloy sheets of 0.05 of an inch in thickness. Thick sheets are too heavy and thin sheets imbalance the axle loads on icy and rainy road surfaces. The purchasing officer along with a manufacturing engineer samples 100 sheets of a shipment before accepting it and calculates an average of 0.048 inches in thickness with a standard deviation of 0.01 of an inch. At a 5% level of significance, should the purchasing officer accept the shipment? What is the probability that the purchasing officer is wrong about rejecting the null hypothesis? A real estate developer is comparing construction wages in two markets. In New York, using a random sample of 150 workers, the average daily wage is $1,800 with a standard deviation of $500 per day. In Los Angeles, for the same skills and experience, a random sample of 125 workers yields a daily wage average of $1,700 per day with a standard deviation of $450 per day. Is there a significant difference in wage levels between the two cities at the 5% level? What is the probability of being wrong about rejecting the null hypothesis? "],
["simple-linear-regression.html", "Chapter 14 Simple Linear Regression 14.1 The perfect relationship 14.2 Linear regression 14.3 Ordinary least squares 14.4 Residual interests 14.5 A dash of calculus 14.6 Now for the residuals 14.7 Have some confidence 14.8 Central to our discussion 14.9 On to the unknown 14.10 By the way, who is Student? 14.11 In all confidence: forecasting for fun and … 14.12 How confident are we in our estimates? 14.13 Next let’s hypothesize 14.14 Yet another school of thought 14.15 Summary of simple linear regression estimation 14.16 How good a fit? 14.17 Let’s build more statistics! 14.18 Analyze this … 14.19 Two samples – same population? 14.20 Anything abnormal? 14.21 Exercises 14.22 References", " Chapter 14 Simple Linear Regression 14.1 The perfect relationship This figure shows two variables whose relationship can be modeled perfectly with a straight line. The equation for the line is \\[ y = 10 + 20x \\] The line cross the vertical y-axis at \\(y=10\\) and \\(x = 0\\). The slope is \\(20\\), that is, if \\(x\\) increases by one unit, then \\(y\\) increases by \\(20\\) units. Imagine what a perfect linear relationship would mean: You would know the exact value of \\(y\\) just by knowing the value of \\(x\\). This is unrealistic in almost any natural process. For example, if we took family income \\(x\\), this value would provide some useful information about how much financial support \\(y\\) a college may offer a prospective student. However, there would still be variability in financial support, even when comparing students whose families have similar financial backgrounds. 14.2 Linear regression Linear regression assumes that the relationship between two variables, \\(x\\) and \\(y\\), can be modeled by a straight line: \\[ y = \\beta_0 + \\beta_1x \\] Here \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the straight line. It is rare for all of the data to fall on a straight line, as seen in the three scatterplots in the next figure. In each case, the data falls around a straight line, even if none of the observations fall exactly on the line. The first plot shows a relatively strong downward linear trend, where the remaining variability in the data around the line is minor relative to the strength of the relationship between \\(x\\) and \\(y\\). The second plot shows an upward trend that, while evident, is not as strong as the first. The last plot shows a very weak downward trend in the data, so slight we can hardly notice it. In each of these examples, we will have some uncertainty regarding our estimates of the model parameters, \\(\\beta_0\\) and \\(\\beta_1\\). For instance, we might wonder, should we move the line up or down a little, or should we tilt it more or less? 14.3 Ordinary least squares Here we begin to calculate the intercept and slope of that linear relationship we just looked at. The way to do this is to minimize the a measure of the errors (residuals) around this line. The traditional approach calculated deviations of the model from the dependent variable, then squares these deviations, and finally looks for the intercept and slope that minimizes the sum of the squared deviations. All of this is due to Carl Friedrich Gauss. It was later called “ordinary” least squares. There are definitely variants that are far from “ordinary” in the menagerie of statistical techniques. 14.4 Residual interests Every \\((x_i,y_i)\\), for \\(i = 1 \\dots N\\) points in the scatterplots above can be conceived a straight line plus or minus a “residual” \\(\\varepsilon\\) \\[ y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i \\] Here we conceive of the \\(\\beta_0\\) and \\(\\beta_1\\) as population parameters and \\(\\varepsilon_i\\) as a population variate, a sample of which produces estimates \\(b_0\\) and \\(b_1\\). The job at hand is to find the unique combination of \\(b_0\\) and \\(b_1\\) such that all of \\(N\\) sample observations of the \\(\\varepsilon_i\\) taken together are as small a distance from \\((x_i,y_i)\\) to the straight line as possible. Let’s look at simple data set before we go any further.Here is data from 10/1/2016 through 9/1/2017 on real consumption and disposable income from FRED. First a scatterplot. We have 12 monthly observations of two variables, consumption and income. Second, suppose we think that the marginal propensity to consume out of disposable income is 0.9 and that even at zero disposable income, the residents of the U.S. would still consume 136. Thus let’s run this straight line through the scatter of data. \\[ \\hat{Y} = b_0 + b_1 X \\] \\[ \\hat{Y} = 136 + 0.9 X \\] where \\(\\hat{Y}\\) is our estimated model of consumption versus income \\(X\\). Don’t confuse \\(X\\) as income here with the \\(X\\) from our scatter plot story above! \\(Y\\) is also called the dependent variable \\(X\\) is the independent variable Some points are practically on the line, others are pretty far away. The vertical distance from a consumption-income point to the line is the residual. Our next step: draw error bars to visualize the residuals. Each sample residual \\(e_i\\) is calculated from the model for consumption: \\[ Y_i = \\hat{Y_i} + e_i \\] \\[ Y_i = b_0 + b_1 X_i + e_i \\] \\[ Y_i = 136 + 0.9 X_i + e_i \\] That is, \\[ e_i = Y_i - (136 + 0.9 X_i) \\] What we really want is to have a line go through this data such that it is the best line. We define “best” as the smallest possible sum of squared residuals for this data set and a straight line that runs through the scatterplot. We are looking for estimates of \\(\\beta_0\\) and \\(\\beta_1\\), namely \\(b_0\\) and \\(b_1\\) that minimize the sum of squared residuals \\(SSR\\). Let’s remember that there are as many possible estimates \\(b_0\\) and \\(b_1\\) as there are potential samples from the population of all consumption-income combinations. \\(SSE\\) is the sum of squared residual errors \\[ SSE = e_1^2 + e_2^2 + \\dots + e_N^2 \\] \\[ SSE = \\sum_{i=1}^{N}\\varepsilon_i^2 \\] Substitute our calculation for \\(\\varepsilon_i\\). Our job is to find the \\(b_0\\) and \\(b_1\\) that minimizes \\[ SSE = \\sum_{i=1}^{N}[Y_i - (b_0 + b_1 X_i)]^2 \\] for all \\(N\\) observations we sampled from the consumption (\\(Y\\)) - income (\\(X\\)) population. 14.5 A dash of calculus Now let’s find the best \\(b_0\\) and \\(b_1\\). To do this recall (with great affection!) the following two rules of differentiation (yes, the calculus). Suppose we have a function \\(u = v^2\\). Then \\[ \\frac{du}{dv} = 2v^{2-1} = 2v^1 = 2v \\] Not so bad! But let’s mix it up a bit and suppose we have another function \\(w = (1-v^2) = w(u(v))\\)? We need to use the chain rule of differentiation of a function of a function to get at a derivative. The rule is this: if w(v) = w(u(v)), then \\[ \\frac{dw}{dv} = \\frac{dw}{du}\\frac{du}{dv} \\] We already know what \\(du/dv = 2v\\). What is \\(dw/du\\)? If we let \\(u=v^2\\), then \\(w=1-u\\) \\[ \\frac{dw}{du}=-1 \\] That’s it. Putting the two derivatives together we have \\[ \\frac{dw}{dv} = \\frac{dw}{du}\\frac{du}{dv} = (-1)(2v) = -2v \\] Back to the \\(SSE\\) story, We have 12 terms like this \\[ SSE_i = (Y_i - b_0 - b_1 X_i)^2 \\] Overall we have two variables for which we want together to minimize \\(SSR\\). We take them one at a time, holding the other “constant.” We take the “partial” derivative to accomplish this task. First, for \\(b_0\\) and for each \\(i\\). \\[ \\frac{\\partial SSE_i}{\\partial b_0} = -2(Y_i - b_0 - b_1 X_i) \\] Then we calculate the partial of \\(SSE_i\\) with respect to \\(b_1\\). \\[ \\frac{\\partial SSE_i}{\\partial b_1} = -2X_i(Y_i - b_0 - b_1 X_i) \\] we can summarize the overall effect of changing first \\(b_0\\) and then \\(b_1\\) by summing the partial derivatives for \\(i=1 \\dots N\\), where \\(N=12\\) in our consumption-income example. Here are the first order conditions (FOC) around \\(SSE(b_0,b_1)\\): \\[ \\frac{\\partial SSE_i}{\\partial b_0} = -2\\sum_{i=1}^{N}[Y_i - (b_0 + b_1 X_i)] = 0 \\] \\[ \\frac{\\partial SSE_i}{\\partial b_1} = 2\\sum_{i=1}^{N}[Y_i - (b_0 + b_1 X_i)](-X_i) = 0 \\] Here we have factored out the \\(-2\\) across the sum of residuals. We can solve these two simultaneous equations for \\(b_0\\) and \\(b_1\\) to get \\[ b_0 = \\frac{\\sum_{i=1}^N Y_i}{N} - b_1 \\frac{\\sum_{i=1}^N X_i}{N} \\] \\[ b_1 = \\frac{N\\sum_{i=1}^N X_i Y_i - \\sum_{i=1}^N X_i \\sum_{i=1}^N Y_i}{N\\sum_{i=1}^N X_i^2 - (\\sum_{i=1}^N X_i)^2} \\] Next we perform some arithmetic. Here is a table of sums we need: \\[ \\begin{center} \\begin{tabular}{c|l|r} \\hline term &amp; Excel name &amp; result\\\\ \\hline $N$ &amp; `n` &amp; 12 \\\\ $\\sum_{i=1}^N Y_i$ &amp; `sumY` &amp; 141.7 \\\\ $\\sum_{i=1}^N X_i$ &amp; `sumX` &amp; 152.6 \\\\ $\\sum_{i=1}^N X_i Y_i$ &amp; `sumXY` &amp; 1801.7\\\\ $\\sum_{i=1}^N X_i^2$ &amp; `sumX2` &amp; 1939.8\\\\ \\hline \\end{tabular} \\end{center} \\] We insert these amounts into the formulae for \\(b_0\\) and \\(b_1\\). We start with \\[ b_1 = \\frac{n \\times sumXY - (sumX) \\times (sumY)}{n \\times sumX2 - (sumX)^2} \\] This translates into the following result: \\[ b_1 = \\frac{12 \\times 1801.7 - 152.6 \\times 141.7 }{12 \\times 1939.8- (152.6)^2} = 0.918 \\] And then we get \\[ b_0 = sumY/N - b_1 \\times sumX/N \\] \\[ b_0 = 141.7/12 - 0.918 (152.6/12) = 0.136 \\] We can summarize our results using our understanding of macroeconomics: The marginal propensity to consume out of disposable income is 91.8%. The rest is “savings.” Structural consumption, almost an idea of permanent consumption, is $136 billion over this sample period. 14.6 Now for the residuals From our definition of residuals we can compute \\[ e_i = Y_i - b_0 -b_1 X_i \\] The sample mean of the residuals is \\[ \\bar e = \\bar Y - b_0 - b_1 \\bar X \\] \\[ = \\bar Y - (\\bar Y - b_1 \\bar X) - b_1 \\bar X \\] \\[ = (\\bar Y - \\bar Y) + b_1 \\bar X - b_1 \\bar X = 0 \\] The mean of resisuals, by definition, is just zero! The variance (standard deviation squared) of the residuals is \\[ var(e_i) = s_e^2 = \\frac{\\sum_{i=1}^N (e_i - \\bar e)^2}{n - k} = \\frac{\\sum_{i=1}^N e_i^2}{N - k} \\] so that the standard error is: \\[ s_e = \\sqrt{var(e)} \\] Here we have \\(k=2\\) sample estimators \\(b_0\\) and \\(b_1\\) and thus \\(n-k = 12 - 2 = 10\\) degrees of freedom. These are “freely” varying observations. This means that if we have 12 observations and have estimated 2 parameters, in effect, we have only 10 freely varying observations and can infer the other 2 by using the two estimated parameters. From our data \\[ var(e) = \\frac{\\sum_{i=1}^N e_i^2}{n - k} = \\frac{0.0171}{10} = 0.00171 \\] \\[ s_e = \\sqrt{var(e)} = \\sqrt{0.00171} = 0.0414 \\] Let’s construct a 95% prediction confidence interval around the ability of this model to predict consumption when we forecast a new level of disposable income. We know that the critical \\(t\\) scores for a two-tailed (2.5% in each tail) 95% confidence region are \\(+/- 2.2281\\). The variance of a forecasted level of consumption given an estimate of the forecasted level of disposable income \\(Y_F\\) is, through quite a bit of algebraic rearrangement, \\[ s_F^2 = s_e^2\\left[1 + \\frac{1}{N} + \\frac{(X_F - \\bar X)^2}{\\sum_{i=1}^N (X_i - \\bar X)^2}\\right] \\] \\[ = (0.00171)\\left[1 + \\frac{1}{12} + \\frac{(13 - 12.7139)^2}{0.0962}\\right] =0.003342 \\] \\[ s_F = \\sqrt{0.03342} = 0.0578 \\] The forecasted consumption is \\[ \\hat Y = Y_F = 0.136 + 0.0918 \\times 13 = 12.07 \\] 14.7 Have some confidence If we need confidence that must mean we are unsure about something. That something is the reliability of our forecast and the underlying estimates of \\(b_0\\) and \\(b_1\\). 14.8 Central to our discussion Take any distribution, say the Poisson with \\(\\lambda = 3\\). Run many, many samples of this distribution of size \\(x = 10\\). Calculate the sums, means (that is, the sums divided by the sample size \\(x\\)), and the variances (square of the standard deviation) of each sampling. Suppose now we take 10,000 samples. Here are the results visualized as distributions. The main take away is that with sampled means we get approximately normal distributions. A t-distribution is indicated when the sampled standard deviations are also random, as they appear to be here. 14.9 On to the unknown Let’s suppose we do not know the population standard deviation. Now the sample standard deviation is also a random variable, like the sample mean. In practice this is nearly always the case. What do we do now? Use the Student’s t distribution to correct for confidences that are, well, not so confident. Here’s a plot of the Student’s t overlaid with the normal distribution. What do we notice? Normal is more pinched in than the \\(t\\) (kurtosis? right!) \\(t\\) has thicker tails than normal Let’s check that: in Excel use =T.INV(2.5%, 10) which returns -2.23, and where the degrees of freedom \\(df\\) of our 12 sample observations is \\(df = n - k = 12 - 2 = 2\\). Thus for the t distribution it takes 2.23 standard deviations below the mean to hit the 2.5% level of cumulative probability. It only took 1.96 standard deviations on the normal distribution. There are 10 degrees of freedom because it only takes 10 out of the 12 sampled consumption-disposable income pairs to get the eleventh and twelth observations (we do this by using 2 estimators, the mean y-intercept \\(b_0\\) and the mean slope \\(b_1\\) we calculated). That it took fewer standard deviations for the normal than for the t distribution to hit the 2.5% level of cumulative probability means that the t distribution is thicker tailed than the normal. 14.10 By the way, who is Student? You may have seen the slogan “Guiness is Good for You.” William Gosset (1876-1937) was a modest, well-liked Englishman who was a brewer and agricultural statistician for the famous Guinness brewing company in Dublin. Guiness insisted that its employees keep their work secret, so Gosset published the distribution under the pseudonym “Student” in 1908. This was one of the first results in modern small-sample statistics. 14.11 In all confidence: forecasting for fun and … A confidence interval has the estimate in the middle with upper and lower bounds on the estimate determined by the number of standard deviations away from the estimate that are tolerable. If there is a tolarance of 5% error, then there are 2.5% tolerances above the upper and 2.5% tolerances below the lower bounds. Another way of putting this is to say that between the upper and lower bounds of the interval we are 95% confident in the range of the population forecast. All of this depends on the assumption that the residuals are normally distributed with zero mean and a constant standard deviation (variance). The 95/% confidence interval of the population forecast \\(Y_F\\) is this probability statement. \\[ Prob[\\hat Y - t_{0.025}s_F \\leq Y_F \\leq \\hat Y + t_{0.975}s_F ] = 0.95 \\] The lower bound of the population forecast of consumption \\(Y_F\\) is \\(t_{0.025}\\) standard deviations \\(s_F\\) below the estimated forecast \\(\\hat Y\\), and similarly for the upper bound. In Excel we can find the number of standard deviations from zero that corresponds to a probability of 2.5% using T.INV(0.025, 10), where \\(10\\) is the degrees of freedom \\(N - k = 12 - 2 = 10\\). This number is \\(-2.23\\). Because the \\(t-\\)distribution is symmetric about zero, we can use the absolute value of the same number in the upper end of the interval. \\[ Prob[12.07 - (2.23)(0.0578) \\leq Y_F \\leq 12.07 + (2.23)(0.0578) ] = 0.95 \\] \\[ Pr[11.9 \\leq Y_F \\leq 12.2] = 0.95 \\] There is a 95% probability that forecasted consumption conditional on a forecast of disposable income equal to $\\(13\\) trillion will lie between \\(\\$11.9\\) and \\(\\$12.2\\) trillion. 14.12 How confident are we in our estimates? Let’s put \\(b_0\\) and \\(b_1\\) into a form that will be really useful when we try to infer the confidence interval for these parameters. This form will also allow us to interpret the \\(b_1\\) estimate in terms of the correlation estimate \\(r_{cy}\\) and the consumption elasticity of income \\(\\eta_{cy}\\). Let’s start with \\[ b_1 = \\frac{N\\sum_{i=1}^N X_i Y_i - \\sum_{i=1}^N X_i \\sum_{i=1}^N Y_i}{N\\sum_{i=1}^N X_i^2 - (\\sum_{i=1}^N X_i)^2} \\] Multiply both sides by \\(1 = N^2 / N^2\\). This manuever will allow us to restate \\(b_1\\) in an algebraicly equivalent way (shout out to Huygens, the astronomer b. 1629). \\[ b_1 = \\frac{\\frac{\\sum_{i=1}^N X_i Y_i}{N} - \\left(\\frac{\\sum_{i=1}^N X_i}{N}\\right) \\left(\\frac{\\sum_{i=1}^N Y_i}{N}\\right)}{\\frac{\\sum_{i=1}^N X_i^2}{N} - \\left(\\frac{\\sum_{i=1}^N X_i}{N}\\right)^2} \\] We rearrange the numerator and denominator into this slightly neat(er) result. \\[ b_1 = \\frac{\\frac{\\sum_{i=1}^N X_i Y_i}{N} - \\bar{X}\\bar{Y}}{\\frac{\\sum_{i=1}^N X_i^2}{N} - \\bar{X}^2} \\] In turn we can express this as \\[ b_1 = \\frac{\\sum_{i=1}^N (X_i - \\bar{X})(Y_i - \\bar Y)}{\\sum_{i=1}^N (X_i - \\bar X)^2} = \\frac{cov(X,Y)}{ var(X)} \\] where \\(cov(X,Y)\\) is the covariance of \\(X\\) and \\(Y\\) and \\(var(X)\\) is the variance (standard deviation squared) of \\(X\\). Using this form we can derive the variance of the random variable \\(b_1\\). Here goes for consumption \\(Y\\) and disposable income \\(X\\): \\[ s_{b_1}^2 = \\frac{s_e^2}{\\sum_{i=1}^N (X_i - \\bar X)^2} = \\frac{0.00171}{0.0962} = 0.01777 \\] \\[ s_{b_1} = \\sqrt{0.01777} = 0.134 \\] with rounding. The 95% confidence interval for estimating the population parameter \\(\\beta_1\\) is this probability statement. \\[ Prob[b_1 - t_{0.025}s_{b_1} \\leq \\beta_1 \\leq b_1 + t_{0.025}s_{b_1} ] = 0.95 \\] \\[ Pr[0.918 - (2.23)(0.134) \\leq \\beta_1 \\leq 0.918 + (2.23)(0.134) ] = 0.95 \\] \\[ Pr[0.619 \\leq \\beta_1 \\leq 1.217] = 0.95 \\] There is a 95% probability that the population marginal propensity to consume out of disposable income will lie between \\(0.619\\) and \\(1.219\\). Decision makers might do well to plan for considerable movement in this number when formulating policy. Again the estimation cuts a wide swathe. This width may be the cause of the wide forecast interval for predicted consumption we noticed above. Let’s compute the variance of the random variable \\(b_0\\). Here it goes: \\[ s_{b_0}^2 = s_e^2\\left[\\frac{1}{N} + \\frac{\\bar X^2}{\\sum_{i=1}^N (X_i - \\bar X)^2}\\right] = 0.00171\\left(0.0833 + \\frac{12.71^2}{0.0962}\\right) = 2.9036 \\] \\[ s_{b_0} = \\sqrt{2.9036} = 1.701 \\] with rounding. Remember that \\(X\\) is the independent variable disposable income. The 95% confidence interval for estimating the population parameter \\(\\beta_0\\) is this probability statement. \\[ Prob[b_0 - t_{0.025}s_{b_0} \\leq \\beta_0 \\leq b_0 + t_{0.025}s_{b_0} ] = 0.95 \\] \\[ Prob[0.136 - (2.23)(1.701) \\leq \\beta_0 \\leq 0.136 + (2.23)(1.701) ] = 0.95 \\] \\[ Prob[-3.661 \\leq \\beta_0 \\leq 3.993] = 0.95 \\] There is a 95% probability that the population structural level of consumption (intercept term) will lie between \\(-3.661\\) and \\(3.993\\). We have further probable evidence that our estimation has a fairly high degree of uncertainty as parameterized by this probability statement for the \\(\\beta_0\\) confidence interval. 14.13 Next let’s hypothesize Herein we test the hypothesis that \\(b_0\\) and \\(b_1\\) are no different than zero. This is called the null hypothesis or \\(H_0\\). The alternative hypothesis or \\(H_1\\) is that the estimators are meaningful, namely, they do not equal zero. Two errors are possible True False Accept Correct II: False Negative Reject I: False Positive Correct Type I Error: A type I error occurs when the null hypothesis (\\(H_0\\)) is true, but is rejected. It is asserting something that is absent, a false hit. A type I error is often called a false positive (a result that indicates that a given condition is present when it actually is not present). Type II Error: A type II error occurs when the null hypothesis ($H_0) is false, but is accepted. It is asserting something that is there, but really is not. A type II error is often called a false negative (a result that indicates that a given condition is absent when it actually is present). How can we control for error? Here is what we can do: Management makes an assumption and forms a hypothesis about the population \\(\\beta_0\\) and \\(\\beta_1\\) estimates. This is a precise statement about two specific metrics. Let’s work with \\(\\beta_0\\). All the same can, and will be said of \\(\\beta_1\\). The null hypothesis (\\(H_0\\)) is that the population metric equals a target value \\(\\beta_0^*\\) or \\(H_0: \\beta_0 = \\beta_0^*\\). Suppose that \\(H_0: \\beta_0 = 0\\). The alternative hypothesis (\\(H_1\\)) is that the population metric does not equal (or is just greater or less than) the target value. Thus we would have \\(H_1: \\beta_0 \\neq 0\\). A decision maker sets a degree of confidence in accepting as true the assumption or hypothesis about the metric. The decision maker determines that 95% of the time \\(\\beta_0 = 0\\). This means there is an \\(\\alpha =\\) 5% significance that the company would be willing to be wrong about rejecting the assertion that \\(H_0: \\beta_0 = 0\\) is true. Under the null hypothesis it is probable that above or below a mean value of zero there is a Type I error of \\(\\alpha = 0.05\\) over the entire distribution of \\(b_0\\) or of \\(b_1\\). This translates into \\(\\alpha / 2 = 0.025\\) above and \\(\\alpha / 2 = 0.025\\) below the mean. Because management expresses the null hypothesis as “not equal,” then this translates into a two-tailed test of the null hypothesis. We have a sample of \\(N = 12\\) observations of consumption and disposable income. We then computed the sample estimate \\(b_0 = 0.136\\) for the average intercept with sample standard deviation \\(s_{b_0} = 1.701\\), and in trillions of USDs. Now compute the \\(t\\) score \\[ t = \\frac{b_0 - 0}{s_{b_0}} = \\frac{0.136 - 0}{1.701} = 0.0799 \\] and compare this value with the the acceptance region of the null hypotheses \\(H_0\\). For a sample size of \\(n = 12\\) and \\(k = 2\\) estimators (\\(\\bar X\\)), then the degrees of freedom \\(df = n - k = 12 - 2 = 10\\). Under a Student’s t distribution with 10 \\(df\\), and using Excel’s =T.INV(0.025, 10), the region is bounded by t scores between \\(-2.23\\) and \\(+2.23\\). The computed t score is 0.0799 and falls in the acceptance region of the null hypothesis \\(H_0: \\beta_0 = 0\\). We can now report that we are 95% confident that a decision maker may accept the null hypothesis that the consumption-income intercept is no different than zero. Another way of reporting this is that there is a 5% probability that we analysts could be wrong in concluding that the intercept is zero. 14.14 Yet another school of thought we could ask this question: If we know the t-score, what is the probability that any other t-scores are greater than this computed t-score? Find the p-value \\(= Pr(|t|)\\) If this p-value is “small” enough, then our parameter estimate is “far enough” away from zero to reject the null hypothesis. Compare with criterion \\(t^*\\) value that is the probability of being wrong about rejecting the null hypothesis and \\(1-Pr(t^*)\\) being right about accepting the alternative hypothesis. Calculating \\(\\dots\\) Use \\(1-\\)T.DIST(abs(score), df, TRUE) This gets us the probability from the score all the way to the end of the distribution (infinity and beyond!) This is the probability that you would be wrong if you accepted the null hypothesis \\(1-\\)p-value is the probability that you would be correcct if you rejected the null hypothesis (accepted the alternative hypothesis) For \\(b_0\\), \\(t = 0.080\\) Set acceptance criterion: at \\(t^* 1\\%\\) probability of being wrong about rejecting the null hypothesis Calculate \\(1-Pr(|t|)\\) = \\(1-\\)T.DIST(abs(0.0799), 10, TRUE) \\(=1-0.47 = 0.53\\) Test: p-value \\(&gt; 1\\%\\), therefore accept the null hypothesis that \\(b_0 = 0\\) For \\(b_1\\), \\(|t| = 6.8503\\) Set acceptance criterion: at \\(t^* = 1\\%\\) probability of being wrong about rejecting the null hypothesis Calculate \\(1-Pr(|t|)\\) = \\(1-\\)T.DIST(abs(6.8503), 10, TRUE) \\(=1- 0.999978 = 0.00002\\) Test: p-value \\(&lt; 1\\%\\), therefore accept the alternative hypothesis that \\(b_1 \\neq 0\\) 14.15 Summary of simple linear regression estimation For the model \\[ Y_i = b_0 + b_1 X_i + e_i \\] parameter estimator standard.deviation t.value \\(b_0\\) \\(\\bar Y - b_1 \\bar X\\) \\(\\sqrt{s_e^2\\left[\\frac{1}{N} + \\frac{\\bar X^2}{\\sum_{i=1}^N (X_i - \\bar X)^2}\\right]}\\) \\(\\frac{b_0}{s_{b0}}\\) \\(b_1\\) \\(\\frac{N\\sum_{i=1}^N X_i Y_i - \\left(\\sum_{i=1}^N X_i\\right) \\left(\\sum_{i=1}^N Y_i\\right)}{N\\sum_{i=1}^N X_i^2 - (\\sum_{i=1}^N X_i)^2}\\) \\(\\sqrt{\\frac{s_e^2}{\\sum_{i=1}^N (X_i - \\bar X)^2}}\\) \\(\\frac{b_1}{s_{b1}}\\) 14.16 How good a fit? Even though we just examined parameter-specific hypotheses: Is \\(b_0\\) far enough away from \\(0\\) to claim that \\(b_0 \\neq 0\\)? Is \\(b_1\\) far enough away from \\(0\\) to claim that \\(b_1 \\neq 0\\)? We still need to ask (and answer probably) Is the model any better than just noise? Are \\(b_0\\) and \\(b_1\\) jointly not far enough away from \\(0\\)? \\[ H_0: b_0 = b_1 = 0 \\] \\[ H_1: b_0, b_1 \\neq 0 \\] 14.17 Let’s build more statistics! To answer these pressing questions we need to look at the variations in the model. Calculate the total variation in \\(Y\\) (consumption \\(c\\)) as the “sum of squares total” or \\(SST\\) around its own mean \\(\\bar Y\\) \\[ SST = \\sum_{i=1}^N (Y_i - \\bar Y)^2 \\] Calculate the variation in the model itself from the average \\(Y\\) as the “sum of squares of the regression” of \\(SSR\\) \\[ SSR = \\sum_{i=1}^N ((b_0 + b_1 X_i) - \\bar Y)^2 \\] Calculate the variation in the error term (we already did this one!) as the “sum of squares of the error” or \\(SSE\\) \\[ SSE = \\sum_{i=1}^N e_i^2 \\] From the model’s point of view we have \\[ Y_i = b_0 + b_1 X_i + e_i \\] Subtract \\(\\bar Y\\) (average disposable income) from both sides to get \\[ Y_i - \\bar Y = b_0 + b_1 X_i + e_i - \\bar Y = (b_0 + b_1 X_i)- \\bar Y + e_i \\] We then square each side. But wait! Then we use a property of the model that there are no cross terms between the error \\(e_i\\) terms and the model. This means that the model measures variations that are in no way at all related to the error terms. They are thus independent of one another. We get this \\[ (Y_i - \\bar Y)^2 = (b_0 + b_1 X_i - \\bar Y)^2 + e_i^2 + 2(b_0 + b_1 X_i - \\bar Y)e_i \\] \\[ (Y_i - \\bar Y)^2 = (b_0 + b_1 X_i - \\bar Y)^2 + e_i^2 \\] Then sum it all up to get \\[ \\underbrace{\\sum_{i=1}^N (Y_i - \\bar Y)^2}_{SST} = \\underbrace{\\sum_{i=1}^N (b_0 + b_1 X_i - \\bar Y)^2}_{SSR} + \\underbrace{\\sum_{i=1}^N e_i^2}_{SSE} \\] That is \\[ \\underbrace{SST}_{total} = \\underbrace{SSR}_{explained} + \\underbrace{SSE}_{unexplained} \\] Forever and for all time. For our consumption-income data we have \\(SST = 0.098\\), \\(SSR = 0.082\\), \\(SSE = 0.017\\), so that \\[ SST = SSR + SSE \\] \\[ 0.098 = 0.082 + 0.017 \\] Now divide both sides by \\(SST\\) to get the proportion of total variation due to the two components: the model and the error \\[ \\frac{SST}{SST} = 1 = \\frac{SSR}{SST} + \\frac{SSE}{SST} \\] Now define the \\(R^2\\) statistic as the fraction of total variation that the regression “explains” or \\[ R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} \\] \\[ R^2 = \\frac{0.082}{0.098} = 1 - \\frac{0.017}{0.098} = 0.84 \\] In words: our model with disposable income explains 84% of the total variation in the consumption data. How good is this? We had to ask \\(\\dots\\)! 14.18 Analyze this … We have just decomposed the total variation into two components: the model and the error term (explained and the unexplained) ratios of model variation and error variation to the total variation (\\(R^2\\)) The null hypothesis is the average variation in the model is no different from zero. This means that under the null hypothesis, the model does not explain anything at all: \\[ H_0: b_0 = b_1 = 0 \\] It’s all just noise. Our job now is to compare the regression variation with the error variation. If the regression variation is very small relative to the error variation then we (probably) have reason to accept the null hypothesis that the model explains nothing much at all. We calculate (SS = Sum of Squares, df = degrees of freedom, MS = Mean Square, F = (F)isher’s statistic) Let’s dig into the \\(F\\) statistic. Just like the \\(t\\) statistic it is a “score” Measures the relative variation of two sums of squares Student’s-t composed of the ratio of a normal distribution to a chi-squared distribution Different from the Student’s-t: composed of the ratio of two chi-squared distributions, each with a different degree of freedom \\[ F(k-1, N-k) \\approx \\frac{SSR / (k-1)}{SSE / (N-k)} \\] Set the probability that we are wrong about accepting the null hypotheses \\(= 1\\%\\) Calculate \\(F = &quot;explained&quot; variation / &quot;unexplained&quot; variation = MSR / MSE = 46.3\\) Calculate the p-value using \\(1-\\) F.DIST(46.9, 1, 10, TRUE) \\(= 1 - 0.999955 = 0.000045\\) Reject the null hypothesis with a probability that you might be wrong \\(0.0045\\%\\) of the time and accept the alternative hypothesis that this model with disposable income sufficiently explains the variation in total consumption \\(99.99\\%\\) of the time. 14.19 Two samples – same population? We often take samples of the same variable at different times or as subsets of a larger pool of observations. Suppose we wonder if the marginal propensity to consume out of disposable income was different before the Volker era of the Federal Reserve, say, early 1980, versus long after, in late 2017. To do this we take two samples of consumption-income at two different times: 1980’s and the very recent past. We find that \\[ \\begin{center} \\begin{tabular}{c|c|c|c|c} sample &amp; parameter &amp; estimate &amp; standard deviation &amp; sample size \\\\ \\hline 1980 &amp; $b_1$ &amp; 0.86 &amp; 0.201 &amp; 14 \\\\ \\hline 2017 &amp; $b_1$ &amp; 0.92 &amp; 0.134 &amp; 12 \\\\ \\hline \\end{tabular} \\end{center} \\] Here is a procedure we can follow: Set the significance level to 1% (or some other level). Form the null and alternative hypotheses: \\[ H_0: \\beta_{1,2017} = \\beta_{1,1980} \\] \\[ H_1: \\beta_{1,2017} \\neq \\beta_{1,1980} \\] where the \\(\\beta_1\\)s are the population parameters for the marginal propensity to consume out of disposable income. This formulation is equivalent to \\[ H_0: \\beta_{1,2017} - \\beta_{1,1980} = 0 \\] \\[ H_1: \\beta_{1,2017} - \\beta_{1,1980} \\neq 0 \\] Calculate the pooled standard deviation of the two samples as \\[ s_{pool} = \\sqrt{s_{1,2017}^2 + s_{1,1980}^2} \\] \\[ s_{pool} = \\sqrt{0.134^2 + 0.501^2} = 0.242 \\] Calculate the t-ratio \\[ t = \\frac{b_{1,2017}-b_{1,1980}}{s_{pool}} = \\frac{0.92 - 0.86}{0.242} = 0.248 \\] Correcting for the two regression error standard deviations embedded in each of the two standard deviations of the \\(b_1\\)s, the degrees of freedom are \\[ df_{pool} = (N_{2017} - 2) + (N_{1980} - 2) = (12-2)+(14-2) = 22 \\] Calculate \\(Pr(&gt;|t|)\\) using =1 - T.DIST(0.248, 22, TRUE) \\(=1-0.594 = 0.406\\), the cumulative probability in the tail of the distribution. Accept the null hypothesis and reject the alternative hypothesis since \\[ Pr(&gt;|t|)= 0.406 &gt; 0.01 \\] far in excess of the significance level. We would be wrong (probably) over 40% of the time if we were to reject the null hypothesis that the two marginal propensities to consume out of disposable income were equal. 14.20 Anything abnormal? We have assumed thruoghout our statistical inference that underlying variables and their statistical estiamtes are normally distributed. Is this so? This doen’t look very symmetric with thin tails. mean median skewness kurtosis error statistics 0.001 -0.003 0.998 3.19 The mean and median are fairly close together. There is some positive (right side) skewness. Kurtosis is not very far from mesokurtic value of 3.0 for the normal distribution. All in all, the errors do not look so non-normal after all. We can bootstrap a confidence interval for the kurtosis of the error term by creating a sample of 1000 of the error terms. For each replication we then calculate the kurtosis. The result is 1000 random samples of kurtosis. Here is the distribution of the kurtosis from this experiment. We can then build this 95% confidence interval around the kurtosis: 0.025 0.25 0.5 0.75 0.975 error quantiles 1.41 2.12 2.98 3.8 6.53 The median tells us that the kurtosis is very close the 3.0 value of a normal distirbution with little kurtosis in excess of 3.0. However, there is again a skewness in that the distance between the 2.5% and 50%tile versus the distance between the 50% and 97.5%tile are quite different. There is a higher probability of values above the median than below, thus the skewness. 14.21 Exercises Repeat the forecast confidence intervals for disposable income equal to 15, 20, 25. What do you observe about the width of the interval as the forecast increases? Test the hypothesis that the population marginal propensity to consume out of disposable income is no different than zero with a probability of type II error equal to 95%. Using the following data sets to compute all regression estimates, standard deviations, a forecast confidence interval for a forecasted independent variable observation, confidence intervals and hypothesis testing for each of the estimators, and \\(R^2\\) and \\(F\\) hypothesis testing for the overall model. For each data set and model extract the error terms and review the percentiles, mean, standard deviation, skewness, and kurtosis. Do they look like they were drawn from a normal distribution? Show all work. Interpret your findings. Peruvian anchovies Bronx corn US House of Representatives data from the package openintro 14.21.1 US House of Representatives seats and unemployment data &lt;- read.csv(&quot;data/unemployment-seats.csv&quot;) #str(data) lm_fit &lt;- lm(unemployment ~ house.seats, data) summary(lm_fit) ## ## Call: ## lm(formula = unemployment ~ house.seats, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.579 -2.264 -1.180 0.065 14.325 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.95447 1.28079 5.43 0.0000096 *** ## house.seats 0.00697 0.08259 0.08 0.93 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.13 on 27 degrees of freedom ## Multiple R-squared: 0.000264, Adjusted R-squared: -0.0368 ## F-statistic: 0.00712 on 1 and 27 DF, p-value: 0.933 14.21.2 Peruvian anchovies data &lt;- read.csv(&quot;data/peruvian_anchovies.csv&quot;) #str(data_peru) lm_fit &lt;- lm(price ~ catch, data) summary(lm_fit) ## ## Call: ## lm(formula = price ~ catch, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -110.0 -38.3 -19.0 34.6 142.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 451.99 36.79 12.28 0.000000037 *** ## catch -29.39 5.09 -5.78 0.000087664 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 71.6 on 12 degrees of freedom ## Multiple R-squared: 0.736, Adjusted R-squared: 0.714 ## F-statistic: 33.4 on 1 and 12 DF, p-value: 0.0000877 14.21.3 Bronx corn data &lt;- read.csv(&quot;data/corn_bronx.csv&quot;) #str(data) lm_fit &lt;- lm(corn ~ fertilizer, data) summary(lm_fit) ## ## Call: ## lm(formula = corn ~ fertilizer, data = data) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 2.839 -2.375 -1.679 -3.589 1.107 3.804 -0.107 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.250 3.057 8.91 0.0003 *** ## fertilizer 1.652 0.142 11.64 0.000082 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3 on 5 degrees of freedom ## Multiple R-squared: 0.964, Adjusted R-squared: 0.957 ## F-statistic: 135 on 1 and 5 DF, p-value: 0.0000822 14.21.4 Consumption and disposable income data_cdi &lt;- read.csv(&quot;data/consumption_income.csv&quot;) #str(data_cdi) lm_fit &lt;- lm(consumption ~ income, data_cdi) summary(lm_fit) ## ## Call: ## lm(formula = consumption ~ income, data = data_cdi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.04821 -0.02181 -0.00425 0.00626 0.08556 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.136 1.704 0.08 0.94 ## income 0.918 0.134 6.85 0.000045 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0416 on 10 degrees of freedom ## Multiple R-squared: 0.824, Adjusted R-squared: 0.807 ## F-statistic: 46.9 on 1 and 10 DF, p-value: 0.0000446 #str(data_cdi) # log-log fit for elasticity estimation lm_fit &lt;- lm(log(consumption) ~ log(income), data_cdi) lm_summary &lt;- summary(lm_fit) lm_summary ## ## Call: ## lm(formula = log(consumption) ~ log(income), data = data_cdi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.004128 -0.001829 -0.000358 0.000542 0.007187 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0411 0.3661 -0.11 0.91 ## log(income) 0.9871 0.1440 6.86 0.000044 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.00352 on 10 degrees of freedom ## Multiple R-squared: 0.825, Adjusted R-squared: 0.807 ## F-statistic: 47 on 1 and 10 DF, p-value: 0.0000443 Is the slope estimate no different than 1? That is, is the elasticity of consumption with respect to disposable income unitary so that a 10% change in income will probably produce a 10% change in consumption? ## H_0: \\beta_1 = 1 &lt;=&gt; \\beta_1 - 1 = 0 b_1 &lt;- lm_fit$coefficients[2] ## extract slope estimate s_b1 &lt;- coef(lm_summary)[,2][2] ## extract slope estimate standard error t_score &lt;- (b_1 - 1) / s_b1 pr_t &lt;- 1-pt(t_score,nrow(data_cdi)-2) t_score ## log(income) ## -0.0894 ifelse(pr_t &gt; 0.01, &quot;Accept $H_0: \\beta_1 = 1$&quot;, &quot;Reject $H_0: \\beta_1 \\neq 1$&quot;) ## log(income) ## &quot;Accept $H_0: \\beta_1 = 1$&quot; Yes, a 10% change in income will probably produce a 10% in consumption in this sample. 14.22 References "],
["quantile-regression.html", "Chapter 15 Quantile regression 15.1 What is it? 15.2 An example 15.3 Even in ggplot2 15.4 Interpretations 15.5 Exercises 15.6 References", " Chapter 15 Quantile regression 15.1 What is it? When we think “regression” we usually think of linear regression where we estimate parameters based on the mean of the dependent variable. This mean is conditional on the various levels of the independent variables. what we are doing is explaining the variability of the dependent variable around its arithmetic mean. But we all know that this will only work if that mean is truly indicative of the central value of the dependent variable. What if it isn’t? What if the distribution of the dependent variable has lots of skewness, and thick (or very thin) tails? What if we do not have to use the mean? What if we can use other measures of position in the distribution of the dependent variable? We can and we will. These other positions are called quantiles. They measure the fraction of observations of the dependent variable less than the quantile position. Even more, we can measure the deviations of a variable around the quantile conditional on a meaningful list of independent explanatory variables. But we don’t have to always estimate the conditional mean. We could estimate the median, or the 0.25 quantile, or the 0.90 quantile. That’s where quantile regression comes in. The math under the hood is a little different, but the interpretation is basically the same. In the end we have regression coefficients that estimate an independent variable’s effect on a specified quantile of our dependent variable.2 15.2 An example Here is a motivating “toy” example. Below we generate data with non-constant variance then plot the data using the ggplot2 package: What do we observe from this scatter plot? We see the relationship of the dependent variable y gets more and more dispersed in its relationship (conditional) with the independent variable x, a well-known condition called heteroskedasticity. This condition fundamentally violates even the weaker of assumptions around the ordinary least squares (OLS) regression model. Our errors are normally distributed, but the variance depends on x. OLS regression in this scenario is of limited value. It is true that the estimated mean of y conditional on x is unbiased and as good an estimate of the mean as we could hope to get, but it doesn’t tell us much about the relationship between x and y, especially as x gets larger. Let’s build a plot of the confidence interval for predicted mean values of y using just OLS. The geom_smooth() function regresses y on x, plots the fitted line and adds a confidence interval. we immediately are taken aback by the incredibly small confidence interval relative to all of the rest of the scatter dots of data! But small x does seem to predict y fairly well. What about mid and higher levels of the relationship between y and x? There’s much more at work here. 15.3 Even in ggplot2 Just like we used geom_smooth() to get the OLS line (through the lm method), we can use geom_quantile()' to build a similar line that estimates intercept (b_0) and slope (b_1) of a line that runs not through the arithmetic mean ofybut through a quantile ofy` instead. Let’s try the 1 in 10 quantile of 90/%. We will look at how x explains deviation of y around the 0.90 quantile of y instead of the mean of y. Here a lot of the relationship between y and x is captured at the higher end of the y distribution. 15.4 Interpretations Let’s use the quantreg package to gain further insight and inference into our toy model. The variable taus captures a range of quantiles in steps of 0.10. The rq function is the quantile regression replacement for the OLS lm function. We display results using summary() with the boot option to calculate standard errors se. ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.1 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.64945 0.28611 19.74578 0.00000 ## x 0.04709 0.01016 4.63701 0.00001 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.2 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.97913 0.32572 18.35639 0.00000 ## x 0.05746 0.01305 4.40299 0.00003 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.3 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.9515 0.2962 20.0913 0.0000 ## x 0.0786 0.0142 5.5294 0.0000 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.4 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.94874 0.20228 29.40835 0.00000 ## x 0.09721 0.00902 10.78267 0.00000 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.5 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.02747 0.22854 26.37370 0.00000 ## x 0.10324 0.00712 14.49712 0.00000 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.6 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.19363 0.19201 32.25680 0.00000 ## x 0.11185 0.00735 15.22189 0.00000 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.7 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.17927 0.23250 26.57723 0.00000 ## x 0.12714 0.00893 14.23928 0.00000 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.8 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.3705 0.2290 27.8177 0.0000 ## x 0.1321 0.0059 22.3813 0.0000 ## ## Call: rq(formula = y ~ x, tau = taus, data = dat) ## ## tau: [1] 0.9 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.5197 0.3102 21.0175 0.0000 ## x 0.1446 0.0123 11.7931 0.0000 The intercepts and slopes change with the quantiles. We can depict these changes in this plot. We now have a distribution of intercepts and slopes that more completely describe the relationship between y and x. The t-stats and p-values indicate a rejection of the null hypotheses that b_0 = 0 or that b_1 = 0. The quantreg package includes a plot method to visualize the change in quantile coefficients along with their confidence intervals. We use the parm argument to indicate we only want to see the slope (or intercept) coefficients. Each dot is the slope coefficient for the quantile indicated on the x axis with a line connecting them. The red lines are the least squares estimate and its confidence interval. The lower and upper quantiles well exceed the OLS estimate. Let’s compare this whole situation of non-constant variance errors with data that has both normal errors and constant variance. Then let’s run quantile regression. The fit looks good for and OLS version of the “truth.” All of the quantile slopes are within the OLS confidence interval of the OLS slope. Nice. However, the quantile estimates of confidence intervals are far outside the OLS (“red”) bounds. This might lead us to question OLS inference in general. 15.5 Exercises Repeat the forecast confidence intervals for disposable income equal to 15, 20, 25. What do you observe about the width of the interval as the forecast increases? Test the hypothesis that the population marginal propensity to consume out of disposable income is no different than zero with a probability of type II error equal to 95%. Using the following data sets to compute all regression estimates, standard deviations, a forecast confidence interval for a forecasted independent variable observation, confidence intervals and hypothesis testing for each of the estimators, and \\(R^2\\) and \\(F\\) hypothesis testing for the overall model. For each data set and model extract the error terms and review the percentiles, mean, standard deviation, skewness, and kurtosis. Do they look like they were drawn from a normal distribution? Test this claim using quantile regression. Plot the quantile estimates against quantiles with confidence intervals. If the quantile confidence intervals are wider than the OLS confidence intervals, the OLS hypothesis testing might be suspect! Show all work. Interpret your findings. Peruvian anchovies Bronx corn US House of Representatives 15.5.1 US House of Representatives seats and unemployment data &lt;- read.csv(&quot;data/unemployment-seats.csv&quot;) #str(data) lm_fit &lt;- lm(unemployment ~ house.seats, data) summary(lm_fit) ## ## Call: ## lm(formula = unemployment ~ house.seats, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.579 -2.264 -1.180 0.065 14.325 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.95447 1.28079 5.43 0.0000096 *** ## house.seats 0.00697 0.08259 0.08 0.93 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.13 on 27 degrees of freedom ## Multiple R-squared: 0.000264, Adjusted R-squared: -0.0368 ## F-statistic: 0.00712 on 1 and 27 DF, p-value: 0.933 taus &lt;- 1:9/10 rq_fit &lt;- rq(unemployment ~ house.seats, data, tau = taus) rq_plot &lt;- summary(rq_fit, se = &quot;boot&quot;) plot(rq_plot) 15.5.2 Peruvian anchovies data &lt;- read.csv(&quot;data/peruvian_anchovies.csv&quot;) #str(data_peru) lm_fit &lt;- lm(price ~ catch, data) summary(lm_fit) ## ## Call: ## lm(formula = price ~ catch, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -110.0 -38.3 -19.0 34.6 142.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 451.99 36.79 12.28 0.000000037 *** ## catch -29.39 5.09 -5.78 0.000087664 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 71.6 on 12 degrees of freedom ## Multiple R-squared: 0.736, Adjusted R-squared: 0.714 ## F-statistic: 33.4 on 1 and 12 DF, p-value: 0.0000877 rq_fit &lt;- rq(price ~ catch, data, tau = taus) rq_plot &lt;- summary(rq_fit, se = &quot;boot&quot;) plot(rq_plot) #, xlim=c(0.001,10), ylim=c(0.001,10)) 15.5.3 Bronx corn data &lt;- read.csv(&quot;data/corn_bronx.csv&quot;) #str(data) lm_fit &lt;- lm(corn ~ fertilizer, data) summary(lm_fit) ## ## Call: ## lm(formula = corn ~ fertilizer, data = data) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 2.839 -2.375 -1.679 -3.589 1.107 3.804 -0.107 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.250 3.057 8.91 0.0003 *** ## fertilizer 1.652 0.142 11.64 0.000082 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3 on 5 degrees of freedom ## Multiple R-squared: 0.964, Adjusted R-squared: 0.957 ## F-statistic: 135 on 1 and 5 DF, p-value: 0.0000822 rq_fit &lt;- rq(corn ~ fertilizer, data, tau = taus) rq_plot &lt;- summary(rq_fit, se = &quot;boot&quot;) plot(rq_plot) 15.5.4 Consumption and disposable income data_cdi &lt;- read.csv(&quot;data/consumption_income.csv&quot;) #str(data_cdi) lm_fit &lt;- lm(consumption ~ income, data_cdi) summary(lm_fit) ## ## Call: ## lm(formula = consumption ~ income, data = data_cdi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.04821 -0.02181 -0.00425 0.00626 0.08556 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.136 1.704 0.08 0.94 ## income 0.918 0.134 6.85 0.000045 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0416 on 10 degrees of freedom ## Multiple R-squared: 0.824, Adjusted R-squared: 0.807 ## F-statistic: 46.9 on 1 and 10 DF, p-value: 0.0000446 #str(data_cdi) # log-log fit for elasticity estimation lm_fit &lt;- lm(log(consumption) ~ log(income), data_cdi) lm_summary &lt;- summary(lm_fit) lm_summary ## ## Call: ## lm(formula = log(consumption) ~ log(income), data = data_cdi) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.004128 -0.001829 -0.000358 0.000542 0.007187 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.0411 0.3661 -0.11 0.91 ## log(income) 0.9871 0.1440 6.86 0.000044 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.00352 on 10 degrees of freedom ## Multiple R-squared: 0.825, Adjusted R-squared: 0.807 ## F-statistic: 47 on 1 and 10 DF, p-value: 0.0000443 Is the slope estimate no different than 1? That is, is the elasticity of consumption with respect to disposable income unitary so that a 10% change in income will probably produce a 10% change in consumption? ## H_0: \\beta_1 = 1 &lt;=&gt; \\beta_1 - 1 = 0 b_1 &lt;- lm_fit$coefficients[2] ## extract slope estimate s_b1 &lt;- coef(lm_summary)[,2][2] ## extract slope estimate standard error t_score &lt;- (b_1 - 1) / s_b1 pr_t &lt;- 1-pt(t_score,nrow(data_cdi)-2) t_score ## log(income) ## -0.0894 ifelse(pr_t &gt; 0.01, &quot;Accept $H_0: \\beta_1 = 1$&quot;, &quot;Reject $H_0: \\beta_1 \\neq 1$&quot;) ## log(income) ## &quot;Accept $H_0: \\beta_1 = 1$&quot; Yes, a 10% change in income will probably produce a 10% in consumption in this sample. 15.6 References A now standard, and very accessible, reference for quantile regression is Koenker (2005).↩ "],
["multiple-linear-regression.html", "Chapter 16 Multiple Linear Regression 16.1 The more perfect relationship 16.2 Arrays and You 16.3 More Array Work 16.4 Exercising the model", " Chapter 16 Multiple Linear Regression 16.1 The more perfect relationship 16.2 Arrays and You Arrays have rows and columns and are akin to tables. All of Excel’s worksheets are organized into cells that are tables with columns and rows. Data frames are more akin to tables in data bases. Here are some simple matrix arrays and functions. We start by making a mistake: ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 1 The matrix() function takes as input here the sequence of numbers from 1 to 11. It then tries to put these 11 elements into a 4 column array with 3 rows. It is missing a number as the error points out. To make a 4 column array out of 11 numbers it needs a twelth number to complete the third row. We then type in these statements ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 In A we take 12 integers in a row and specify they be organized into 4 columns, and in R this is by row. In the next statement we see that A.col and column binding cbind() are equivalent. ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [,1] [,2] [,3] [,4] ## [1,] TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE Using the outer product allows us to operate on matrix elements, first picking the minimum, then the maximum of each row. The pmin and pmax compare rows element by element. If you used min and max you would get the minimum and maximum of the whole matrix. ## [,1] [,2] [,3] [,4] ## [1,] 0.75 0.75 0.75 0.75 ## [2,] 0.75 1.00 1.00 1.00 ## [3,] 0.75 1.00 1.25 1.25 ## [4,] 0.75 1.00 1.25 1.50 ## [,1] [,2] [,3] [,4] ## [1,] 0.75 1.00 1.25 1.5 ## [2,] 1.00 1.00 1.25 1.5 ## [3,] 1.25 1.25 1.25 1.5 ## [4,] 1.50 1.50 1.50 1.5 We build a symmetrical matrix and replace the diagonal with 1. A.sym looks like a correlation matrix. Here all we were doing is playing with shaping data. ## [,1] [,2] [,3] [,4] ## [1,] -0.50 -0.25 0.00 0.25 ## [2,] -0.25 -0.50 -0.25 0.00 ## [3,] 0.00 -0.25 -0.50 -0.25 ## [4,] 0.25 0.00 -0.25 -0.50 ## [,1] [,2] [,3] [,4] ## [1,] 1.00 -0.25 0.00 0.25 ## [2,] -0.25 1.00 -0.25 0.00 ## [3,] 0.00 -0.25 1.00 -0.25 ## [4,] 0.25 0.00 -0.25 1.00 16.2.1 Running a regression The inner product %*% cross-multiplies successive elements of a row with the successive elements of a column. If there are two rows with 5 columns, there must be a matrix at least with 1 column that has 5 rows in it. Let’s run these statements. rgamma allows us to generate n.sim versions of the gamma distribution with scale parameter 0.5 and shape parameter 0.2. rlnorm is a popular financial return distribution with mean 0.15 and standard deviation 0.25. We can call up ??distributions to get detailed information. Let’s plot the histograms of each simulated random variate using hist(). The cbind function binds into matrix columns the row arrays x.1 and x.2. These might be simulations of operational and financial losses. The X matrix could look like the “design” matrix for a regression. Let’s simulate a response vector, say equity, and call it y and look at its histogram. Now we have a model for \\(y\\): \\[ y = X \\beta + \\varepsilon \\] where \\(y\\) is a 100 \\(\\times\\) 1 (rows \\(\\times\\) columns) vector, \\(X\\) is a 100 \\(\\times\\) 2 matrix, \\(\\beta\\) is a 2 \\(\\times\\) 1 vector, and \\(\\epsilon\\) is a 100 \\(\\times\\) 1 vector of disturbances (a.k.a., “errors”). Multiplying out the matrix term \\(X \\beta\\) we have \\[ y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon \\] where \\(y\\), \\(x_1\\), \\(x_2\\), and \\(\\varepsilon\\) are all vectors with 100 rows for simulated observations. If we look for \\(\\beta\\) to minimize the sum of squared \\(\\varepsilon\\) we would find that the solution is \\[ \\hat{\\beta} = (X^T X)^{-1} X^{T} y. \\] Where \\(\\hat{\\beta}\\) is read as “beta hat”. The result \\(y\\) with its hist() is The rubber meets the road here as we compute \\(\\hat{\\beta}\\). ## [,1] ## x.1 1.42 ## x.2 3.28 The beta.hat coefficients are much different than our model for y. Why? Because of the innovation, error, disturbance term rnorm(n.sim, 1, 2) we added to the 1.5*x.1 + 0.8 * x.2 terms. Now for the estimated \\(\\varepsilon\\) where we use the matrix inner product %*%. We need to be sure to pre-multiply beta.hat with X! We see that the “residuals” are almost centered at 0. 16.2.2 More about residuals For no charge at all let’s calculate the sum of squared errors in matrix talk, along with the number of obervations n and degrees of freedom n - k, all to get the standard error of the regression e.se. Mathematically we are computing \\[ \\sigma_{\\varepsilon} = \\sqrt{\\sum_{i=1}^N \\frac{\\varepsilon_i^2}{n-k}} \\] ## [,1] ## [1,] 2266 ## [1] 100 ## [1] 2 ## [,1] ## [1,] 4.81 The statement dim(X)[1] returns the first of two dimensions of the matrix X. Finally, again for no charge at all, lets load library psych (use install.packages(\"psych\") as needed). We will use pairs.panels() for a pretty picture of our work in this try out. First column bind cbind() the y, X, and e arrays to create a data frame for pairs.panel(). We then invoke the pairs.panels() function using the all array we just created. The result is a scatterplot matrix with histograms of each variate down the diagonal. The lower triangle of the matrix is populated with scatterplots. The upper triangle of the matrix has correlations depicted with increasing font sizes for higher correlations. We will use this tool again and again to explore the multivariate relationships among our data. 16.3 More Array Work We show off some more array operations in the following statements. ## [1] 4 ## [1] 4 ## [1] 4 4 We calculate the number of rows and columns first. We then see that these exactly correspond to the two element vector produced by dim. Next we enter these statements into the console. ## [1] 3.00 3.75 4.25 4.50 ## [1] 3.00 3.75 4.25 4.50 ## [1] 3.00 3.75 4.25 4.50 ## [1] 3.00 3.75 4.25 4.50 We also calculate the sums of each row and each column. Alternatively we can use the apply function on the first dimension (rows) and then on the second dimension (columns) of the matrix. Some matrix multiplications follow below. ## [,1] [,2] [,3] [,4] ## [1,] 0.750 0.750 0.812 0.875 ## [2,] 0.375 0.562 0.500 0.500 ## [3,] 0.375 0.500 0.688 0.625 ## [4,] 0.750 0.938 1.125 1.375 Starting from the inner circle of embedded parentheses we pull every row (the [,col] piece) for columns from the first to the second dimension of the dim() of A.min. We then transpose (row for column) the elements of A.min and cross left multiply in an inner product this transposed matrix with A.sym. We have already deployed very useful matrix operation, the inverse. The R function solve() provides the answer to the question: what two matrices, when multiplied by one another, produces the identity matrix? The identity matrix is a matrix of all ones down the diagonal and zeros elsewhere. ## [,1] [,2] [,3] [,4] ## [1,] 4.952380952380952550 -3.05 -1.14 -1.52 ## [2,] -2.285714285714285587 6.86 -2.29 0.00 ## [3,] 0.000000000000000127 -2.29 6.86 -2.29 ## [4,] -1.142857142857143016 -1.14 -3.43 3.43 Now we use our inverse with the original matrix we inverted. ## [,1] [,2] [,3] [,4] ## [1,] 0.999999999999999778 0.000000000000000000 0.000000000000000000 0.000000000000000000 ## [2,] 0.000000000000000222 1.000000000000000222 0.000000000000000000 0.000000000000000000 ## [3,] 0.000000000000000111 0.000000000000000111 1.000000000000000000 0.000000000000000444 ## [4,] 0.000000000000000222 -0.000000000000000444 -0.000000000000000888 1.000000000000000000 16.4 Exercising the model 16.4.1 Exercise 1: housing market determinants 16.4.2 Exercise 2: risk driver model using regression analysis 16.4.2.1 Problem We work for a healthcare insurer and our management is interested in understanding the relationship between input admission and outpatient rates as drivers of expenses, payroll, and employment. We gathered a sample of 200 hospitals in a test market in this data set. 16.4.2.2 Questions Build a table that explores this data set variable by variable and relationships among variables. ## hospital beds admissions outpatients births expense payroll fte ## 1 1 210 7713 86982 312 56831 22061 792 ## 2 2 347 16065 149222 1077 127223 55799 1762 ## 3 3 511 23028 222565 1027 157093 61326 2310 ## 4 4 142 4338 36710 355 24462 10503 328 ## 5 5 40 905 13350 168 13730 6368 181 ## 6 6 220 15563 88721 3810 93257 33920 1077 ## hospital beds admissions outpatients births expense payroll fte ## 195 195 70 2089 24369 387 17257 7425 216 ## 196 196 334 15696 102641 1946 168045 78118 1593 ## 197 197 190 6395 244254 545 79859 33639 1055 ## 198 198 122 441 0 0 15321 8878 399 ## 199 199 170 7244 167454 838 58247 25018 834 ## 200 200 73 352 9714 51 4565 2228 104 ## hospital beds admissions outpatients births ## Min. : 1.0 Min. : 7 Min. : 111 Min. : 0 Min. : 0 ## 1st Qu.: 50.8 1st Qu.: 84 1st Qu.: 1615 1st Qu.: 27316 1st Qu.: 0 ## Median :100.5 Median : 160 Median : 4777 Median : 65329 Median : 480 ## Mean :100.5 Mean : 210 Mean : 6832 Mean : 98225 Mean : 874 ## 3rd Qu.:150.2 3rd Qu.: 270 3rd Qu.: 9766 3rd Qu.:123263 3rd Qu.:1309 ## Max. :200.0 Max. :1297 Max. :37375 Max. :813369 Max. :5699 ## expense payroll fte ## Min. : 2082 Min. : 1053 Min. : 50 ## 1st Qu.: 20544 1st Qu.: 8693 1st Qu.: 314 ## Median : 43364 Median : 20740 Median : 590 ## Mean : 67140 Mean : 30501 Mean : 862 ## 3rd Qu.: 89898 3rd Qu.: 40275 3rd Qu.:1095 ## Max. :367706 Max. :188865 Max. :4087 Investigate the influence of admission and outpatient rates on expenses and payroll. First, form these arrays. ## [1] 56831 127223 157093 24462 13730 93257 ## 1 admissions outpatients ## [1,] 1 7713 86982 ## [2,] 1 16065 149222 ## [3,] 1 23028 222565 ## [4,] 1 4338 36710 ## [5,] 1 905 13350 ## [6,] 1 15563 88721 Next, compute the regression coefficients. ## [,1] ## 1 -118.9178 ## admissions 8.6995 ## outpatients 0.0797 Finally, compute the regression statistics. ## [,1] ## [1,] 171528065201 ## [1] 200 ## [1] 3 ## [,1] ## [1,] 29508 Use this code to investigate further the relationship among predicted expenses and the drivers, admissions and outpatients. 16.4.3 Exercise 3: commercial loan projection using regression 16.4.3.1 Purpose and problem This project will allow us to practice various R features using live data to support a decision regarding the provision of captive financing to customers at the beginning of this chapter. We will focus on translating regression statistics into R, plotting results, and interpreting ordinary least squares regression outcomes. As we researched how to provide captive financing and insurance for our customers we found that we needed to understand the relationships among lending rates and various terms and conditions of typical equipment financing contracts. We will focus on one question: What is the influence of terms and conditions on the lending rate of fully committed commercial loans with maturities greater than one year? The data set commloan.csv contains data from the St. Louis Federal Reserve Bank’s FRED website we will use to get some high level insights. The quarterly data extends from the first quarter of 2003 to the second quarter of 2016 and aggregates a survey administered by the St. Louis Fed. There are several time series included. Each is by the time that pricing terms Were set and by commitment, with maturities more than 365 Days from a survey of all commercial banks. Here are the definitions. rate: weighted-average effective loan rate in per cent per annum prepay: per cent of value of loans subject to prepayment penalty per annum maturity: weighted-average maturity/repricing interval in days size: average loan size in thousands USD volume: total value of loans in millions USD 16.4.4 Work Flow Prepare the data. Visit the FRED website. Include any information on the site to enhance the interpretation of results. Use read.csv to read the data into R. Be sure to set the project’s working directory where the data directory resides. Use na.omit() to clean the data. Assign the data to a variable called x.data. Examine the first and last 5 entries. Run a summary of the data set. ## date prepaypenalty maturity rate size volume ## 1 4/1/2003 16.5 124 3.77 449 11406 ## 2 7/1/2003 18.1 70 3.09 356 14586 ## 3 10/1/2003 44.9 48 2.83 532 21022 ## 4 1/1/2004 30.4 87 3.06 602 21472 ## 5 4/1/2004 23.5 68 2.97 600 22359 ## 6 7/1/2004 20.0 80 3.36 593 23780 ## date prepaypenalty maturity rate size volume ## 49 4/1/2015 17.8 65 2.47 1151 24620 ## 50 7/1/2015 16.9 76 2.30 1405 30586 ## 51 10/1/2015 11.7 77 2.31 1534 36840 ## 52 1/1/2016 13.6 66 2.43 1317 36316 ## 53 4/1/2016 20.6 93 2.63 1227 24803 ## 54 7/1/2016 14.5 66 2.41 1460 40682 ## date prepaypenalty maturity rate size volume ## 1/1/2004: 1 Min. : 8.8 Min. : 40 Min. :2.24 Min. : 356 Min. :11406 ## 1/1/2005: 1 1st Qu.:16.9 1st Qu.: 68 1st Qu.:2.48 1st Qu.: 640 1st Qu.:15451 ## 1/1/2006: 1 Median :20.7 Median : 89 Median :2.83 Median : 824 Median :18670 ## 1/1/2007: 1 Mean :23.1 Mean : 95 Mean :3.65 Mean : 882 Mean :20824 ## 1/1/2008: 1 3rd Qu.:29.9 3rd Qu.:112 3rd Qu.:4.20 3rd Qu.:1018 3rd Qu.:24258 ## 1/1/2009: 1 Max. :51.9 Max. :396 Max. :7.41 Max. :1715 Max. :40682 ## (Other) :48 What anomalies appear based on these procedures? Explore the data. Let’s plot the time series data using this code: Describe the data frame that melt() produces. Let’s load the psych library and produce a scatterplot matrix. Interpret this exploration. Analyze the data. Let’s regress rate on the rest of the variables in x.data. To do this we form a matrix of independent variables (predictor or explanatory variables) in the matrix X and a separate vector y for the dependent (response) variable rate. We recall that the 1 vector will produce a constant intercept in the regression model. ## [1] 3.77 3.09 2.83 3.06 2.97 3.36 ## 1 prepaypenalty maturity size volume ## 1 1 16.5 124 449 11406 ## 2 1 18.1 70 356 14586 ## 3 1 44.9 48 532 21022 ## 4 1 30.4 87 602 21472 ## 5 1 23.5 68 600 22359 ## 6 1 20.0 80 593 23780 Explain the code used to form y and X. Calculate the \\(\\hat{\\beta}\\) coefficients and interpret their meaning. ## [,1] ## 1 7.7714381 ## prepaypenalty -0.0696900 ## maturity 0.0064000 ## size -0.0020414 ## volume -0.0000635 Calculate actual and predicted rates and plot using this code. Insert explanatory comments into the code chunk to document the work flow for this plot. Interpret the graphs of actual and residual versus predicted values of rate. Calculate the standard error of the residuals, Interpret its meaning. ## [,1] ## [1,] 88.6 ## [1] 54 ## [1] 5 ## [,1] ## [1,] 1.34 Interpret and present results. We will produce an R Markdown document with code chunks to document and interpret our results. The format will introduce the problem to be analyzed, with sections that discuss the data to be used, and which follow the work flow we have defined. "],
["binary-regression.html", "Chapter 17 Binary Regression 17.1 What is Binary Risk Analysis? 17.2 An expository simulation 17.3 Enter the discrete decision maker 17.4 The odds are … 17.5 Predict now … 17.6 A deposit 17.7 References", " Chapter 17 Binary Regression 17.1 What is Binary Risk Analysis? In many applications we want to predict the odds of an outcome, e.g., default, product acceptance, machine failure, fraud, etc. In risk management we can use the prediction of odds to quantify the likelihood of a risk event occurring. This simulation illustrates the idea: Create two indicators of the occurrence of a risk event, for example vendor inability to deliver, borrower inability to pay, customer refusal to buy, etc. Combine these indicators into a score n &lt;- 1000 set.seed(555) x1 &lt;- rnorm(1000) # some continuous variables we can call risk factors x2 &lt;- rnorm(1000) z &lt;- 1 + 2*x1 + 3*x2 # the &quot;risk score&quot; is a linear combination with a bias xz_df &lt;- data_frame(x1 = x1, x2 = x2, z = z) p &lt;- ggplot(xz_df, aes(x = 1:length(z), y = z)) + geom_line(color = &quot;blue&quot;, size = 1.00) + geom_point(color = &quot;red&quot;, size = 1.25) ggplotly(p) The odds that an event might occur is defined as the ratio of the probability that the event occurs, \\(\\pi\\) to the probability that the event does not occur, \\(1 - \\pi\\), We observe several vendor, credit, or product acceptance accounts. When the account is not accepted as a vendor, or if a customer defaults, or when a product is refused, we assign a 1, otherwise we will assign a zero. The odds ratio ends up being the sum of 1’s divided by the sum of 0’s we observe accross accounts. We hypothesize that several factors might help systematically explain the choices behind, for example, default / acceptance / refusal. Suppose all other reasons why a default / acceptance / refusal might occur are contained in a factor we will call \\(z\\). To examine the hypothesis that \\(z\\) explains behavior we set the behaviorially observed odds ratio equal to a predictor model: \\[ \\frac{\\pi}{1-\\pi} = exp(z), \\] and solve for \\(\\pi\\) to get \\[ \\pi = \\frac{1}{1+exp(-z)}. \\] This is the traditional statistical derivation behind the use of a logistic representation of a decision maker’s choice. This paper further asks how can a discrete choice derivation of the logistic representation improve our interpretation of the statistics of a decision makers binary choice. 17.2 An expository simulation Let’s translate the mathemetical expression for the logistic probability \\(\\pi\\) directly into R. Then let’s calculate “Bernoulli” coin toss trials. A Bernouli coin flip is an example of a 1 (e.g., “heads = default”) occurring (indicated by a \\(1\\)) and not occurring (indicated by a \\(0\\)). We then plot the results. prob &lt;- 1/(1+exp(-z)) # pass through an inverse-logit function # odds ratio: probability that 1 in 1+exp(-z) event occurs y &lt;- rbinom(1000,1,prob) # bernoulli response variable y_df &lt;- data_frame(y = y, z = z, prob = prob) p &lt;- ggplot(y_df, aes(x = z, y = prob )) + geom_line(color = &quot;blue&quot;, size = 1.00) + geom_point(color = &quot;red&quot;, size = 1.25) ggplotly(p) # plot logistic curve The scatter shows a gathering of indicators at \\(1\\) and \\(0\\) with a cloud of potential probable occurrences in between. The plot of probability versus the \\(z\\) scores marks out the logistic \\(S\\) curve. p &lt;- ggplot(xz_df, aes(x = z, y = y )) + geom_point(color = &quot;blue&quot;, size = 1.25) ggplotly(p) More insight can be gotten by looking at the cross scatter plot and histograms of the event indicator \\(y\\) and predictor variables \\(x_1\\) and \\(x_2\\). # Load the GGally library to view relationships and histograms # install.packages(&quot;psych&quot;) #if necessary yx1x2_df &lt;- tibble(y = y, x1 = x1, x2 = x2) #make a column-wise matrix ggpairs(yx1x2_df) Let’s build the predictor model using ordinary least squares (OLS) and the general linear model (GLM). GLM assumes that the indicator variable is binomially distributed like a (loaded) coin toss. OLS assumes that the outcomes are normally distributed. fit_logistic &lt;- glm( y~x1+x2,data = yx1x2_df, family = &quot;binomial&quot;) summary(fit_logistic) ## ## Call: ## glm(formula = y ~ x1 + x2, family = &quot;binomial&quot;, data = yx1x2_df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8179 -0.3140 0.0791 0.4392 2.2531 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.160 0.120 9.68 &lt;0.0000000000000002 *** ## x1 2.047 0.163 12.58 &lt;0.0000000000000002 *** ## x2 3.101 0.212 14.64 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1341.0 on 999 degrees of freedom ## Residual deviance: 594.7 on 997 degrees of freedom ## AIC: 600.7 ## ## Number of Fisher Scoring iterations: 6 #fit_ols &lt;- lm( y~x1+x2,data = yx1x2_df) #summary(fit_ols) Let’s attempt an interpretation of the coefficients of the logistic regression. For that to happen we must discuss what the odds are. 17.3 Enter the discrete decision maker The generation of logistic-binomial factor models derives from an agent’s discrete (that is, binary) choices under constraint. When modeling risk, we consider unsystematic deviations from systematic expectations. Decision makers have preferences for risk relative to performance. These preferences can be expressed in terms of von Neuman-Morgenstern utility functions.3 Let \\(i\\) index decision makers and \\(j\\) alternatives so that\\(x_{ij}\\) are a vector of features of alternative \\(j\\) for individual decision maker \\(i\\) and \\(s_{i}\\) a vector of characteristics of each decision maker \\(i\\). Together \\(z_{ij}=z_{ij}(x_{ij},s_{i})\\) describe features and characteristics for each decision maker \\(i\\). Each decision maker has an indirect utility \\(u(z_{ij})\\) based on quantities and qualities of chosen goods and services. In turn each decision maker forms expectations \\(V\\) about utility. In this way indirect utility decomposes into \\[ u(z_{ij}) = V(z_{ij}) + \\varepsilon_{ij} \\] where \\(\\varepsilon_{ij}\\) is a mean zero, finite variance random term. Decision maker \\(i\\) will choose alternative \\(j\\) over \\(k\\), for \\(j \\neq k\\) when \\(u(z_{ij}) &gt; u(z_{ik})\\) so that \\[ V(z_{ij}) + \\varepsilon_{ij} &gt; V(z_{ik}) + \\varepsilon_{ik} \\] With a collection of like terms there is \\[ \\varepsilon_{ik} - \\varepsilon_{ij} &lt; V(z_{ij}) - V(z_{ik}) \\] and a transposition of terms in \\(j\\) and \\(k\\). The probability that a decision maker \\(i\\) will choose alternative \\(j\\) over \\(k\\) is then the cumulative probability \\(Q_{ij}\\) \\[ Q_{ij} = Pr[\\varepsilon_{ik} - \\varepsilon_{ij} &lt; V(z_{ij}) - V(z_{ik})] \\] In the binary case with two alternatives \\(j=(1,2)\\) and assuming one decision maker, there is \\[ Q_{1} = Pr[\\varepsilon_{2} - \\varepsilon_{1} &lt; V(z_{1}) - V(z_{2})] \\] When indirect utility is linear in \\(z_{ij}\\) there is \\[ V(z_{ij}) = \\beta^Tz_{ij} \\] Further assuming that \\(\\varepsilon_{ij}=\\varepsilon_{i}\\) is independent of \\(z_{ij}\\) and statistically independent, there is the question of what distributions of \\(\\varepsilon_{i}\\) give rise to a logistic representation of discrete choice. Suppose we assume a simple one-parameter version of the Weibull function. This function in multiple parameters is widely used in reliability analysis owing to the richness of potential shapes of the distribution function. For a very naive one-parameter version of Weibull realizations \\(w_j\\) of alternative \\(j\\) we suppose that \\(w_j = \\alpha + \\varepsilon_j\\). Then the cumulative probabiity function (CDF) for the Weibull function is \\[ Pr[\\varepsilon_j &lt; \\varepsilon] = -exp(e^{-(\\alpha_i + \\varepsilon)}) \\] Differentiating with respect to \\(\\varepsilon\\) we get the probability density function (PDF) \\[ Pr[\\varepsilon] = -e^{-(\\alpha_i + \\varepsilon)}exp(e^{-(\\alpha_i + \\varepsilon)}) \\] Take any constant \\(c\\) we can show that deviations shifted by \\(c\\), \\(\\varepsilon_i+c\\), from anticipated indirect utility \\(V_i\\) will have a Weibull distributed with parameter \\(\\alpha_i-c\\). The maximum of \\(\\varepsilon_j\\) across all choices from \\(j=1,\\dots,J\\) will also have a Weibull distribution with parameter \\(-log \\, \\, \\Sigma_{j=1}^Jexp(-\\alpha_j)\\) all very nicely keeping with the convenient properties of exponents and their logarithms. We doo all of thiis wrangling to evaluate the probability of rankings of indirect utility. Let’s take the binary case of choosing between door number 1 and door number 2. We let the indirect utilities take the place of the constant \\(c\\) so that the probability of choosing 1 over 2 is then \\[ Pr(V_1 + \\varepsilon_1 &gt; V_1 + \\varepsilon_1)=\\frac{e^{V_1-\\alpha_1}}{e^{V_1-\\alpha_1}+e^{V_2-\\alpha_2}} \\] If we now collect observations of features \\(i\\) for each of the two choices \\(k=1,2\\) in a matrix \\(z\\) we can relate these features with choices. \\[ V_k=\\alpha_k+z_{ki}\\beta \\] Then the probability of choosing door number 1 versus door number 2 is the logistic model \\[ Pr(V_1 + \\varepsilon_1 &gt; V_1 + \\varepsilon_1)=\\frac{e^{z_{1i}\\beta}}{e^{z_{1i}\\beta}+e^{z_{2i}\\beta}} =\\frac{1}{1+e^{(z_{1i}-z_{2i})\\beta}} \\] Now we can relate features to choices. Even more interestly we can relate odds of features relative to choices as marginal (indirect) utilities of risk due to decision factors \\(z\\). As an example consider this situation. [BUILD EXAMPLE HERE] 17.4 The odds are … Everything with odds starts with the concept of probability. Probability in a decision context means preferences for risk and return. At one stage in a decision these preferences are prioritization weights. Such weights can further be modelled in a linear goal programming model. Let’s say that the probability of success of some event is 0.8. Then the probability of failure is 1 – 0.8 = 0.2. The odds of success are defined as the ratio of the probability of success over the probability of failure. In our example, the odds of success are \\(0.8/0.2 = 4\\). That is to say that the odds of success are 4 to 1: it is 4 times more likely to be successful than not. If the probability of success is 0.5, i.e., 50-50 percent chance, then the odds of success are 1 to 1, that is, equally likely. Let’s say we have a choice betweeen a bright shiny object and no object at all, just letting business be as usual. A success is defined then as the choice of a bright shiny object. If the odds of success are 4 to 1, then we might interpret the odds as a 4 times more preferential choice for the bright shiny object versus standing pat and choosing the status quo. In this context of discrete choice the adjectives preferential, credible, and likely layer our interpretation of results. The transformation from probability to odds is monotonic, that is, the odds increase as the probability increases and vice-versa. Probability ranges from 0 and 1. Odds range from 0 and positive infinity. Below is a table of the transformation from probability to odds, where the odds in favor of an event are read x times favorable to 1 time unfavorable. probability &lt;- seq(0.01, 0.99, length.out = 10) odds &lt;- round(probability / (1 - probability), 2) odds_tbl &lt;- tibble(probability = probability, odds = paste0(odds, &quot; : 1&quot;)) kable(odds_tbl) probability odds 0.010 0.01 : 1 0.119 0.13 : 1 0.228 0.29 : 1 0.337 0.51 : 1 0.446 0.8 : 1 0.554 1.24 : 1 0.663 1.97 : 1 0.772 3.39 : 1 0.881 7.41 : 1 0.990 99 : 1 A plot shows the transformaation more clearly than the table. probability &lt;- seq(0.01, 0.99, length.out = 100) odds &lt;- round(probability / (1 - probability), 2) odds_2_tbl &lt;- tibble(probability = probability, odds) p &lt;- ggplot(odds_2_tbl, aes(x = probability, y = odds)) + geom_line(color = &quot;blue&quot;, size = 1.25) ggplotly(p) The odds ratio has a very sharply changing bend with probabilities greater than 0.75. Now for the coefficients of the logistic regression. We go back to the definition of the logistic curve that transforms a binomial sequence of 0’s andd 1’s, failures and successes, choices of door number zero or door number one, into an odds ratio. Here again \\(\\pi\\) is the probability of a favorable outcome, that is a success, a choice of a bright shiny object or door number one. The odds ratio relates to the scoring model with features conditioning our choices \\(z\\) through the transformation \\(exp(z)\\). \\[ \\frac{\\pi}{1-\\pi} = exp(z), \\] If wew take the natural logarithm of both sides we can isolate the coefficients in \\(z=\\beta_0+\\beta_1x_1+\\beta_2x_2\\).4 \\[ log\\left(\\frac{\\pi}{1-\\pi}\\right) = z = \\beta_0+\\beta_1x_1+\\beta_2x_2 , \\] Let’s hold constant \\(x_2\\) while \\(\\beta_0\\) certainly doesn’t change and so that \\(\\beta_2x_2\\) is also a constant. We are left with the term in \\(x_1\\) by itself, ceteris paribus, all else held equal. Then the change in the log odds for a unit change in \\(x_1\\) is thus just \\(\\beta_1\\). This means that the coefficients of the logistic regression are really expressed in terms of logarithms. To recover the odds inside the logarithms all we have to calculate the inverse of the log so that \\(exp(\\beta_1)=exp(2.0469)=7.696\\). We are not yet done: we must subtract a 1 from this expression since we were calculating the impact of a one unit change in the value of the \\(x_1\\) variate. Thus a one unit change in \\(x_1\\) changes the odds by 669.59%. The \\(x_1\\) variate is quite an influential factor! 17.5 Predict now … We begin a so-called prediction with some new data we think might occur for the values of \\(x_1\\) and \\(x_2\\). suppose we think a reasonable scenario is the 75th quantile of \\(x_1\\) and the 25th quantile of \\(x_2\\). (data_scenario &lt;- yx1x2_df %&gt;% summarize(x1 = quantile(x1, 0.75), x2 = quantile(x2, 0.25))) ## # A tibble: 1 x 2 ## x1 x2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.639 -0.709 In specifying the data for the scenario we must use the same variable names that we used to estimate the logistic regression. (data_scenario$x1_prob &lt;- predict(fit_logistic, newdata = data_scenario, type = &quot;response&quot;)) ## 1 ## 0.567 This gives us a single forecast of 57% probability that favorable event would occur. Let’s visualize this by generating a grid of values for \\(x_1\\) and plotting probabilities versus those values data_scenario_plot &lt;- yx1x2_df %&gt;% mutate(x1 = seq(from = quantile(x1, 0.50), to = max(x1), length.out = 1000), x2 = mean(x2)) data_scenario_pred &lt;- predict(fit_logistic, newdata = data_scenario_plot , type = &quot;link&quot;, se = TRUE) data_scenario_pred &lt;- cbind(data_scenario_plot, data_scenario_pred) data_scenario_CI &lt;- within(data_scenario_pred, { prob_pred &lt;- plogis(fit) LL &lt;- plogis(fit - (1.96 * se.fit)) UL &lt;- plogis(fit + (1.96 * se.fit)) }) p &lt;- ggplot(data_scenario_CI, aes(x = x1, y = prob_pred)) + geom_ribbon(aes(ymin = LL, ymax = UL), alpha = 0.6) + geom_line(color = &quot;blue&quot;, size = 1.25) ggplotly(p) We can now forecast with a confidence interval of estimated probabilities of success conditional on levels of \\(x_1\\). 17.6 A deposit The coup de grace: We next deposit the results of this analysis into a goal programming model. 17.7 References McFadden Hogg and Craig Current goal programming review This discussions follow McFadden (1981) closely.↩ How interestyin gis it that a further interpretation of odds rears its beautiful head. From information theory, the informativeness of a message, or here a choice conditional on features – a compound message, is the weighted average of the \\(log\\) odds of choosing one path or another, one door, one object, or another. And this is conditional on the features experienced for each choice.↩ "],
["time-series-analysis.html", "Chapter 18 Time Series Analysis 18.1 Preliminaries 18.2 Where do we start? 18.3 Autoregressive processes 18.4 Moving average processes 18.5 Takeaways 18.6 References", " Chapter 18 Time Series Analysis What follows are notes on time series models. By its nature a set of notes is incomplete, may contain a few annoying errors of formulation, organization, and grammar. For deeper dives we should consult the far more extensive time series treatments provided by Ruppert and Matteson (2015) and McNeil, Frey, and Embrechts (2015) (also see their chapter on the empirical characteristics of time series). 18.1 Preliminaries Autoregressive processes represent the dependence of a current value of a series \\(y_t\\) on past episodes \\(s\\) of \\(y_{t-s}\\). AR(2) would look like this: \\[ y_t=\\phi_0 + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\varepsilon_t \\] An AR process can be distinguished from an MA process by its persistence. Since autoregression is an iterative process, values of the random error fade away slowly as each year feeds to the next. The MA process represents correlations only of the random component, so after \\(q\\) periods the random error is no longer in the system. MA(2) looks like this \\[ y_t=\\theta_0 + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\varepsilon_t \\] The persistence of lagged terms can be examined through the autocorrelation function \\[ ACF_s(y_t) = \\frac{cov(y_t, y_{t-s})}{\\sigma_y^2} \\] The partial autocorrelation function (PACF) are the coefficients \\(\\phi_s\\) of the regression of \\(y_t\\) on \\(y_{t-s}\\) \\[ y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots \\phi_s y_{t-s} + \\varepsilon_t \\] The PACF is used to examine the persistence of error terms as they feed lagged values of a time series variate into the error terms of the time series representation. 18.2 Where do we start? We often start by first differencing a series to remove non-stationary trends. After a time series has been differenced, the next step is to examine the impact of past values of a series on the current value, and do this for all values of a series. By looking at the autocorrelation function (ACF) and partial autocorrelation (PACF) plots of the differenced series, we can tentatively identify the numbers (lags) of AR and/or MA terms that will explain current values of a series. The ACF plot is typically visualized as a bar chart of the raw coefficients of correlation between a time series and each lag of itself, each taken separately. The PACF plot is a plot of the partial correlation coefficients between the series and lags of itself. In general, the partial correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. For example, if we are regressing a variable \\(Y\\) on other variables \\(X_1\\), \\(X_2\\), and \\(X_3\\), the partial correlation between \\(Y\\) and \\(X_3\\) is the amount of correlation between \\(Y\\) and \\(X_3\\) that is not explained by their common correlations with \\(X_1\\) and \\(X_2\\). This partial correlation can be computed as the square root of the reduction in variance that is achieved by adding \\(X_3\\) to the regression of \\(Y\\) on \\(X_1\\) and \\(X_2\\). In a similar way a partial autocorrelation is the amount of correlation between a variable and a lag of itself that is not explained by correlations at all lower order lags. This lack of explanation is none other than the remaining \\(\\varepsilon_{t-s}\\) as lags are considered through the PACF process. The autocorrelation of a time series\\(Y\\) at lag 1 is the coefficient of correlation between \\(Y_t\\) and \\(Y_{t-1}\\), which is also the correlation between \\(Y_{t-1}\\) and \\(Y_{t-2}\\), and on average between all one lag pairs of \\(Y\\). But if \\(Y_{t}\\) is correlated with \\(Y_{t-1}\\), and \\(Y_{t-1}\\) has the same correlation with \\(Y_{t-2}\\), then we should also expect to find a correlation between \\(Y_{t}\\) and \\(Y_{t-2}\\). In this scenario, the size of the correlation we should expect at lag 2 is the square of the lag 1 correlation. Thus, the correlation at lag 1 propagates to lag 2 and possibly to higher-order lags through the error terms. The partial autocorrelation at lag 2 is therefore the difference between the actual correlation at lag 2 and the expected correlation due to the propagation of correlation at lag 1. If the PACF displays a sharp cutoff while the ACF decays more slowly (i.e., has significant spikes at higher lags), we say that the stationarized series displays an AR signature. This means that the autocorrelation pattern can be explained more easily by adding \\(p\\) AR terms at the PACF cutoff than by adding MA terms. There is thus persistence in the dependence of current values on the past, back to the lag at which the PACF sharply drops off to near zero, that is, statistically insignificant, partial autocorrelations. Theoretically, autocorrelation patterns can be removed from a stationary series by adding sufficient autoregressive terms (lags of the stationary series) to the equation. It is the role of the PACF to tells us how many lagged terms are likely be needed. However, this is not always the simplest way to explain a pattern of autocorrelation. Sometimes it is more efficient to add MA terms (lags of the forecast errors) instead. The autocorrelation function (ACF) plays the same role for MA terms that the PACF plays for AR terms. The ACF tells you how many MA terms are likely to be needed to remove the remaining autocorrelation from the differenced series. If the autocorrelation is significant at lag k but not at any higher lags, this indicates that \\(q\\) MA terms should be used to represent the series. In the this case, we say that the stationary series displays an MA signature. This means that the autocorrelation pattern can be explained by adding some MA terms with some AR terms. This is the principle of parsimony in modeling at work. We use only what we need. It can be shown algebraicly that the \\(AR(1\\) process is equivalent to a \\(MA(\\infty)\\) process and vice-versa. Parsinomy indicates that we should use a few AR terms when many MA terms might be present and vice-versa. An MA signature is commonly associated with negative autocorrelation at lag 1. The reason for this is that an MA term can partially offset an order of differencing in the process. To see this, we can create a ARIMA(0,1,1) model without constant that is equivalent to an exponential smoothing model. The forecasting equation for this model is \\[ y_t = \\mu + y_{t-1} - \\theta_1 \\varepsilon_{t-1} + \\varepsilon_{t} \\] where the MA(1) coefficient $_$1 corresponds to the term parameter \\(1- \\alpha\\) in the exponential smoothing model. If \\(\\theta_1\\) is equal to 1, this corresponds to an exponential smoothing model with \\(\\alpha=0\\), which is just a constant model as the process is never updated. On the other hand, if the moving-average coefficient is equal to 0, this model reduces to a random walk model where the differencing produces noise only around a constant mean \\(\\mu\\). 18.3 Autoregressive processes Let’s examine some examples of models with varying characteristics. We focus here on the AR(1) models. set.seed(7511) e &lt;- rnorm(200) x1 = x2 = x3 = x4 = e for (t in 2:200){ x1[t] = 0.98*x1[t-1]+e[t] x2[t] = -0.6*x2[t-1]+e[t] x3[t] = 1.00*x3[t-1]+e[t] x4[t] = 1.01*x4[t-1]+e[t] } x1234 &lt;- tibble(x1=x1, x2=x2, x3=x3, x4=x4) p &lt;- ggplot(x1234, aes(x=1:200, y = x1)) + geom_line(color=&quot;blue&quot;) + annotate(&quot;text&quot;,x=50, y=10, label=&quot;x1[t] = 0.98*x1[t-1]+e[t]&quot;) p acf(x1); pacf(x1) We see that there is some trend in this series and that trend results in a slowly decaying ACF. The PACF sharply cuts off at the very first lag of the AR(1) process. The series is persistent and on average exhibits lag one behavior throughout its history. Here is another AR(1) history. This time the coefficient on the AR(1) term negative. p &lt;- ggplot(x1234, aes(x=1:200, y = x2)) + geom_line(color=&quot;blue&quot;) + annotate(&quot;text&quot;,x=50, y=10, label=&quot;x2[t] = -0.6*x2[t-1]+e[t]&quot;) p acf(x1); pacf(x1) We see this time there is no trend in this series and still a slowly decaying ACF. The PACF again sharply cuts off at the very first lag of the AR(1) process. The series is persistent and on average exhibits lag one behavior throughout its history. This series also exhibits mean-reverting behavior. Now let’s look at an AR(1) history with a unit root. The unit root means that the AR coefficient is statistically no different than one. p &lt;- ggplot(x1234, aes(x=1:200, y = x3)) + geom_line(color=&quot;blue&quot;) + annotate(&quot;text&quot;,x=50, y=10, label=&quot;x3[t] = 1.00*x3[t-1]+e[t]&quot;) p acf(x3); pacf(x3) We see this time there is trend in this series with a slowly decaying ACF. The PACF again sharply cuts off at the very first lag of the AR(1) process. The series is persistent and on average exhibits lag one behavior throughout its history. This series also exhibits behavior similar to the first series. 18.4 Moving average processes Let’s look at the ACF and PACF patterns associated with a MA(1) process this time. We will use the mean-reverting model shortly on top of this model. n &lt;- 180 set.seed(7511) y1 &lt;- rnorm(n) y2 &lt;- rnorm(n) e &lt;- rnorm(n) e2 &lt;- rnorm(n) for (t in 9:n) { y1[t] = e[t] + 0.3*e[t-1] y2[t] = -0.6*y2[t-1] + e[t] + 0.3*e[t-1] } yarma &lt;- tibble(y1ma=y1, y2arma=y2) p &lt;- ggplot(yarma, aes(x=1:length(y1ma), y = y1ma)) + geom_line(color=&quot;blue&quot;) + annotate(&quot;text&quot;,x=50, y=10, label=&quot;y1[t] = e[t] + 0.3*e[t-1]&quot;) p acf(y1); pacf(y1) The autocorrelation is statistically nil at all lags. The PACF has a positive correlation at lag one and is clean thereafter. This means that only a MA(1) term is present in the history of this series. This is the hallmark of the MA only model. Let’s add to this very unpersistent model a persistence causing AR(1) term. p &lt;- ggplot(yarma, aes(x=1:length(y1ma), y = y2arma)) + geom_line(color=&quot;blue&quot;) + annotate(&quot;text&quot;,x=50, y=10, label=&quot;y2[t] = -0.6*y2[t-1] + e[t] + 0.3*e[t-1]&quot;) p acf(y2); pacf(y2) We have made out modeling lives difficult again. Introducing the AR(1) term brings back an oscillatory persistence in this mean-reverting model. Significant lags of 1, 2, and 3 periods are present in the ACF. A strong negative spike at lag 1 in the PACF is probably sufficient to represent the impact of the AR(1) term in the simulation of this history. The MA(1) term will represent the spikes evident in the ACF. 18.5 Takeaways Here are some takeaways from this analysis: AR and MA processes are invertible one into the other. Short AR lags are representable as very long MA lags and vice-versa. Slowly decaying ACF and sharply truncated PACF at lag \\(p\\) means that the series is persistent and \\(AR(p)\\) will represent the series. A very short ACF lag, say \\(q=1\\), and a PACF with a spike near lag 1 will indicate a (nearly) pure \\(MA(q)\\) process at work without any \\(AR(p)\\) influences. Adding an \\(AR(p)\\) to an \\(MA(q)\\) process can produce rich oscillating behavior when the \\(AR(1)\\) coefficient is negative and between 0 and 1. This is classic mean-reverting behavior. ACF and PACF with no spikes means that the series has no discernible pattern. The series is not predictable. In the case of a returns series we typically see a fast decaying ACF that indicates returns are not predictable. However we should bew care not to stray into a belief that returns are just a random walk. They can be skewed and thick tailed too. In the case of a price series, a slowly decaying ACF means that a single MA term may represent numerous AR lags of persistent dependence. These models indicate that there are patterns of the series that we can recover. In the case of the volatility of returns, a slowly decaying ACF indicates that there are perhaps several underlying episodes of changing volatility. We should then expect that the kurtosis of the series is also prominent. 18.6 References References "],
["bootstrapping-estimates.html", "Chapter 19 Bootstrapping Estimates 19.1 Imagine 19.2 Tools 19.3 [d p q r]unif 19.4 Permutations with sample() 19.5 Resampling with sample() 19.6 Expected shortfall test", " Chapter 19 Bootstrapping Estimates 19.1 Imagine Imagine if we surveyed our management team to guage their preferences and their opinions about the likelihood of events occuring How often a customer segment tends to default and how large a default that might be the minimum, most likely, maximum of on-time-in-full metrics for vendors by material and service category The probability that a low, medium, or high price will occur for the market value of our refined vegetable oil The priorities we place on serving (a triage!) customer complaints Each of these situations pairs outcomes with probabilities, in short a probability distribution. We can even feature priorities as probabilities if we define them the same way as a probability (always positive, all of them add up to one, etc.). 19.2 Tools Tools and thought we will need How to make draws from, and use random distributions Writing functions with known run-times and verifiable results Why we don’t use for loops What we will do now Write simulations using random number generation Explore the jackknife and end up at the bootstrap Simulate processes with memory Finally, for loops! 19.3 [d p q r]unif runif(): draws from the uniform distribution (others follow) Build a discrete distributions: Use sample(). Assign some data to values, specify the number of samples to be drawn fromm values in n_samples, specify probabilities for low, medium, and high values in ‘probs’, and also direct that we will sample with replacement. Count the draws from the three levels sampled using table() population_values &lt;- 1:3 n_samples &lt;- 100000 probs &lt;- c(.5, .2, .3) my_draw &lt;- sample (population_values, n_samples, probs, replace=TRUE) table(my_draw) ## my_draw ## 1 2 3 ## 50270 20041 29689 19.4 Permutations with sample() sample() is powerful – it works on any object that has a defined length(). Permutations of values sample(5) ## [1] 1 2 3 5 4 sample(1:6) ## [1] 5 4 3 6 2 1 Use replicate() with each sample() in a vector replicate(3,sample(c(&quot;Curly&quot;,&quot;Larry&quot;,&quot;Moe&quot;,&quot;Shemp&quot;))) ## [,1] [,2] [,3] ## [1,] &quot;Shemp&quot; &quot;Curly&quot; &quot;Curly&quot; ## [2,] &quot;Moe&quot; &quot;Shemp&quot; &quot;Shemp&quot; ## [3,] &quot;Larry&quot; &quot;Larry&quot; &quot;Larry&quot; ## [4,] &quot;Curly&quot; &quot;Moe&quot; &quot;Moe&quot; Or generate random lists sample(list(&quot;A&quot;,3,sum)) ## [[1]] ## [1] &quot;A&quot; ## ## [[2]] ## [1] 3 ## ## [[3]] ## function (..., na.rm = FALSE) .Primitive(&quot;sum&quot;) 19.5 Resampling with sample() When we resample from any existing distribution we generate the bootstrap family of estimators. The bootstrap_resample function just draws one sample of size n_sample = 3 from the data. Then we replicate this sampling so many times (5 here) bootstrap_resample &lt;- function (data, n_sample) sample(data, n_sample, replace=TRUE) t(replicate(5, bootstrap_resample (6:10, 3))) ## [,1] [,2] [,3] ## [1,] 7 9 8 ## [2,] 6 10 8 ## [3,] 6 7 8 ## [4,] 9 10 8 ## [5,] 10 9 6 The transpose simply arranges the samples in columns with the replications in rows. 19.6 Expected shortfall test Suppose management, or even more so wary investors, wanted to understand how much capital they could probably need to have to cover loss exposures. So far we have been using value at risk to set the threshold for the expected shortfall as a gross risk-informed measure of the amount of capital required against potential losses. To do this we would calculate an estimate of the range within which we could expect the expected shortfall to be, say, 95% of the time. The range would then identify at least (lower bound) and at most (upper bound) the amount of capital needed for a given probability that our expresses our confidence. First, we get some data, here exchange rates, and calculate percentage changes exrates &lt;- na.omit(read.csv(&quot;data/exrates.csv&quot;, header = TRUE)) exrates.r &lt;- diff(log(as.matrix(exrates[, -1]))) * 100 Then we build a helper function to calculated the expected shortfall once. ES_calc &lt;- function(data, prob){ data &lt;- -as.matrix(data) return(mean(data[data &gt; quantile(data, prob),])) } ES_1 &lt;- ES_calc(exrates.r[,1], 0.95) ES_1 ## [1] 1.7 Next, we take a sample of exchange rate returns (250 of them at random), calculate the ES, and do this a lot of times (1000 for now). ES_sample &lt;- replicate(10000, ES_calc(bootstrap_resample (-exrates.r[,1], 250), 0.95)) summary(ES_sample) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.36 1.81 1.94 1.96 2.10 3.10 Finally, we calculate the upper and lower bounds using the same quantile() function as we did in finding a value at risk threshold for the expected shortfall. (q_0.025 &lt;- quantile(ES_sample, 0.025)) ## 2.5% ## 1.58 (q_0.975 &lt;- quantile(ES_sample, 0.975)) ## 97.5% ## 2.43 (q_0.500 &lt;- quantile(ES_sample, 0.500)) ## 50% ## 1.94 A plot shows off our handiwork. ES_sample_df &lt;- data.frame(ES = ES_sample ) ES_title &lt;- &quot;Expected Shortfall USD-EUR 95%&quot; library(ggplot2) #ES_sample_df %&gt;% ggplot(ES_sample_df, aes(x = ES)) + geom_density() + ggtitle(ES_title) + geom_vline(xintercept = q_0.025, colour=&quot;red&quot;) + geom_vline(xintercept = q_0.975, colour=&quot;red&quot;) + geom_vline(xintercept = q_0.500, color = &quot;blue&quot;) "]
]
